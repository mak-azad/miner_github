@misc{grigorik2012github,
  author = {Ilya Grigorik},
  title = {GitHub Archive},
  year = {2012},
  howpublished = {\url{https://www.githubarchive.org}},
  note = {Retrieved from \url{https://www.githubarchive.org}}
}

@manual{MistralAPI,
  title = {Mistral API Documentation},
  author = {{Mistral AI}},
  year = {2024},
  url = {https://docs.mistral.ai/api/},
  note = {Accessed: 2024-05-23}
}


@misc{noauthor_files_nodate,
	title = {Files},
	url = {https://canvas.umd.umich.edu/courses/521888/files/folder/Homework?},
	urldate = {2021-11-11},
	file = {Files:/Users/akazad/Zotero/storage/T7PV3W3K/Homework.html:text/html},
}

@misc{noauthor_files_nodate-1,
	title = {Files},
	url = {https://canvas.umd.umich.edu/courses/521888/files/folder/Homework?},
	urldate = {2021-11-11},
}

@article{yin_learning_2018,
	title = {Learning to {Mine} {Aligned} {Code} and {Natural} {Language} {Pairs} from {Stack} {Overflow}},
	url = {http://arxiv.org/abs/1805.08949},
	abstract = {For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.},
	language = {en},
	urldate = {2021-12-03},
	journal = {arXiv:1805.08949 [cs]},
	author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08949},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
	file = {Yin et al. - 2018 - Learning to Mine Aligned Code and Natural Language.pdf:/Users/akazad/Zotero/storage/CIXGR9NK/Yin et al. - 2018 - Learning to Mine Aligned Code and Natural Language.pdf:application/pdf},
}

@inproceedings{zhou_benchmarking_2018,
	address = {Gothenburg Sweden},
	title = {Benchmarking microservice systems for software engineering research},
	isbn = {978-1-4503-5663-3},
	url = {https://dl.acm.org/doi/10.1145/3183440.3194991},
	doi = {10.1145/3183440.3194991},
	abstract = {Despite the prevalence and importance of microservices in industry, there exists limited research on microservices, partly due to lacking a benchmark system that reflects the characteristics of industrial microservice systems. To fill this gap, we conduct a review of literature and open source systems to identify the gap between existing benchmark systems and industrial microservice systems. Based on the results of the gap analysis, we then develop and release a medium-size benchmark system of microservice architecture.},
	language = {en},
	urldate = {2022-03-05},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceeedings}},
	publisher = {ACM},
	author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Xu, Chenjie and Ji, Chao and Zhao, Wenyun},
	month = may,
	year = {2018},
	pages = {323--324},
}

@article{wan_are_nodate,
	title = {Are {Machine} {Learning} {Cloud} {APIs} {Used} {Correctly}?},
	abstract = {Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efﬁciently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70\% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.},
	language = {en},
	author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
	keywords = {api misuse},
	pages = {13},
	file = {Wan et al. - Are Machine Learning Cloud APIs Used Correctly.pdf:/Users/akazad/Zotero/storage/DQSRMBPN/Wan et al. - Are Machine Learning Cloud APIs Used Correctly.pdf:application/pdf},
}

@article{maas_taxonomy_2020,
	title = {A {Taxonomy} of {ML} for {Systems} {Problems}},
	volume = {40},
	issn = {1937-4143},
	doi = {10.1109/MM.2020.3012883},
	abstract = {Machine learning has the potential to significantly improve systems, but only under certain conditions. We describe a taxonomy to help identify whether or not machine learning should be applied to particular systems problems, and which approaches are most promising. We believe that this taxonomy can help practitioners and researchers decide how to most effectively use machine learning in their systems, and provide the community with a framework and vocabulary to discuss different approaches for applying machine learning in systems.},
	number = {5},
	journal = {IEEE Micro},
	author = {Maas, Martin},
	month = sep,
	year = {2020},
	note = {Conference Name: IEEE Micro},
	keywords = {Adaptation models, Hardware, Learning systems, Machine learning, Neural networks, Taxonomy, ML for systems 2020},
	pages = {8--16},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/KRDHUJQD/9153088.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/K9V64UVA/Maas - 2020 - A Taxonomy of ML for Systems Problems.pdf:application/pdf},
}

@article{long_reporting_2022,
	title = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}: {An} {Exploratory} {Study} from {GitHub}},
	shorttitle = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}},
	url = {http://arxiv.org/abs/2204.07893},
	abstract = {The tremendous success of Deep Learning (DL) has significantly boosted the number of open-sourced DL frameworks hosted on GitHub. Among others, performance and accuracy bugs are critical factors that affect the reputation of these DL frameworks, therefore understanding the practice of discovering and investigating them for DL is important. In this paper, we conduct an exploratory study on the nature of reporting performance and accuracy bugs bugs for DL frameworks, aiming to improve our knowledge on this topic. Our study covers 10 most popular open-sourced DL frameworks on GitHub (e.g., TensorFlow, Keras, and PyTorch), based on which we sample 664 representative performance and accuracy bugs bug reports out of a total population of 22,522. Through systematic analysis of these samples, our key findings are: (1) low speed is the primary reason that a performance bug related report is submitted but we see no consistent pattern for accuracy related ones; (2) most of the reports are about issues encountered in the training stage; (3) only a small proportion of the reports provide insufficient information to investigate; (4) the majority of the performance and accuracy bugs bug reports (from 69\% to 100\%) are not related to the actual bug or regarded as unclassified; (5) around 50\% of the performance and accuracy bug reports, which indeed reveal bugs, are not resolved by direct patches. Deriving from the above, we discuss a set of actionable implications to the researchers, maintainers, and report submitters on this subject. To promote open science, the labeled dataset has been made publicly available at https://tinyurl.com/4x3tap9w.},
	urldate = {2022-04-24},
	journal = {arXiv:2204.07893 [cs]},
	author = {Long, Guoming and Chen, Tao},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.07893},
	keywords = {Computer Science - Software Engineering, HPC perf bug},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/GVJTDYFM/Long and Chen - 2022 - On Reporting Performance and Accuracy Bugs for Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/DU8TTW3D/2204.html:text/html},
}

@misc{noauthor_spring_nodate,
	title = {Spring {Boot} {Kubernetes}},
	url = {https://spring.io/guides/gs/spring-boot-kubernetes/},
	abstract = {this guide is designed to get you productive as quickly as possible and using the latest Spring project releases and techniques as recommended by the Spring team},
	language = {en},
	urldate = {2022-05-31},
	file = {Snapshot:/Users/akazad/Zotero/storage/T9RFRIUH/spring-boot-kubernetes.html:text/html},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8987574},
	urldate = {2022-06-11},
	file = {IEEE Xplore Full-Text PDF\::/Users/akazad/Zotero/storage/MM4FK2LB/stamp.html:text/html},
}

@inproceedings{guthaus_mibench_2001,
	address = {Austin, TX, USA},
	title = {{MiBench}: {A} free, commercially representative embedded benchmark suite},
	isbn = {978-0-7803-7315-0},
	shorttitle = {{MiBench}},
	url = {http://ieeexplore.ieee.org/document/990739/},
	doi = {10.1109/WWC.2001.990739},
	abstract = {This paper examines a set of commercially representative embedded programs and compares them to an existing benchmark suite, SPEC2000. A new version of SimpleScalar that has been adapted to the ARM instruction set is used to characterize the performance of the benchmarks using configurations similar to current and next generation embedded processors. Several characteristics distinguish the representative embedded programs from the existing SPEC benchmarks including instruction distribution, memory behavior, and available parallelism. The embedded benchmarks, called MiBench, are freely available to all researchers.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of the {Fourth} {Annual} {IEEE} {International} {Workshop} on {Workload} {Characterization}. {WWC}-4 ({Cat}. {No}.{01EX538})},
	publisher = {IEEE},
	author = {Guthaus, M.R. and Ringenberg, J.S. and Ernst, D. and Austin, T.M. and Mudge, T. and Brown, R.B.},
	year = {2001},
	pages = {3--14},
}

@article{parimi_datacenter_nodate,
	title = {Datacenter {Tax} {Cuts}: {Improving} {WSC} {Efﬁciency} {Through} {Protocol} {Buffer} {Acceleration}},
	abstract = {According to recent literature, at least 25\% of cycles in a modern warehouse-scale computer are spent on common “building blocks” [1]. Referred to by Kanev et al. as a “datacenter tax”, these tasks are prime candidates for hardware acceleration, as even modest improvements here can generate immense cost and power savings given the scale of such systems.},
	language = {en},
	author = {Parimi, Dinesh and Zhao, William J and Zhao, Jerry},
	pages = {10},
	file = {Parimi et al. - Datacenter Tax Cuts Improving WSC Efﬁciency Throu.pdf:/Users/akazad/Zotero/storage/TTBJ6IQ5/Parimi et al. - Datacenter Tax Cuts Improving WSC Efﬁciency Throu.pdf:application/pdf},
}

@inproceedings{kudrjavets_mining_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Mining code review data to understand waiting times between acceptance and merging: an empirical analysis},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Mining code review data to understand waiting times between acceptance and merging},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528432},
	doi = {10.1145/3524842.3528432},
	abstract = {Increasing code velocity (or the speed with which code changes are reviewed and merged) is integral to speeding up development and contributes to the work satisfaction of engineers. While factors affecting code change acceptance have been investigated in the past, solutions to decrease the code review lifetime are less understood. This study investigates the code review process to quantify delays and investigate opportunities to potentially increase code velocity. We study the temporal characteristics of half a million code reviews hosted on Gerrit and Phabricator, starting from the first response, to a decision to accept or reject the changes, and until the changes are merged into a target branch. We identified two types of time delays: (a) the wait time from the proposal of code changes until first response, and (b) the wait time between acceptance and merging. Our study indicates that reducing the time between acceptance and merging has the potential to speed up Phabricator code reviews by 29–63\%. Small code changes and changes made by authors with a large number of previously accepted code reviews have a higher chance of being immediately accepted, without code review iterations. Our analysis suggests that switching from manual to automatic merges can help increase code velocity.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Kudrjavets, Gunnar and Kumar, Aditya and Nagappan, Nachiappan and Rastogi, Ayushi},
	month = may,
	year = {2022},
	keywords = {*},
	pages = {579--590},
	file = {Kudrjavets et al. - 2022 - Mining code review data to understand waiting time.pdf:/Users/akazad/Zotero/storage/4SJ737ZH/Kudrjavets et al. - 2022 - Mining code review data to understand waiting time.pdf:application/pdf},
}

@inproceedings{kudrjavets_small_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Do small code changes merge faster?: a multi-language empirical investigation},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Do small code changes merge faster?},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528448},
	doi = {10.1145/3524842.3528448},
	abstract = {Code velocity, or the speed with which code changes are integrated into a production environment, plays a crucial role in Continuous Integration and Continuous Deployment. Many studies report factors influencing code velocity. However, solutions to increase code velocity are unclear. Meanwhile, the industry continues to issue guidelines on “ideal” code change size, believing it increases code velocity despite lacking evidence validating the practice. Surprisingly, this fundamental question has not been studied to date. This study investigates the practicality of improving code velocity by optimizing pull request size and composition (ratio of insertions, deletions, and modifications).},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Kudrjavets, Gunnar and Nagappan, Nachiappan and Rastogi, Ayushi},
	month = may,
	year = {2022},
	pages = {537--548},
	file = {Kudrjavets et al. - 2022 - Do small code changes merge faster a multi-langu.pdf:/Users/akazad/Zotero/storage/6B76EEXT/Kudrjavets et al. - 2022 - Do small code changes merge faster a multi-langu.pdf:application/pdf},
}

@misc{alomar_code_2022,
	title = {Code {Review} {Practices} for {Refactoring} {Changes}: {An} {Empirical} {Study} on {OpenStack}},
	shorttitle = {Code {Review} {Practices} for {Refactoring} {Changes}},
	url = {http://arxiv.org/abs/2203.14404},
	abstract = {Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.},
	urldate = {2022-12-10},
	publisher = {arXiv},
	author = {AlOmar, Eman Abdullah and Chouchen, Moataz and Mkaouer, Mohamed Wiem and Ouni, Ali},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14404 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/QTNIVZ5X/AlOmar et al. - 2022 - Code Review Practices for Refactoring Changes An .pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/8Y2P4ZV4/2203.html:text/html},
}

@inproceedings{ait_empirical_2022,
	address = {Pittsburgh Pennsylvania},
	title = {An empirical study on the survival rate of {GitHub} projects},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3527941},
	doi = {10.1145/3524842.3527941},
	abstract = {The number of Open Source projects hosted in social coding platforms such as GitHub is constantly growing. However, many of these projects are not regularly maintained and some are even abandoned shortly after they were created. In this paper we analyze early project development dynamics in software projects hosted on GitHub, including their survival rate. To this aim, we collected all 1,127 GitHub repositories from four different ecosystems (i.e., NPM packages, R packages, WordPress plugins and Laravel packages) created in 2016. We stored their activity in a time series database and analyzed their activity evolution along their lifespan, from 2016 to now. Our results reveal that the prototypical development process consists of intensive coding-driven active periods followed by long periods of inactivity. More importantly, we have found that a significant number of projects die in the first year of existence with the survival rate decreasing year after year. In fact, the probability of surviving longer than five years is less than 50\% though some types of projects have better chances of survival.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Ait, Adem and Izquierdo, Javier Luis Cánovas and Cabot, Jordi},
	month = may,
	year = {2022},
	pages = {365--375},
	file = {Ait et al. - 2022 - An empirical study on the survival rate of GitHub .pdf:/Users/akazad/Zotero/storage/N5CZDF8W/Ait et al. - 2022 - An empirical study on the survival rate of GitHub .pdf:application/pdf},
}

@inproceedings{grotov_large-scale_2022,
	address = {Pittsburgh Pennsylvania},
	title = {A large-scale comparison of {Python} code in {Jupyter} notebooks and scripts},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528447},
	doi = {10.1145/3524842.3528447},
	abstract = {In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Grotov, Konstantin and Titov, Sergey and Sotnikov, Vladimir and Golubev, Yaroslav and Bryksin, Timofey},
	month = may,
	year = {2022},
	pages = {353--364},
	file = {Grotov et al. - 2022 - A large-scale comparison of Python code in Jupyter.pdf:/Users/akazad/Zotero/storage/CHF5IRT9/Grotov et al. - 2022 - A large-scale comparison of Python code in Jupyter.pdf:application/pdf},
}

@inproceedings{veloso_characterizing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Characterizing high-quality test methods: a first empirical study},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Characterizing high-quality test methods},
	url = {https://dl.acm.org/doi/10.1145/3524842.3529092},
	doi = {10.1145/3524842.3529092},
	abstract = {To assess the quality of a test suite, one can rely on mutation testing, which computes whether the overall test cases are adequately exercising the covered lines. However, this high level of granularity may overshadow the quality of individual test methods. In this paper, we propose an empirical study to assess the quality of test methods by relying on mutation testing at the method level. We find no major differences between high-quality and low-quality test methods in terms of size, number of asserts, and modifications. In contrast, high-quality test methods are less affected by critical test smells. Finally, we discuss practical implications for researchers and practitioners.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Veloso, Victor and Hora, Andre},
	month = may,
	year = {2022},
	pages = {265--269},
	file = {Veloso and Hora - 2022 - Characterizing high-quality test methods a first .pdf:/Users/akazad/Zotero/storage/WIK7VJGF/Veloso and Hora - 2022 - Characterizing high-quality test methods a first .pdf:application/pdf},
}

@inproceedings{zimmerle_mining_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Mining the usage of reactive programming {APIs}: a study on {GitHub} and stack overflow},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Mining the usage of reactive programming {APIs}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3527966},
	doi = {10.1145/3524842.3527966},
	abstract = {Conventionally, callbacks and inversion of control have been the main tools to structure event-driven applications. Sadly, those patterns constitute a well-known source of design problems. The Reactive Programming (RP) paradigm has arisen as an approach to mitigate these problems. Yet, little evidence has been provided regarding the advantages of RP, and concerns have also arisen about the API usability of RP libraries given their disparate number of operators. In this work, we conduct a study on GitHub (GH) and Stack Overflow (SO) and explore three Reactive Extensions (Rx) libraries (RxJava, RxJS, and RxSwift) with the most GH projects to understand how much the vast Rx operators are being used. Also, we examine Rx SO posts to complement the results from the GH exploration by understanding the problems faced by RP developers and how they relate with the operators’ frequencies found in open source projects. Results reveal that, in spite of its API size, the great majority of the Rx operators are actually being used (95.2\%), with only a few, mostly related to RxJava, not being utilized. Also, we unveil 23 topics from SO with more posts concerning the Stream Abstraction (36.4\%). Posts related to Dependency Management, Introductory Questions, and iOS Development figure as relevant topics to the community. The findings herein present can not only stimulate advancements in the field by understanding the usage of RP API and the main problems faced by developers, but also help newcomers in identifying the most important operators and the areas that are the most likely to be relevant for a RP application.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Zimmerle, Carlos and Gama, Kiev and Castor, Fernando and Filho, José Murilo Mota},
	month = may,
	year = {2022},
	pages = {203--214},
	file = {Zimmerle et al. - 2022 - Mining the usage of reactive programming APIs a s.pdf:/Users/akazad/Zotero/storage/PDKIKSJS/Zimmerle et al. - 2022 - Mining the usage of reactive programming APIs a s.pdf:application/pdf},
}

@misc{kochanthara_painting_2022,
	title = {Painting the {Landscape} of {Automotive} {Software} in {GitHub}},
	url = {http://arxiv.org/abs/2203.08936},
	abstract = {The automotive industry has transitioned from being an electro-mechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook's front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing the first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing its characteristics and development styles. The landscape is defined by 15,000+ users contributing to {\textasciitilde}600 actively-developed automotive software projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics-related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe shifts along multiple dimensions, including preferred language from MATLAB to Python and prevalence of perception and decision-related software over traditional automotive software. This study witnesses the open-source automotive software boom in its infancy with many implications for future research and practice.},
	urldate = {2022-12-10},
	publisher = {arXiv},
	author = {Kochanthara, Sangeeth and Dajsuren, Yanja and Cleophas, Loek and Brand, Mark van den},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08936 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/MFYZK3W5/Kochanthara et al. - 2022 - Painting the Landscape of Automotive Software in G.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/B2GCNI77/2203.html:text/html},
}

@inproceedings{abdellatif_bothunter_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{BotHunter}: an approach to detect software bots in {GitHub}},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{BotHunter}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3527959},
	doi = {10.1145/3524842.3527959},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Abdellatif, Ahmad and Wessel, Mairieli and Steinmacher, Igor and Gerosa, Marco A. and Shihab, Emad},
	month = may,
	year = {2022},
	pages = {6--17},
	file = {Abdellatif et al. - 2022 - BotHunter an approach to detect software bots in .pdf:/Users/akazad/Zotero/storage/PXES3EZE/Abdellatif et al. - 2022 - BotHunter an approach to detect software bots in .pdf:application/pdf},
}

@inproceedings{galappaththi_does_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Does this apply to me?: an empirical study of technical context in stack overflow},
	isbn = {978-1-4503-9303-4},
	shorttitle = {Does this apply to me?},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528435},
	doi = {10.1145/3524842.3528435},
	abstract = {Stack Overflow has become an essential technical resource for developers. However, given the vast amount of knowledge available on Stack Overflow, finding the right information that is relevant for a given task is still challenging, especially when a developer is looking for a solution that applies to their specific requirements or technology stack. Clearly marking answers with their technical context, i.e., the information that characterizes the technologies and assumptions needed for this answer, is potentially one way to improve navigation. However, there is no information about how often such context is mentioned, and what kind of information it might offer. In this paper, we conduct an empirical study to understand the occurrence of technical context in Stack Overflow answers and comments, using tags as a proxy for technical context. We specifically focus on additional context, where answers/comments mention information that is not already discussed in the question. Our results show that nearly half of our studied threads contain at least one additional context. We find that almost 50\% of the additional context are either a library/framework, a programming language, a tool/application, an API, or a database. Overall, our findings show the promise of using additional context as navigational cues.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Galappaththi, Akalanka and Nadi, Sarah and Treude, Christoph},
	month = may,
	year = {2022},
	pages = {23--34},
	file = {Galappaththi et al. - 2022 - Does this apply to me an empirical study of tech.pdf:/Users/akazad/Zotero/storage/AGSYS7N5/Galappaththi et al. - 2022 - Does this apply to me an empirical study of tech.pdf:application/pdf},
}

@inproceedings{al_alamin_empirical_2021,
	address = {Madrid, Spain},
	title = {An {Empirical} {Study} of {Developer} {Discussions} on {Low}-{Code} {Software} {Development} {Challenges}},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463132/},
	doi = {10.1109/MSR52588.2021.00018},
	abstract = {Low-code software development (LCSD) is an emerging paradigm that combines minimal source code with interactive graphical interfaces to promote rapid application development. LCSD aims to democratize application development to software practitioners with diverse backgrounds. Given that LCSD is relatively a new paradigm, it is vital to learn about the challenges developers face during their adoption of LCSD platforms. The online developer forum, Stack Overﬂow (SO), is popular among software developers to ask for solutions to their technical problems. We observe a growing body of posts in SO with discussions of LCSD platforms. In this paper, we present an empirical study of around 5K SO posts (questions + accepted answers) that contain discussions of nine popular LCSD platforms. We apply topic modeling on the posts to determine the types of topics discussed. We ﬁnd 13 topics related to LCSD in SO. The 13 topics are grouped into four categories: Customization, Platform Adoption, Database Management, and Third-Party Integration. More than 40\% of the questions are about customization, i.e., developers frequently face challenges with customizing user interfaces or services offered by LCSD platforms. The topic “Dynamic Event Handling” under the “Customization” category is the most popular (in terms of average view counts per question of the topic) as well as the most difﬁcult. It means that developers frequently search for customization solutions such as how to attach dynamic events to a form in low-code UI, yet most (75.9\%) of their questions remain without an accepted answer. We manually label 900 questions from the posts to determine the prevalence of the topics’ challenges across LCSD phases. We ﬁnd that most of the questions are related to the development phase, and low-code developers also face challenges with automated testing. Our study ﬁndings offer implications for low-code practitioners, platform providers, educators, and researchers.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Al Alamin, Md Abdullah and Malakar, Sanjay and Uddin, Gias and Afroz, Sadia and Haider, Tameem Bin and Iqbal, Anindya},
	month = may,
	year = {2021},
	pages = {46--57},
	file = {Al Alamin et al. - 2021 - An Empirical Study of Developer Discussions on Low.pdf:/Users/akazad/Zotero/storage/FJW9QZBE/Al Alamin et al. - 2021 - An Empirical Study of Developer Discussions on Low.pdf:application/pdf},
}

@misc{kinsman_how_2021,
	title = {How {Do} {Software} {Developers} {Use} {GitHub} {Actions} to {Automate} {Their} {Workflows}?},
	url = {http://arxiv.org/abs/2103.12224},
	abstract = {Automated tools are frequently used in social coding repositories to perform repetitive activities that are part of the distributed software development process. Recently, GitHub introduced GitHub Actions, a feature providing automated workflows for repository maintainers. Although several Actions have been built and used by practitioners, relatively little has been done to evaluate them. Understanding and anticipating the effects of adopting such kind of technology is important for planning and management. Our research is the first to investigate how developers use Actions and how several activity indicators change after their adoption. Our results indicate that, although only a small subset of repositories adopted GitHub Actions to date, there is a positive perception of the technology. Our findings also indicate that the adoption of GitHub Actions increases the number of monthly rejected pull requests and decreases the monthly number of commits on merged pull requests. These results are especially relevant for practitioners to understand and prevent undesirable effects on their projects.},
	urldate = {2022-12-10},
	publisher = {arXiv},
	author = {Kinsman, Timothy and Wessel, Mairieli and Gerosa, Marco A. and Treude, Christoph},
	month = mar,
	year = {2021},
	note = {arXiv:2103.12224 [cs]},
	keywords = {Computer Science - Software Engineering, stat},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/VEU979V7/Kinsman et al. - 2021 - How Do Software Developers Use GitHub Actions to A.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/JHEYA399/2103.html:text/html},
}

@inproceedings{scoccia_challenges_2021,
	address = {Madrid, Spain},
	title = {Challenges in {Developing} {Desktop} {Web} {Apps}: a {Study} of {Stack} {Overflow} and {GitHub}},
	isbn = {978-1-72818-710-5},
	shorttitle = {Challenges in {Developing} {Desktop} {Web} {Apps}},
	url = {https://ieeexplore.ieee.org/document/9463138/},
	doi = {10.1109/MSR52588.2021.00039},
	abstract = {Software companies have an interest in reaching the maximum amount of potential customers while, at the same time, providing a frictionless experience. Desktop web app frameworks are promising in this respect, allowing developers and companies to reuse existing code and knowledge of web applications to create cross-platform apps integrated with native APIs. Despite their growing popularity, existing challenges in employing these technologies have not been documented, and it is hard for individuals and companies to weigh beneﬁts and pros against drawbacks and cons.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Scoccia, Gian Luca and Migliarini, Patrizio and Autili, Marco},
	month = may,
	year = {2021},
	pages = {271--282},
	file = {Scoccia et al. - 2021 - Challenges in Developing Desktop Web Apps a Study.pdf:/Users/akazad/Zotero/storage/72L39XSV/Scoccia et al. - 2021 - Challenges in Developing Desktop Web Apps a Study.pdf:application/pdf},
}

@misc{ciniselli_empirical_2021,
	title = {An {Empirical} {Study} on the {Usage} of {BERT} {Models} for {Code} {Completion}},
	url = {http://arxiv.org/abs/2103.07115},
	abstract = {Code completion is one of the main features of modern Integrated Development Environments (IDEs). Its objective is to speed up code writing by predicting the next code token(s) the developer is likely to write. Research in this area has substantially bolstered the predictive performance of these techniques. However, the support to developers is still limited to the prediction of the next few tokens to type. In this work, we take a step further in this direction by presenting a large-scale empirical study aimed at exploring the capabilities of state-of-the-art deep learning (DL) models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). To this aim, we train and test several adapted variants of the recently proposed RoBERTa model, and evaluate its predictions from several perspectives, including: (i) metrics usually adopted when assessing DL generative models (i.e., BLEU score and Levenshtein distance); (ii) the percentage of perfect predictions (i.e., the predicted code snippets that match those written by developers); and (iii) the "semantic" equivalence of the generated code as compared to the one written by developers. The achieved results show that BERT models represent a viable solution for code completion, with perfect predictions ranging from {\textasciitilde}7\%, obtained when asking the model to guess entire blocks, up to {\textasciitilde}58\%, reached in the simpler scenario of few tokens masked from the same code statement.},
	urldate = {2022-12-10},
	publisher = {arXiv},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	month = mar,
	year = {2021},
	note = {arXiv:2103.07115 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/YSXXDCIN/Ciniselli et al. - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/EML43XMQ/2103.html:text/html},
}

@inproceedings{alghamdi_characterising_2021,
	title = {Characterising the {Knowledge} about {Primitive} {Variables} in {Java} {Code} {Comments}},
	url = {http://arxiv.org/abs/2103.12291},
	doi = {10.1109/MSR52588.2021.00058},
	abstract = {Primitive types are fundamental components available in any programming language, which serve as the building blocks of data manipulation. Understanding the role of these types in source code is essential to write software. Little work has been conducted on how often these variables are documented in code comments and what types of knowledge the comments provide about variables of primitive types. In this paper, we present an approach for detecting primitive variables and their description in comments using lexical matching and advanced matching. We evaluate our approaches by comparing the lexical and advanced matching performance in terms of recall, precision, and F-score, against 600 manually annotated variables from a sample of GitHub projects. The performance of our advanced approach based on F-score was superior compared to lexical matching, 0.986 and 0.942, respectively. We then create a taxonomy of the types of knowledge contained in these comments about variables of primitive types. Our study showed that developers usually documented the variables' identifiers of a numeric data type with their purpose{\textasciitilde}(69.16\%) and concept{\textasciitilde}(72.75\%) more than the variables' identifiers of type String which were less documented with purpose{\textasciitilde}(61.14\%) and concept{\textasciitilde}(55.46\%). Our findings characterise the current state of the practice of documenting primitive variables and point at areas that are often not well documented, such as the meaning of boolean variables or the purpose of fields and local variables.},
	urldate = {2022-12-10},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Alghamdi, Mahfouth and Hayashi, Shinpei and Kobayashi, Takashi and Treude, Christoph},
	month = may,
	year = {2021},
	note = {arXiv:2103.12291 [cs]},
	keywords = {Computer Science - Software Engineering, D.2, I.2.7},
	pages = {460--470},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/6EZGH2P5/Alghamdi et al. - 2021 - Characterising the Knowledge about Primitive Varia.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/EQI8Y6CB/2103.html:text/html},
}

@misc{noauthor_notitle_nodate,
}

@inproceedings{manes_studying_2021,
	address = {Madrid, Spain},
	title = {Studying the {Change} {Histories} of {Stack} {Overflow} and {GitHub} {Snippets}},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463143/},
	doi = {10.1109/MSR52588.2021.00040},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Manes, Saraj Singh and Baysal, Olga},
	month = may,
	year = {2021},
	pages = {283--294},
	file = {Manes and Baysal - 2021 - Studying the Change Histories of Stack Overflow an.pdf:/Users/akazad/Zotero/storage/WC9U3LA5/Manes and Baysal - 2021 - Studying the Change Histories of Stack Overflow an.pdf:application/pdf},
}

@misc{sellik_learning_2021,
	title = {Learning {Off}-{By}-{One} {Mistakes}: {An} {Empirical} {Study}},
	shorttitle = {Learning {Off}-{By}-{One} {Mistakes}},
	url = {http://arxiv.org/abs/2102.12429},
	abstract = {Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., {\textless} or {\textgreater} instead of {\textless}= or {\textgreater}=. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers. While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85\% and a recall of 84\% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers.},
	urldate = {2022-12-10},
	publisher = {arXiv},
	author = {Sellik, Hendrig and van Paridon, Onno and Gousios, Georgios and Aniche, Maurício},
	month = feb,
	year = {2021},
	note = {arXiv:2102.12429 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/TPWX9IBQ/Sellik et al. - 2021 - Learning Off-By-One Mistakes An Empirical Study.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/5XBM5J7C/2102.html:text/html},
}

@inproceedings{abdellatif_challenges_2020,
	address = {Seoul Republic of Korea},
	title = {Challenges in {Chatbot} {Development}: {A} {Study} of {Stack} {Overflow} {Posts}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {Challenges in {Chatbot} {Development}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387472},
	doi = {10.1145/3379597.3387472},
	abstract = {Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad},
	month = jun,
	year = {2020},
	keywords = {2020},
	pages = {174--185},
	file = {Abdellatif et al. - 2020 - Challenges in Chatbot Development A Study of Stac.pdf:/Users/akazad/Zotero/storage/2VIMYRSA/Abdellatif et al. - 2020 - Challenges in Chatbot Development A Study of Stac.pdf:application/pdf},
}

@inproceedings{wang_empirical_2020,
	address = {Seoul Republic of Korea},
	title = {An {Empirical} {Study} on {Regular} {Expression} {Bugs}},
	isbn = {978-1-4503-7517-7},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387464},
	doi = {10.1145/3379597.3387464},
	abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice. This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3\%). The remaining root causes are incorrect API usage (9.3\%) and other code issues that require regular expression changes in the fix (29.5\%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
	month = jun,
	year = {2020},
	pages = {103--113},
	file = {Wang et al. - 2020 - An Empirical Study on Regular Expression Bugs.pdf:/Users/akazad/Zotero/storage/9MSZLJYK/Wang et al. - 2020 - An Empirical Study on Regular Expression Bugs.pdf:application/pdf},
}

@inproceedings{nakamaru_empirical_2020,
	address = {Seoul Republic of Korea},
	title = {An {Empirical} {Study} of {Method} {Chaining} in {Java}},
	isbn = {978-1-4503-7517-7},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387441},
	doi = {10.1145/3379597.3387441},
	abstract = {While some promote method chaining as a good practice for improving code readability, others refer to it as a bad practice that worsens code quality. In this paper, we first investigate whether method chaining is a programming style accepted by real-world programmers. To answer this question, we collected 2,814 Java repositories on GitHub and analyzed historical trends in the frequency of method chaining. The results of our analysis revealed the increasing use of method chaining; 23.1\% of method invocations were part of method chains in 2018, whereas only 16.0\% were such invocations in 2010. We then explore language features that are helpful to the method-chaining style but have not been supported yet in Java. For this aim, we conducted manual inspections of method chains that are randomly sampled from the collected repositories. We also estimated how effective they are to encourage the method-chaining style if they are adopted in Java.},
	language = {en},
	urldate = {2022-12-10},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Nakamaru, Tomoki and Matsunaga, Tomomasa and Yamazaki, Tetsuro and Akiyama, Soramichi and Chiba, Shigeru},
	month = jun,
	year = {2020},
	pages = {93--102},
	file = {Nakamaru et al. - 2020 - An Empirical Study of Method Chaining in Java.pdf:/Users/akazad/Zotero/storage/E45F4BNL/Nakamaru et al. - 2020 - An Empirical Study of Method Chaining in Java.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate-1,
}

@inproceedings{noauthor_our_nodate,
	title = {Our paper},
	file = {_.pdf:/Users/akazad/Zotero/storage/WHANIIWU/_.pdf:application/pdf},
}

@inproceedings{zaman_qualitative_2012,
	address = {Zurich},
	title = {A qualitative study on performance bugs},
	isbn = {978-1-4673-1761-0 978-1-4673-1760-3},
	url = {http://ieeexplore.ieee.org/document/6224281/},
	doi = {10.1109/MSR.2012.6224281},
	abstract = {Software performance is one of the important qualities that makes software stand out in a competitive market. However, in earlier work we found that performance bugs take more time to ﬁx, need to be ﬁxed by more experienced developers and require changes to more code than non-performance bugs. In order to be able to improve the resolution of performance bugs, a better understanding is needed of the current practice and shortcomings of reporting, reproducing, tracking and ﬁxing performance bugs. This paper qualitatively studies a random sample of 400 performance and non-performance bug reports of Mozilla Firefox and Google Chrome across four dimensions (Impact, Context, Fix and Fix validation). We found that developers and users face problems in reproducing performance bugs and have to spend more time discussing performance bugs than other kinds of bugs. Sometimes performance regressions are tolerated as a tradeoff to improve something else.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {2012 9th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Zaman, S. and Adams, B. and Hassan, A. E.},
	month = jun,
	year = {2012},
	pages = {199--208},
	file = {Zaman et al. - 2012 - A qualitative study on performance bugs.pdf:/Users/akazad/Zotero/storage/S6LW5KAF/Zaman et al. - 2012 - A qualitative study on performance bugs.pdf:application/pdf},
}

@inproceedings{nistor_discovering_2013,
	address = {San Francisco, CA, USA},
	title = {Discovering, reporting, and fixing performance bugs},
	isbn = {978-1-4673-2936-1 978-1-4799-0345-0},
	url = {http://ieeexplore.ieee.org/document/6624035/},
	doi = {10.1109/MSR.2013.6624035},
	abstract = {Software performance is critical for how users perceive the quality of software products. Performance bugs—programming errors that cause signiﬁcant performance degradation—lead to poor user experience and low system throughput. Designing effective techniques to address performance bugs requires a deep understanding of how performance bugs are discovered, reported, and ﬁxed. In this paper, we study how performance bugs are discovered, reported to developers, and ﬁxed by developers, and compare the results with those for non-performance bugs. We study performance and non-performance bugs from three popular code bases: Eclipse JDT, Eclipse SWT, and Mozilla. First, we ﬁnd little evidence that ﬁxing performance bugs has a higher chance to introduce new functional bugs than ﬁxing non-performance bugs, which implies that developers may not need to be overconcerned about ﬁxing performance bugs. Second, although ﬁxing performance bugs is about as error-prone as ﬁxing nonperformance bugs, ﬁxing performance bugs is more difﬁcult than ﬁxing non-performance bugs, indicating that developers need better tool support for ﬁxing performance bugs and testing performance bug patches. Third, unlike many non-performance bugs, a large percentage of performance bugs are discovered through code reasoning, not through users observing the negative effects of the bugs (e.g., performance degradation) or through proﬁling. The result suggests that techniques to help developers reason about performance, better test oracles, and better proﬁling techniques are needed for discovering performance bugs.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {2013 10th {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Nistor, Adrian and Jiang, Tian and Tan, Lin},
	month = may,
	year = {2013},
	pages = {237--246},
	file = {Nistor et al. - 2013 - Discovering, reporting, and fixing performance bug.pdf:/Users/akazad/Zotero/storage/VM34VCM5/Nistor et al. - 2013 - Discovering, reporting, and fixing performance bug.pdf:application/pdf},
}

@article{nistor_understanding_nodate,
	title = {{UNDERSTANDING}, {DETECTING}, {AND} {REPAIRING} {PERFORMANCE} {BUGS}},
	language = {en},
	author = {Nistor, Adrian},
	pages = {97},
	file = {Nistor - UNDERSTANDING, DETECTING, AND REPAIRING PERFORMANC.pdf:/Users/akazad/Zotero/storage/499CFGJJ/Nistor - UNDERSTANDING, DETECTING, AND REPAIRING PERFORMANC.pdf:application/pdf},
}

@inproceedings{he_perfsig_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{PerfSig}: extracting performance bug signatures via multi-modality causal analysis},
	isbn = {978-1-4503-9221-1},
	shorttitle = {{PerfSig}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510110},
	doi = {10.1145/3510003.3510110},
	abstract = {Diagnosing a performance bug triggered in production cloud environments is notoriously challenging. Extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs. In this paper, we present PerfSig, a multi-modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs. PerfSig performs fine-grained anomaly detection over various machine data such as system metrics, system logs, and function call traces. We then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug. PerfSig generates bug signatures as the combination of the identified anomaly patterns and root cause functions. We have implemented a prototype of PerfSig and conducted evaluation using 20 real world performance bugs in six commonly used cloud systems. Our experimental results show that PerfSig captures various kinds of fine-grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi-modality causal analysis for 19 out of 20 tested performance bugs.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {He, Jingzhu and Lin, Yuhang and Gu, Xiaohui and Yeh, Chin-Chia Michael and Zhuang, Zhongfang},
	month = may,
	year = {2022},
	pages = {1669--1680},
	file = {He et al. - 2022 - PerfSig extracting performance bug signatures via.pdf:/Users/akazad/Zotero/storage/L5UVRC3A/He et al. - 2022 - PerfSig extracting performance bug signatures via.pdf:application/pdf},
}

@incollection{wang_assessment_2019,
	address = {Singapore},
	title = {On the {Assessment} of {Security} and {Performance} {Bugs} in {Chromium} {Open}-{Source} {Project}},
	volume = {1123},
	isbn = {9789811513039 9789811513046},
	url = {http://link.springer.com/10.1007/978-981-15-1304-6_12},
	abstract = {An individual working in software development should have a fundamental understanding of how different types of bugs impact various project aspects. This knowledge allows one to improve the quality of the created software. The problem, however, is that previous research typically treats all types of bugs as similar when analyzing several aspects of software quality (e.g. predicting the time to ﬁx a bug) or concentrates on a particular bug type (e.g. performance bugs) with little comparison to other types. In this paper, we look at how different types of bugs, speciﬁcally performance and security bugs, differ from one another. Our study is based on a previous study done by Zaman et al. [1] in which the study was performed on the FireFox project. Instead of Firefox, we will be working with the Chrome web-browser. In our case study, we ﬁnd that security bugs are ﬁxed faster than performance bugs and that the developers who were assigned to security bugs typically have more experience than the ones assigned to performance bugs. Our ﬁndings emphasize the importance of considering the different types of bugs in software quality research and practice.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {Dependability in {Sensor}, {Cloud}, and {Big} {Data} {Systems} and {Applications}},
	publisher = {Springer Singapore},
	author = {Imseis, Joseph and Nachuma, Costain and Arifuzzaman, Shaikh and Zibran, Minhaz and Bhuiyan, Zakirul Alam},
	editor = {Wang, Guojun and Bhuiyan, Md Zakirul Alam and De Capitani di Vimercati, Sabrina and Ren, Yizhi},
	year = {2019},
	doi = {10.1007/978-981-15-1304-6_12},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {145--157},
	file = {Imseis et al. - 2019 - On the Assessment of Security and Performance Bugs.pdf:/Users/akazad/Zotero/storage/XCP6UYJF/Imseis et al. - 2019 - On the Assessment of Security and Performance Bugs.pdf:application/pdf},
}

@article{makkouk_empirical_nodate,
	title = {An {Empirical} {Study} on {Performance} {Bugs} in {Deep} {Learning} {Frameworks}},
	abstract = {Machine Learning (ML) and Deep Learning (DL) applications are becoming more popular due to the availability of DL frameworks such as TensorFlow and PyTorch. Therefore, the quality of DL frameworks is essential to ensure DL/ML application quality. Given the computationally expensive nature of DL tasks (e.g., training), performance is a critical aspect of DL frameworks. However, optimizing DL frameworks may have its own unique challenges due to the peculiarities of DL (e.g., hardware integration and the nature of the computation). In this paper, we conduct an empirical study on the performance bugs in DL frameworks. We conduct our study on TensorFlow and PyTorch by identifying the performance and non-performance bugs by mining the GitHub repositories. We ﬁnd that 1) the proportion of newly reported performance bugs increases faster than ﬁxed performance bugs, and the ratio of performance bugs among all bugs increases over time; 2) performance bugs take more time to ﬁx, have larger ﬁx sizes, and more community engagement (e.g., discussion) compared to non-performance bugs; and 3) we manually derived a taxonomy of 12 categories and 19 sub-categories of the root causes of performance bugs by studying all performance bug ﬁxes. Finally, we present some actionable implications for researchers and developers.},
	language = {en},
	author = {Makkouk, Tarek and Kim, Dong Jae},
	pages = {12},
	file = {Makkouk and Kim - An Empirical Study on Performance Bugs in Deep Lea.pdf:/Users/akazad/Zotero/storage/WN2ER6DZ/Makkouk and Kim - An Empirical Study on Performance Bugs in Deep Lea.pdf:application/pdf},
}

@inproceedings{wan_bug_2017,
	address = {Buenos Aires, Argentina},
	title = {Bug {Characteristics} in {Blockchain} {Systems}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-5386-1544-7},
	shorttitle = {Bug {Characteristics} in {Blockchain} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7962390/},
	doi = {10.1109/MSR.2017.59},
	abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug ﬁxing time. The ﬁndings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be ﬁxed; (4) 35.71\% performance bugs are ﬁxed in more than one year; performance bugs take the longest average time to be ﬁxed.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
	month = may,
	year = {2017},
	pages = {413--424},
	file = {Wan et al. - 2017 - Bug Characteristics in Blockchain Systems A Large.pdf:/Users/akazad/Zotero/storage/T9D9DIB5/Wan et al. - 2017 - Bug Characteristics in Blockchain Systems A Large.pdf:application/pdf},
}

@inproceedings{kamienski_pysstubs_2021,
	address = {Madrid, Spain},
	title = {{PySStuBs}: {Characterizing} {Single}-{Statement} {Bugs} in {Popular} {Open}-{Source} {Python} {Projects}},
	isbn = {978-1-72818-710-5},
	shorttitle = {{PySStuBs}},
	url = {https://ieeexplore.ieee.org/document/9463149/},
	doi = {10.1109/MSR52588.2021.00066},
	abstract = {Single-statement bugs (SStuBs) can have a severe impact on developer productivity. Despite usually being simple and not offering much of a challenge to ﬁx, these bugs may still disturb a developer’s workﬂow and waste precious development time. However, few studies have paid attention to these simple bugs, focusing instead on bugs of any size and complexity. In this study, we explore the occurrence of SStuBs in some of the most popular open-source Python projects on GitHub, while also characterizing their patterns and distribution. We further compare these bugs to SStuBs found in a previous study on Java Maven projects. We ﬁnd that these Python projects have different SStuB patterns than the ones in Java Maven projects and identify 7 new SStuB patterns. Our results may help uncover the importance of understanding these bugs for the Python programming language, and how developers can handle them more effectively.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Kamienski, Arthur V. and Palechor, Luisa and Bezemer, Cor-Paul and Hindle, Abram},
	month = may,
	year = {2021},
	pages = {520--524},
	file = {Kamienski et al. - 2021 - PySStuBs Characterizing Single-Statement Bugs in .pdf:/Users/akazad/Zotero/storage/M64G7SSB/Kamienski et al. - 2021 - PySStuBs Characterizing Single-Statement Bugs in .pdf:application/pdf},
}

@inproceedings{long_reporting_2022-1,
	address = {Gothenburg Sweden},
	title = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}: {An} {Exploratory} {Study} from {GitHub}},
	isbn = {978-1-4503-9613-4},
	shorttitle = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}},
	url = {https://dl.acm.org/doi/10.1145/3530019.3530029},
	doi = {10.1145/3530019.3530029},
	abstract = {The tremendous success of Deep Learning (DL) has significantly boosted the number of open-sourced DL frameworks hosted on GitHub. Among others, performance and accuracy bugs are critical factors that affect the reputation of these DL frameworks, therefore understanding the practice of discovering and investigating them for DL is important. In this paper, we conduct an exploratory study on the nature of reporting performance and accuracy bugs for DL frameworks, aiming to improve our knowledge on this topic. Our study covers 10 most popular open-sourced DL frameworks on GitHub (e.g., TensorFlow, Keras, and PyTorch), based on which we sample 664 representative performance and accuracy bug reports out of a total population of 22,522. Through systematic analysis, we found that: (1) low speed is the primary reason that a performance bug related report is submitted but we see no consistent pattern for accuracy related ones; (2) most of the reports are about issues encountered in the training stage; (3) only a small proportion of the reports provide insufficient information to investigate; (4) the majority of the performance and accuracy bug reports (from 69\% to 100\%) are not related to the actual bug or regarded as unclassified; (5) around 50\% of the performance and accuracy bug reports, which indeed reveal bugs, are not resolved by direct patches. Deriving from the above, we discuss a set of actionable implications to the researchers, maintainers, and report submitters. To promote open science, the labeled dataset has been made publicly available at https://zenodo.org/record/6371676.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {The {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2022},
	publisher = {ACM},
	author = {Long, Guoming and Chen, Tao},
	month = jun,
	year = {2022},
	pages = {90--99},
	file = {Long and Chen - 2022 - On Reporting Performance and Accuracy Bugs for Dee.pdf:/Users/akazad/Zotero/storage/5L924FPE/Long and Chen - 2022 - On Reporting Performance and Accuracy Bugs for Dee.pdf:application/pdf},
}

@inproceedings{long_reporting_2022-2,
	address = {Gothenburg Sweden},
	title = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}: {An} {Exploratory} {Study} from {GitHub}},
	isbn = {978-1-4503-9613-4},
	shorttitle = {On {Reporting} {Performance} and {Accuracy} {Bugs} for {Deep} {Learning} {Frameworks}},
	url = {https://dl.acm.org/doi/10.1145/3530019.3530029},
	doi = {10.1145/3530019.3530029},
	abstract = {The tremendous success of Deep Learning (DL) has significantly boosted the number of open-sourced DL frameworks hosted on GitHub. Among others, performance and accuracy bugs are critical factors that affect the reputation of these DL frameworks, therefore understanding the practice of discovering and investigating them for DL is important. In this paper, we conduct an exploratory study on the nature of reporting performance and accuracy bugs for DL frameworks, aiming to improve our knowledge on this topic. Our study covers 10 most popular open-sourced DL frameworks on GitHub (e.g., TensorFlow, Keras, and PyTorch), based on which we sample 664 representative performance and accuracy bug reports out of a total population of 22,522. Through systematic analysis, we found that: (1) low speed is the primary reason that a performance bug related report is submitted but we see no consistent pattern for accuracy related ones; (2) most of the reports are about issues encountered in the training stage; (3) only a small proportion of the reports provide insufficient information to investigate; (4) the majority of the performance and accuracy bug reports (from 69\% to 100\%) are not related to the actual bug or regarded as unclassified; (5) around 50\% of the performance and accuracy bug reports, which indeed reveal bugs, are not resolved by direct patches. Deriving from the above, we discuss a set of actionable implications to the researchers, maintainers, and report submitters. To promote open science, the labeled dataset has been made publicly available at https://zenodo.org/record/6371676.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {The {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2022},
	publisher = {ACM},
	author = {Long, Guoming and Chen, Tao},
	month = jun,
	year = {2022},
	pages = {90--99},
	file = {Long and Chen - 2022 - On Reporting Performance and Accuracy Bugs for Dee.pdf:/Users/akazad/Zotero/storage/AZ9U66LL/Long and Chen - 2022 - On Reporting Performance and Accuracy Bugs for Dee.pdf:application/pdf},
}

@inproceedings{chen_inferring_2019,
	address = {Berlin, Germany},
	title = {Inferring {Performance} {Bug} {Patterns} from {Developer} {Commits}},
	isbn = {978-1-72814-982-0},
	url = {https://ieeexplore.ieee.org/document/8987574/},
	doi = {10.1109/ISSRE.2019.00017},
	abstract = {Performance bugs, i.e., program source code that is unnecessarily inefﬁcient, have received signiﬁcant attention by the research community in recent years. A number of empirical studies have investigated how these bugs differ from “ordinary” bugs that cause functional deviations and several approaches to aid their detection, localization, and removal have been proposed. Many of these approaches focus on certain subclasses of performance bugs, e.g., those resulting from redundant computations or unnecessary synchronization, and the evaluation of their effectiveness is usually limited to a small number of known instances of these bugs. To provide researchers working on performance bug detection and localization techniques with a larger corpus of performance bugs to evaluate against, we conduct a study of more than 700 performance bug ﬁxing commits across 13 popular open source projects written in C and C++ and investigate the relative frequency of bug types as well as their complexity. Our results show that many of these ﬁxes follow a small set of bug patterns, that they are contributed by experienced developers, and that the number of lines needed to ﬁx performance bugs is highly project dependent.},
	language = {en},
	urldate = {2022-12-11},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Chen, Yiqun and Winter, Stefan and Suri, Neeraj},
	month = oct,
	year = {2019},
	pages = {70--81},
	file = {Chen et al. - 2019 - Inferring Performance Bug Patterns from Developer .pdf:/Users/akazad/Zotero/storage/NCNB9RG7/Chen et al. - 2019 - Inferring Performance Bug Patterns from Developer .pdf:application/pdf},
}

@inproceedings{dey_representation_2021,
	title = {Representation of {Developer} {Expertise} in {Open} {Source} {Software}},
	doi = {10.1109/ICSE43902.2021.00094},
	abstract = {Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Dey, Tapajit and Karnauch, Andrey and Mockus, Audris},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Software engineering, Ecosystems, Open source software, Task analysis, Expertise, Developer Expertise, Vector Embedding, Doc2Vec, API, API embedding, Project embedding, Developer embedding, Skill Space, Machine Learning, Open Source, World of Code, Software measurement, Topology, Trajectory},
	pages = {995--1007},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/YZC4U6TS/9401957.html:text/html;Submitted Version:/Users/akazad/Zotero/storage/GBX63PMA/Dey et al. - 2021 - Representation of Developer Expertise in Open Sour.pdf:application/pdf},
}

@misc{noauthor_gensim_nodate,
	title = {Gensim - {Doc2Vec} {Model}},
	url = {https://www.tutorialspoint.com/gensim/gensim_doc2vec_model.htm},
	urldate = {2022-12-19},
	file = {Gensim - Doc2Vec Model:/Users/akazad/Zotero/storage/U6GID4ME/gensim_doc2vec_model.html:text/html;Gensim - Doc2Vec Model:/Users/akazad/Zotero/storage/BFJY7L76/gensim_doc2vec_model.html:text/html},
}

@inproceedings{gallaba_dont_2015,
	address = {Beijing, China},
	title = {Don't {Call} {Us}, {We}'ll {Call} {You}: {Characterizing} {Callbacks} in {Javascript}},
	isbn = {978-1-4673-7899-4},
	shorttitle = {Don't {Call} {Us}, {We}'ll {Call} {You}},
	url = {http://ieeexplore.ieee.org/document/7321196/},
	doi = {10.1109/ESEM.2015.7321196},
	abstract = {JavaScript is a popular language for developing web applications and is increasingly used for both client-side and server-side application logic. The JavaScript runtime is inherently event-driven and callbacks are a key language feature. Unfortunately, callbacks induce a non-linear control ﬂow and can be deferred to execute asynchronously, declared anonymously, and may be nested to arbitrary levels. All of these features make callbacks difﬁcult to understand and maintain. We perform an empirical study to characterize JavaScript callback usage across a representative corpus of 138 JavaScript programs, with over 5 million lines of JavaScript code. We ﬁnd that on average, every 10th function deﬁnition takes a callback argument, and that over 43\% of all callback-accepting function callsites are anonymous. Furthermore, the majority of callbacks are nested, more than half of all callbacks are asynchronous, and asynchronous callbacks, on average, appear more frequently in client-side code (72\%) than server-side (55\%). We also study three well-known solutions designed to help with the complexities associated with callbacks, including the errorﬁrst callback convention, Async.js library, and Promises. Our results inform the design of future JavaScript analysis and code comprehension tools.},
	language = {en},
	urldate = {2022-12-25},
	booktitle = {2015 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {IEEE},
	author = {Gallaba, Keheliya and Mesbah, Ali and Beschastnikh, Ivan},
	month = oct,
	year = {2015},
	pages = {1--10},
	file = {Gallaba et al. - 2015 - Don't Call Us, We'll Call You Characterizing Call.pdf:/Users/akazad/Zotero/storage/3MYMZEUG/Gallaba et al. - 2015 - Don't Call Us, We'll Call You Characterizing Call.pdf:application/pdf},
}

@inproceedings{fang_discovering_2014,
	address = {Torino, Italy},
	title = {Discovering buffer overflow vulnerabilities in the wild: an empirical study},
	isbn = {978-1-4503-2774-9},
	shorttitle = {Discovering buffer overflow vulnerabilities in the wild},
	url = {http://dl.acm.org/citation.cfm?doid=2652524.2652533},
	doi = {10.1145/2652524.2652533},
	abstract = {Reporters of security vulnerabilities possess rich Context: information about the security engineering process. Goal: We performed an empirical study on reporters of bu↵er overﬂow vulnerabilities to understand the methods and tools used during the discovery. Method: We ran the study in the form of an email questionnaire with open ended questions. The participants were reporters featured in the SecurityFocus repository during two six-month periods; we collected 58 responses.},
	language = {en},
	urldate = {2022-12-25},
	booktitle = {Proceedings of the 8th {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} - {ESEM} '14},
	publisher = {ACM Press},
	author = {Fang, Ming and Hafiz, Munawar},
	year = {2014},
	pages = {1--10},
	file = {Fang and Hafiz - 2014 - Discovering buffer overflow vulnerabilities in the.pdf:/Users/akazad/Zotero/storage/RW4YX2UI/Fang and Hafiz - 2014 - Discovering buffer overflow vulnerabilities in the.pdf:application/pdf},
}

@inproceedings{caivano_exploratory_2021,
	address = {Bari Italy},
	title = {An {Exploratory} {Study} on {Dead} {Methods} in {Open}-source {Java} {Desktop} {Applications}},
	isbn = {978-1-4503-8665-4},
	url = {https://dl.acm.org/doi/10.1145/3475716.3475773},
	doi = {10.1145/3475716.3475773},
	abstract = {Background. Dead code is a code smell. It can refer to code blocks, fields, methods, etc. that are unused and/or unreachable. Empirical evidence shows that dead code harms source code comprehensibility and maintainability in software applications. Researchers have gathered little empirical evidence on the spread of dead code in software applications. Moreover, we know little about the role of this code smell during software evolution. Aims. Our goal is to gather preliminary empirical evidence on the spread and evolution of dead methods in open-source Java desktop applications. Given the exploratory nature of our investigation, we believe that its results can justify more resource- and timedemanding research on dead methods.
Method. We quantitatively analyzed the commit histories of 13 open-source Java desktop applications, whose software projects were hosted on GitHub, for a total of 1,044 commits. We focused on dead methods detected at a commit level to investigate the spread and evolution of dead methods in the studied applications. The perspective of our explorative study is that of both practitioners and researchers.
Results. The most important take-away results can be summarized as follows: (i) dead methods seems to affect open-source Java desktop applications; (ii) dead methods generally survive for a long time, in terms of commits, before being “buried” or “revived;” (iii) dead methods are rarely revived; and (iv) most dead methods are dead since the creation of the corresponding methods.
Conclusions. We conclude that developers should carefully handle dead methods in open-source Java desktop applications since this code smell is harmful, widespread, rarely revived, and survives for a long time in software applications. Our results also justify future research on dead methods.},
	language = {en},
	urldate = {2022-12-25},
	booktitle = {Proceedings of the 15th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {ACM},
	author = {Caivano, Danilo and Cassieri, Pietro and Romano, Simone and Scanniello, Giuseppe},
	month = oct,
	year = {2021},
	pages = {1--11},
	file = {Caivano et al. - 2021 - An Exploratory Study on Dead Methods in Open-sourc.pdf:/Users/akazad/Zotero/storage/S2Y2CCBN/Caivano et al. - 2021 - An Exploratory Study on Dead Methods in Open-sourc.pdf:application/pdf},
}

@inproceedings{shaw_writing_2003,
	address = {Portland, OR, USA},
	title = {Writing good software engineering research papers},
	isbn = {978-0-7695-1877-0},
	url = {http://ieeexplore.ieee.org/document/1201262/},
	doi = {10.1109/ICSE.2003.1201262},
	abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to ICSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
	language = {en},
	urldate = {2022-12-25},
	booktitle = {25th {International} {Conference} on {Software} {Engineering}, 2003. {Proceedings}.},
	publisher = {IEEE},
	author = {Shaw, M.},
	year = {2003},
	pages = {726--736},
	file = {Shaw - 2003 - Writing good software engineering research papers.pdf:/Users/akazad/Zotero/storage/X4WHEE3K/Shaw - 2003 - Writing good software engineering research papers.pdf:application/pdf},
}

@inproceedings{munoz_baron_empirical_2020,
	address = {Bari Italy},
	title = {An {Empirical} {Validation} of {Cognitive} {Complexity} as a {Measure} of {Source} {Code} {Understandability}},
	isbn = {978-1-4503-7580-1},
	url = {https://dl.acm.org/doi/10.1145/3382494.3410636},
	doi = {10.1145/3382494.3410636},
	abstract = {Background: Developers spend a lot of their time on understanding source code. Static code analysis tools can draw attention to code that is difficult for developers to understand. However, most of the findings are based on non-validated metrics, which can lead to confusion and code, that is hard to understand, not being identified. Aims: In this work, we validate a metric called Cognitive Complexity which was explicitly designed to measure code understandability and which is already widely used due to its integration in wellknown static code analysis tools.
Method: We conducted a systematic literature search to obtain data sets from studies which measured code understandability. This way we obtained about 24,000 understandability evaluations of 427 code snippets. We calculated the correlations of these measurements with the corresponding metric values and statistically summarized the correlation coefficients through a meta-analysis.
Results: Cognitive Complexity positively correlates with comprehension time and subjective ratings of understandability. The metric showed mixed results for the correlation with the correctness of comprehension tasks and with physiological measures.
Conclusions: It is the first validated and solely code-based metric which is able to reflect at least some aspects of code understandability. Moreover, due to its methodology, this work shows that code understanding is currently measured in many different ways, which we also do not know how they are related. This makes it difficult to compare the results of individual studies as well as to develop a metric that measures code understanding in all its facets.},
	language = {en},
	urldate = {2022-12-25},
	booktitle = {Proceedings of the 14th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {ACM},
	author = {Muñoz Barón, Marvin and Wyrich, Marvin and Wagner, Stefan},
	month = oct,
	year = {2020},
	pages = {1--12},
	file = {Muñoz Barón et al. - 2020 - An Empirical Validation of Cognitive Complexity as.pdf:/Users/akazad/Zotero/storage/3W7ATLA5/2007.12520.pdf:application/pdf},
}

@article{perry_empirical_nodate,
	title = {Empirical {Studies} of {Software} {Engineering}: {A} {Roadmap}},
	abstract = {In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.},
	language = {en},
	author = {Perry, Dewayne E and Porter, Adam A and Votta, Lawrence G},
	file = {Perry et al. - Empirical Studies of Software Engineering A Roadm.pdf:/Users/akazad/Zotero/storage/6MCUKKLL/Perry et al. - Empirical Studies of Software Engineering A Roadm.pdf:application/pdf},
}

@inproceedings{pascarella_classifying_2017,
	address = {Buenos Aires, Argentina},
	title = {Classifying {Code} {Comments} in {Java} {Open}-{Source} {Software} {Systems}},
	isbn = {978-1-5386-1544-7},
	url = {http://ieeexplore.ieee.org/document/7962372/},
	doi = {10.1109/MSR.2017.63},
	abstract = {Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classiﬁcation is within reach.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Pascarella, Luca and Bacchelli, Alberto},
	month = may,
	year = {2017},
	pages = {227--237},
	file = {Pascarella and Bacchelli - 2017 - Classifying Code Comments in Java Open-Source Soft.pdf:/Users/akazad/Zotero/storage/RLYYSKFA/Pascarella and Bacchelli - 2017 - Classifying Code Comments in Java Open-Source Soft.pdf:application/pdf},
}

@inproceedings{asaduzzaman_answering_2013,
	address = {San Francisco, CA, USA},
	title = {Answering questions about unanswered questions of {Stack} {Overflow}},
	isbn = {978-1-4673-2936-1 978-1-4799-0345-0},
	url = {http://ieeexplore.ieee.org/document/6624015/},
	doi = {10.1109/MSR.2013.6624015},
	abstract = {Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overﬂow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overﬂow are answered in a very short time. However, we found that the number of unanswered questions has increased signiﬁcantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overﬂow services. In this paper, we mine data on unanswered questions from Stack Overﬂow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difﬁcult to ﬁnd otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overﬂow.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {2013 10th {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Asaduzzaman, Muhammad and Mashiyat, Ahmed Shah and Roy, Chanchal K. and Schneider, Kevin A.},
	month = may,
	year = {2013},
	pages = {97--100},
	file = {Asaduzzaman et al. - 2013 - Answering questions about unanswered questions of .pdf:/Users/akazad/Zotero/storage/V3ZXZAJX/Asaduzzaman et al. - 2013 - Answering questions about unanswered questions of .pdf:application/pdf},
}

@inproceedings{mondal_can_2019,
	address = {Montreal, QC, Canada},
	title = {Can {Issues} {Reported} at {Stack} {Overflow} {Questions} be {Reproduced}? {An} {Exploratory} {Study}},
	isbn = {978-1-72813-412-3},
	shorttitle = {Can {Issues} {Reported} at {Stack} {Overflow} {Questions} be {Reproduced}?},
	url = {https://ieeexplore.ieee.org/document/8816784/},
	doi = {10.1109/MSR.2019.00074},
	abstract = {Software developers often look for solutions to their code level problems at Stack Overﬂow. Hence, they frequently submit their questions with sample code segments and issue descriptions. Unfortunately, it is not always possible to reproduce their reported issues from such code segments. This phenomenon might prevent their questions from getting prompt and appropriate solutions. In this paper, we report an exploratory study on the reproducibility of the issues discussed in 400 questions of Stack Overﬂow. In particular, we parse, compile, execute and even carefully examine the code segments from these questions, spent a total of 200 man hours, and then attempt to reproduce their programming issues. The outcomes of our study are two-fold. First, we ﬁnd that 68\% of the code segments require minor and major modiﬁcations in order to reproduce the issues reported by the developers. On the contrary, 22\% code segments completely fail to reproduce the issues. We also carefully investigate why these issues could not be reproduced and then provide evidence-based guidelines for writing effective code examples for Stack Overﬂow questions. Second, we investigate the correlation between issue reproducibility status (of questions) and corresponding answer meta-data such as the presence of an accepted answer. According to our analysis, a question with reproducible issues has at least three times higher chance of receiving an accepted answer than the question with irreproducible issues.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Mondal, Saikat and Rahman, Mohammad Masudur and Roy, Chanchal K.},
	month = may,
	year = {2019},
	pages = {479--489},
	file = {Mondal et al. - 2019 - Can Issues Reported at Stack Overflow Questions be.pdf:/Users/akazad/Zotero/storage/BI6CWC68/Mondal et al. - 2019 - Can Issues Reported at Stack Overflow Questions be.pdf:application/pdf},
}

@article{han_study_2021,
	title = {A {Study} of {Performance} {Testing} in {Configurable} {Software} {Systems}},
	volume = {14},
	issn = {1945-3116, 1945-3124},
	url = {https://www.scirp.org/journal/doi.aspx?doi=10.4236/jsea.2021.149028},
	doi = {10.4236/jsea.2021.149028},
	abstract = {Customizing applications through program configuration options has been proved by many open-source and commercial projects as one of the best practices in software engineering. However, traditional performance testing is not in synch with this industrial practice. Traditional performance testing techniques consider program inputs as the only external factor. It ignores the performance influence of configuration options. This study aims to stimulate research interest in performance testing in the context of configurable software systems by answering three research questions. That is, why it is necessary to conduct research in performance testing, what are the state-of-the-art techniques, and how do we conduct performance testing research in configurable software systems. In this study, we examine the unique characteristics and challenges of performance testing research in configurable software systems. We review and discuss research topics on the performance bug study, performance anti-patterns, program analysis, and performance testing. We share the research findings from the empirical study and outline the opening opportunities for new and advanced researchers to contribute to the research community.},
	language = {en},
	number = {09},
	urldate = {2022-12-28},
	journal = {Journal of Software Engineering and Applications},
	author = {Han, Xue},
	year = {2021},
	pages = {474--492},
	file = {Han - 2021 - A Study of Performance Testing in Configurable Sof.pdf:/Users/akazad/Zotero/storage/7JGLWWFD/jsea_2021091016145712.pdf:application/pdf},
}

@article{rodeghero_empirical_2016,
	title = {An empirical study on how expert knowledge affects bug reports: {Expert} {Knowledge} {Affects} {Bug} {Reports}},
	volume = {28},
	issn = {20477473},
	shorttitle = {An empirical study on how expert knowledge affects bug reports},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/smr.1773},
	doi = {10.1002/smr.1773},
	abstract = {Bug reports are crucial software artifacts for both software maintenance researchers and practitioners. A typical use of bug reports by researchers is to evaluate automated software maintenance tools: a large repository of reports is used as input for a tool, and metrics are calculated from the tool’s output. But this process is quite different from practitioners, who distinguish between reports written by experts, such as programmers, and reports written by non-experts, such as users. Practitioners recognize that the content of a bug report depends on its author’s expert knowledge. In this paper, we present an empirical study of the textual difference between bug reports written by experts and non-experts. We ﬁnd that a signiﬁcant difference exists, and that this difference has a signiﬁcant impact on the results from a state-of-the-art feature location tool. Through an additional study, we also found no evidence that these encountered differences were caused by the increased usage of terms from the source code in the expert bug reports. Our recommendation is that researchers evaluate maintenance tools using different sets of bug reports for experts and non-experts. Copyright c 0000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2022-12-28},
	journal = {Journal of Software: Evolution and Process},
	author = {Rodeghero, Paige and Huo, Da and Ding, Tao and McMillan, Collin and Gethers, Malcom},
	month = jul,
	year = {2016},
	pages = {542--564},
	file = {Rodeghero et al. - 2016 - An empirical study on how expert knowledge affects.pdf:/Users/akazad/Zotero/storage/XK7GRS92/Rodeghero et al. - 2016 - An empirical study on how expert knowledge affects.pdf:application/pdf},
}

@inproceedings{de_f_farias_systematic_2016,
	address = {Pisa Italy},
	title = {A systematic mapping study on mining software repositories},
	isbn = {978-1-4503-3739-7},
	url = {https://dl.acm.org/doi/10.1145/2851613.2851786},
	doi = {10.1145/2851613.2851786},
	abstract = {Background: Software repositories provide large amount of data encompassing software changes throughout its evolution. Those repositories can be effectively used to extract and analyze pertinent information and derive conclusions related to the software history or its current snapshot. Objective: This work aims to investigate recent studies on Mining Software Repositories (MSR) approaches collecting evidences about software analysis goals (purpose, focus, and object of analysis), data sources, evaluation methods, tools, and how the area is evolving.
Method: A systematic mapping study was performed to identify and analyze research on mining software repositories by analyzing five editions of Working Conference on Mining Software Repositories – the main conference on this area. Results: MSR approaches have been used for many different goals, mainly for comprehension of defects, analysis of the contribution and behavior of developers, and software evolution comprehension. Besides, some gaps were identified with respect to their goals, focus, and data source type (e.g. lack of usage of comments to identify smells, refactoring, and issues of software quality). Regarding the evaluation method, our analysis pointed out to an extensive usage of some types of empirical evaluation.
Conclusion: Studies of the MSR have focused on different goals, however there are still many research opportunities to be explored and issues associated with MSR that should be considered.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {de F. Farias, Mário André and Novais, Renato and Júnior, Methanias Colaço and da Silva Carvalho, Luís Paulo and Mendonça, Manoel and Spínola, Rodrigo Oliveira},
	month = apr,
	year = {2016},
	pages = {1472--1479},
	file = {de F. Farias et al. - 2016 - A systematic mapping study on mining software repo.pdf:/Users/akazad/Zotero/storage/3EDB4GNK/de F. Farias et al. - 2016 - A systematic mapping study on mining software repo.pdf:application/pdf},
}

@inproceedings{macho_extracting_2017,
	address = {Buenos Aires, Argentina},
	title = {Extracting {Build} {Changes} with {BUILDDIFF}},
	isbn = {978-1-5386-1544-7},
	url = {http://ieeexplore.ieee.org/document/7962386/},
	doi = {10.1109/MSR.2017.65},
	abstract = {Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build conﬁgurations and reasons for build breakage, but they did this only on a coarse grained level.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Macho, Christian and McIntosh, Shane and Pinzger, Martin},
	month = may,
	year = {2017},
	pages = {368--378},
	file = {Macho et al. - 2017 - Extracting Build Changes with BUILDDIFF.pdf:/Users/akazad/Zotero/storage/FEQLE8I9/Macho et al. - 2017 - Extracting Build Changes with BUILDDIFF.pdf:application/pdf},
}

@article{kagdi_towards_nodate,
	title = {Towards a {Taxonomy} of {Approaches} for {Mining} of {Source} {Code} {Repositories}},
	abstract = {Source code version repositories provide a treasure of information encompassing the changes introduced in the system throughout its evolution. These repositories are typically managed by tools such as CVS. However, these tools identify and express changes in terms of physical attributes i.e., file and line numbers. Recently, to help support the mining of software repositories (MSR), researchers have proposed methods to derive and express changes from source code repositories in a more source-code “aware” manner (i.e., syntax and semantic). Here, we discuss these MSR techniques in light of what changes are identified, how they are expressed, the adopted methodology, evaluation, and results. This work forms the basis for a taxonomic description of MSR approaches.},
	language = {en},
	author = {Kagdi, Huzefa and Collard, Michael L and Maletic, Jonathan I},
	file = {Kagdi et al. - Towards a Taxonomy of Approaches for Mining of Sou.pdf:/Users/akazad/Zotero/storage/QNY8Q97T/Kagdi et al. - Towards a Taxonomy of Approaches for Mining of Sou.pdf:application/pdf},
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0164121222000103?token=21EFB0814EC1C9552DD8B87B6C4E3F37097D93617A51A5316DBAD432EDBAC37E4654BD225C0308CCBC7D431C0FDE49BF&originRegion=us-east-1&originCreation=20221228213109},
	language = {en},
	urldate = {2022-12-28},
	doi = {10.1016/j.jss.2022.111233},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/9NY3RBBQ/Elsevier Enhanced Reader.pdf:application/pdf},
}

@inproceedings{das_empirical_2022,
	address = {Gothenburg Sweden},
	title = {An {Empirical} {Study} of {Blockchain} {Repositories} in {GitHub}},
	isbn = {978-1-4503-9613-4},
	url = {https://dl.acm.org/doi/10.1145/3530019.3530041},
	doi = {10.1145/3530019.3530041},
	abstract = {Blockchain is a distributed ledger technique that guarantees the traceability of transactions. Blockchain is adopted in multiple domains like finance (e.g., cryptocurrency), healthcare, security, and supply chain. In the open-source software (OSS) portal GitHub, we observe a growing adoption of Blockchain-based solutions. Given the rapid emergence of Blockchain-based solutions in our daily life and the evolving cryptocurrency market, it is important to know the status quo, how developers generally interact in those repos, and how much freedom they have in applying code changes. We report an empirical study of 3,664 Blockchain software repositories from GitHub. We divide the Blockchain repositories into two categories: Tool (e.g., SDKs) and Applications (e.g., service/solutions developed using SDKs). The Application category is further divided into two sub-categories: Crypto and Non-Crypto applications. In all Blockchain repository categories, the contribution interactions on commits are the most common interaction type. We found that more organizations contributing to the Blockchain repos than individual users. The median numbers of internal and external users in tools are higher than the application repos. We observed a higher degree of collaboration (e.g., for maintenance efforts) among users in Blockchain tools than those in the application repos. Among the artifacts, issues have a greater number of interactions than commits and pull requests. Related to autonomy we found that less than half of total project contributions are autonomous. Our findings offer implications to Blockchain stakeholders, like developers to stay aware of OSS practices around Blockchain software.},
	language = {en},
	urldate = {2022-12-28},
	booktitle = {The {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2022},
	publisher = {ACM},
	author = {Das, Ajoy and Uddin, Gias and Ruhe, Guenther},
	month = jun,
	year = {2022},
	pages = {211--220},
	file = {Das et al. - 2022 - An Empirical Study of Blockchain Repositories in G.pdf:/Users/akazad/Zotero/storage/6WDRNZCR/Das et al. - 2022 - An Empirical Study of Blockchain Repositories in G.pdf:application/pdf},
}

@misc{montandon_identifying_2019,
	title = {Identifying {Experts} in {Software} {Libraries} and {Frameworks} among {GitHub} {Users}},
	url = {http://arxiv.org/abs/1903.08113},
	abstract = {Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.},
	urldate = {2022-12-31},
	publisher = {arXiv},
	author = {Montandon, Joao Eduardo and Silva, Luciana Lourdes and Valente, Marco Tulio},
	month = mar,
	year = {2019},
	note = {arXiv:1903.08113 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/IWRSJXSX/Montandon et al. - 2019 - Identifying Experts in Software Libraries and Fram.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/TSZJ5XN4/1903.html:text/html},
}

@inproceedings{montandon_identifying_2019-1,
	address = {Montreal, QC, Canada},
	title = {Identifying {Experts} in {Software} {Libraries} and {Frameworks} {Among} {GitHub} {Users}},
	isbn = {978-1-72813-412-3},
	url = {https://ieeexplore.ieee.org/document/8816776/},
	doi = {10.1109/MSR.2019.00054},
	abstract = {Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classiﬁers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code ﬁles that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our ﬁndings, we document the challenges of using machine learning classiﬁers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin proﬁles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.},
	language = {en},
	urldate = {2022-12-31},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Montandon, Joao Eduardo and Lourdes Silva, Luciana and Valente, Marco Tulio},
	month = may,
	year = {2019},
	pages = {276--287},
	file = {Montandon et al. - 2019 - Identifying Experts in Software Libraries and Fram.pdf:/Users/akazad/Zotero/storage/6W7XNQZY/Montandon et al. - 2019 - Identifying Experts in Software Libraries and Fram.pdf:application/pdf},
}

@inproceedings{greene_cvexplorer_2016,
	address = {Singapore Singapore},
	title = {{CVExplorer}: identifying candidate developers by mining and exploring their open source contributions},
	isbn = {978-1-4503-3845-5},
	shorttitle = {{CVExplorer}},
	url = {https://dl.acm.org/doi/10.1145/2970276.2970285},
	doi = {10.1145/2970276.2970285},
	language = {en},
	urldate = {2023-01-01},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Greene, Gillian J. and Fischer, Bernd},
	month = aug,
	year = {2016},
	pages = {804--809},
	file = {Greene and Fischer - 2016 - CVExplorer identifying candidate developers by mi.pdf:/Users/akazad/Zotero/storage/ZVB3BT5F/Greene and Fischer - 2016 - CVExplorer identifying candidate developers by mi.pdf:application/pdf},
}

@inproceedings{huang_cpdscorer_2016,
	title = {{CPDScorer}: {Modeling} and {Evaluating} {Developer} {Programming} {Ability} across {Software} {Communities}},
	shorttitle = {{CPDScorer}},
	url = {http://ksiresearchorg.ipage.com/seke/seke16paper/seke16paper_12.pdf},
	doi = {10.18293/SEKE2016-012},
	abstract = {Since developer ability is recognized as a determinant of better software project performance, it is a critical step to model and evaluate the programming ability of developers. However, most existing approaches require manual assessment, like 360 degree performance evaluation. With the emergence of social networking sites such as StackOverﬂow and Github, a vast amount of developer information is created on a daily basis. Such personal and social context data has huge potential to support automatic and effective developer ability evaluation. In this paper, we propose CPDScorer, a novel approach to model and score the programming ability of developer through mining heterogeneous information from both Community Question Answering (CQA) sites and Open-Source Software (OSS) communities. CPDScorer analyzes the answers posted in CQA sites and evaluates the projects submitted in OSS communities to assign expertise scores to developers, considering both the quantitative and qualitative factors. When modeling the programming ability of developer, a programming ability term extraction algorithm is also designed based on set covering. We have conducted experiments on StackOverﬂow and Github to measure the effectiveness of CPDScorer. The results show that our approach is feasible and practical in user programming ability modeling. In particular, the precision of our approach reaches 80\%.},
	language = {en},
	urldate = {2023-01-02},
	author = {Huang, Weizhi and Mo, Wenkai and Shen, Beijun and Yang, Yu and Li, Ning},
	month = jul,
	year = {2016},
	pages = {87--92},
	file = {Huang et al. - 2016 - CPDScorer Modeling and Evaluating Developer Progr.pdf:/Users/akazad/Zotero/storage/C8QY4BMH/Huang et al. - 2016 - CPDScorer Modeling and Evaluating Developer Progr.pdf:application/pdf},
}

@inproceedings{hauff_matching_2015,
	address = {Florence, Italy},
	title = {Matching {GitHub} {Developer} {Profiles} to {Job} {Advertisements}},
	isbn = {978-0-7695-5594-2},
	url = {http://ieeexplore.ieee.org/document/7180095/},
	doi = {10.1109/MSR.2015.41},
	abstract = {GitHub is a social coding platform that enables developers to efﬁciently work on projects, connect with other developers, collaborate and generally “be seen” by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer’s skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.},
	language = {en},
	urldate = {2023-01-02},
	booktitle = {2015 {IEEE}/{ACM} 12th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE},
	author = {Hauff, Claudia and Gousios, Georgios},
	month = may,
	year = {2015},
	pages = {362--366},
	file = {Hauff and Gousios - 2015 - Matching GitHub Developer Profiles to Job Advertis.pdf:/Users/akazad/Zotero/storage/Z3MJVHKU/Hauff and Gousios - 2015 - Matching GitHub Developer Profiles to Job Advertis.pdf:application/pdf},
}

@misc{noauthor_taxonomies_nodate,
	title = {Taxonomies in software engineering: {A} {Systematic} mapping study and a revised taxonomy development method {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Taxonomies in software engineering},
	url = {https://reader.elsevier.com/reader/sd/pii/S0950584917300472?token=1C921B7DEB3716C4B1F366B3E283769DBFE6A148AC5C42718BFD1DB8CDF27A7E3461C75C418A64758D3548CA73A4FAAA&originRegion=us-east-1&originCreation=20230105040937},
	language = {en},
	urldate = {2023-01-05},
	doi = {10.1016/j.infsof.2017.01.006},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/XQTDW46A/Taxonomies in software engineering A Systematic m.pdf:application/pdf},
}

@article{nickerson_method_2013,
	title = {A method for taxonomy development and its application in information systems},
	volume = {22},
	issn = {0960-085X, 1476-9344},
	url = {https://www.tandfonline.com/doi/full/10.1057/ejis.2012.26},
	doi = {10.1057/ejis.2012.26},
	abstract = {A fundamental problem in many disciplines is the classification of objects in a domain of interest into a taxonomy. Developing a taxonomy, however, is a complex process that has not been adequately addressed in the information systems (IS) literature. The purpose of this paper is to present a method for taxonomy development that can be used in IS. First, this paper demonstrates through a comprehensive literature survey that taxonomy development in IS has largely been ad hoc. Then the paper defines the problem of taxonomy development. Next, the paper presents a method for taxonomy development that is based on taxonomy development literature in other disciplines and shows that the method has certain desirable qualities. Finally, the paper demonstrates the efficacy of the method by developing a taxonomy in a domain in IS.},
	language = {en},
	number = {3},
	urldate = {2023-01-05},
	journal = {European Journal of Information Systems},
	author = {Nickerson, Robert C and Varshney, Upkar and Muntermann, Jan},
	month = may,
	year = {2013},
	pages = {336--359},
	file = {Nickerson et al. - 2013 - A method for taxonomy development and its applicat.pdf:/Users/akazad/Zotero/storage/DB4LP88I/Nickerson et al. - 2013 - A method for taxonomy development and its applicat.pdf:application/pdf},
}

@incollection{decker_study_1994,
	address = {Basel},
	title = {A {Study} of {Software} {Development} for {High} {Performance} {Computing}},
	isbn = {978-3-0348-9668-9 978-3-0348-8534-8},
	url = {http://link.springer.com/10.1007/978-3-0348-8534-8_11},
	abstract = {Software development in a High Performance Computing (HPC) environment is non-trivial and requires a thorough understanding of the application and the architecture. The objective of this paper is to study the software development process in a high performance computing environment and to outline the stages typically encountered in this process. Support required at each stage is also highlighted. The modeling of stock option pricing is used as a running example in the study.},
	language = {en},
	urldate = {2023-01-05},
	booktitle = {Programming {Environments} for {Massively} {Parallel} {Distributed} {Systems}},
	publisher = {Birkhäuser Basel},
	author = {Parashar, Manish and Hariri, Salim and Haupt, Tomasz and Fox, Geoffrey},
	editor = {Decker, Karsten M. and Rehmann, René M.},
	year = {1994},
	doi = {10.1007/978-3-0348-8534-8_11},
	keywords = {se in hpc},
	pages = {107--116},
	file = {Parashar et al. - 1994 - A Study of Software Development for High Performan.pdf:/Users/akazad/Zotero/storage/Y7QRYK2I/Parashar et al. - 1994 - A Study of Software Development for High Performan.pdf:application/pdf},
}

@inproceedings{li_exploratory_2020,
	title = {An {Exploratory} {Study} of {Bugs} in {Extended} {Reality} {Applications} on the {Web}},
	doi = {10.1109/ISSRE5003.2020.00025},
	abstract = {Extended Reality (XR) technologies are becoming increasingly popular in recent years. To help developers deploy XR applications on the Web, W3C released the WebXR Device API in 2019, which enable users to interact with browsers using XR devices. Given the convenience brought by WebXR, a growing number of WebXR projects have been deployed in practice. However, many WebXR applications are insufficiently tested before being released. They suffer from various bugs that can degrade user experience or cause undesirable consequences. Yet, the community has limited understanding towards the bugs in the WebXR ecosystem, which impedes the advance of techniques for assuring the reliability of WebXR applications. To bridge this gap, we conducted the first empirical study of WebXR bugs. We collected 368 real bugs from 33 WebXR projects hosted on GitHub. Via a seven-round manual analysis of these bugs, we built a taxonomy of WebXR bugs according to their symptoms and root causes. Furthermore, to understand the uniqueness of WebXR bugs, we compared them with bugs in conventional JavaScript programs and web applications. We believe that our findings can inspire future researches on relevant topics and we released our bug dataset to facilitate follow-up studies.},
	booktitle = {2020 {IEEE} 31st {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Li, Shuqing and Wu, Yechang and Liu, Yi and Wang, Dinghua and Wen, Ming and Tao, Yida and Sui, Yulei and Liu, Yepang},
	month = oct,
	year = {2020},
	note = {ISSN: 2332-6549},
	keywords = {Taxonomy, Computer bugs, bug taxonomy, empirical study, Extended reality, Software reliability, User experience, W3C, WebXR, X reality},
	pages = {172--183},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/IRWYJGLB/9251053.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/2XHCRNQG/Li et al. - 2020 - An Exploratory Study of Bugs in Extended Reality A.pdf:application/pdf},
}

@inproceedings{aghajani_software_2019,
	address = {Montreal, QC, Canada},
	title = {Software {Documentation} {Issues} {Unveiled}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8811931/},
	doi = {10.1109/ICSE.2019.00122},
	abstract = {Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavorable take on documentation.},
	language = {en},
	urldate = {2023-01-10},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Aghajani, Emad and Nagy, Csaba and Vega-Marquez, Olga Lucero and Linares-Vasquez, Mario and Moreno, Laura and Bavota, Gabriele and Lanza, Michele},
	month = may,
	year = {2019},
	pages = {1199--1210},
}

@inproceedings{meng_classifying_2021,
	title = {Classifying {Code} {Commits} with {Convolutional} {Neural} {Networks}},
	doi = {10.1109/IJCNN52387.2021.9533534},
	abstract = {Developers change software programs for various purposes (e.g., bug fixes, feature additions, and code refactorings), but the intents of code changes are often not recorded or are poorly documented. To automatically infer the change intent of each program commit (i.e., a set of code changes), existing work classifies commits based on commit messages and/or the sheer counts of edited files, lines, or abstract syntax tree (AST) nodes. However, none of these tools reason about the syntactic or semantic dependencies between co-applied changes, neither do they adopt any deep learning method. To better characterize program commits, in this paper, we present CClassifier—a new approach that classifies commits by (1) using advanced static program analysis to comprehend relationship between co-applied edits, (2) representing edits and their relationship via graphs, and (3) applying convolutional neural networks (CNN) to classify those graphs. Compared with prior work, CClassifier extracts a richer set of features from program changes; it is the first to classify program commits using CNN. For evaluation, we prepared a benchmark that contains 7,414 code changes from 5 open-source Java projects. On this benchmark, we empirically compared CClassifier and the state-of-the-art approach with five-fold cross validation. On average, when predicting bug-fixing commits within the same projects, CClassifier improved the prediction accuracy from 70\% to 72\%. More importantly, prior work seldom identifies feature-addition commits; CClassifier can successfully identify such commits in a lot more scenarios. Our evaluation shows that CClassifier outperforms prior work due to its usage of advanced program analysis and CNN.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Meng, Na and Jiang, Zijian and Zhong, Hao},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Tools, Neural networks, Feature extraction, Benchmark testing, classification, Codes, deep learning, Program commit, Semantics, Syntactics},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/XFTGUFBE/9533534.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/MXI2IT48/Meng et al. - 2021 - Classifying Code Commits with Convolutional Neural.pdf:application/pdf},
}

@misc{noauthor_augmenting_nodate,
	title = {Augmenting commit classification by using fine-grained source code changes and a pre-trained deep neural language model {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0950584921000495?token=BFF6636A0C187DBA457839F810AD017F2BEC5AF800E5DF58329AFA140EE2A1999E87AC7D51863828CE6ED26206F14903&originRegion=us-east-1&originCreation=20230113165411},
	language = {en},
	urldate = {2023-01-13},
	doi = {10.1016/j.infsof.2021.106566},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/KDDN84LF/Augmenting commit classification by using fine-gra.pdf:application/pdf},
}

@inproceedings{mariano_feature_2019,
	title = {Feature {Changes} in {Source} {Code} for {Commit} {Classification} {Into} {Maintenance} {Activities}},
	doi = {10.1109/ICMLA.2019.00096},
	abstract = {Software maintenance plays an important role during software development and life cycle. Indeed, previous works show that maintenance activities consume most of the software budget. Therefore, understanding how these activities are performed can help software managers to previously plan and allocate resources in projects. Despite previous works, there is still a lack in accurate models to classify developers commits into maintenance activities. In the present article, we propose improvements in a state-of-the-art approach used to classify commits. Particularly, we include three additional features in the classification model and we use XGBoost, a boosting tree learning algorithm, for classification. Experimental results show that our approach outperforms the state-of-the-art baseline achieving more than 77\% of accuracy and more than 64\% in Kappa metric.},
	booktitle = {2019 18th {IEEE} {International} {Conference} {On} {Machine} {Learning} {And} {Applications} ({ICMLA})},
	author = {Mariano, Richard V. R. and dos Santos, Geanderson E. and V. de Almeida, Markos and Brandão, Wladmir C.},
	month = dec,
	year = {2019},
	keywords = {Measurement, Boosting, Classification algorithms, classification model, CP, Forestry, machine learning, Maintenance engineering, software maintenance, Software maintenance, Source code changes},
	pages = {515--518},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/M2IBFQ25/8999295.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/GA5BTK4C/Mariano et al. - 2019 - Feature Changes in Source Code for Commit Classifi.pdf:application/pdf},
}

@article{kim_classifying_2008,
	title = {Classifying {Software} {Changes}: {Clean} or {Buggy}?},
	volume = {34},
	issn = {1939-3520},
	shorttitle = {Classifying {Software} {Changes}},
	doi = {10.1109/TSE.2007.70773},
	abstract = {This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78\% accuracy and 65\% buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kim, Sunghun and Whitehead, , E. James and Zhang, Yi},
	month = mar,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Machine learning, Computer bugs, Feature extraction, Computer languages, Open source software, classification, Classification algorithms, Software maintenance, and association rules, Clustering, Configuration Management, Data mining, History, Metrics/Measurement, Project management, Software debugging},
	pages = {181--196},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/YY74FC7H/4408585.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/HW9B7U84/Kim et al. - 2008 - Classifying Software Changes Clean or Buggy.pdf:application/pdf},
}

@misc{sabetta_practical_2018,
	title = {A {Practical} {Approach} to the {Automatic} {Classification} of {Security}-{Relevant} {Commits}},
	url = {http://arxiv.org/abs/1807.02458},
	abstract = {The lack of reliable sources of detailed information on the vulnerabilities of open-source software (OSS) components is a major obstacle to maintaining a secure software supply chain and an effective vulnerability management process. Standard sources of advisories and vulnerability data, such as the National Vulnerability Database (NVD), are known to suffer from poor coverage and inconsistent quality. To reduce our dependency on these sources, we propose an approach that uses machine-learning to analyze source code repositories and to automatically identify commits that are security-relevant (i.e., that are likely to fix a vulnerability). We treat the source code changes introduced by commits as documents written in natural language, classifying them using standard document classification methods. Combining independent classifiers that use information from different facets of commits, our method can yield high precision (80\%) while ensuring acceptable recall (43\%). In particular, the use of information extracted from the source code changes yields a substantial improvement over the best known approach in state of the art, while requiring a significantly smaller amount of training data and employing a simpler architecture.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Sabetta, Antonino and Bezzi, Michele},
	month = jul,
	year = {2018},
	note = {arXiv:1807.02458 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/622WBWLM/Sabetta and Bezzi - 2018 - A Practical Approach to the Automatic Classificati.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/BS5SJYUX/1807.html:text/html},
}

@misc{noauthor_mining_nodate,
	title = {Mining software repositories for adaptive change commits using machine learning techniques {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0950584919300084?token=4D879A81307ACB6116A91120F6B1ECC8235961DBF3F9A9D9BFEFFD89A27E51F41986AF948CF236CED2EBED8A98C8F12D&originRegion=us-east-1&originCreation=20230113194030},
	language = {en},
	urldate = {2023-01-13},
	doi = {10.1016/j.infsof.2019.01.008},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/G73T8BUQ/Mining software repositories for adaptive change c.pdf:application/pdf},
}

@misc{li_watch_2016,
	title = {Watch out for {This} {Commit}! {A} {Study} of {Influential} {Software} {Changes}},
	url = {http://arxiv.org/abs/1606.03266},
	abstract = {One single code change can significantly influence a wide range of software systems and their users. For example, 1) adding a new feature can spread defects in several modules, while 2) changing an API method can improve the performance of all client programs. Developers often may not clearly know whether their or others' changes are influential at commit time. Rather, it turns out to be influential after affecting many aspects of a system later. This paper investigates influential software changes and proposes an approach to identify them early, i.e., immediately when they are applied. We first conduct a post-mortem analysis to discover existing influential changes by using intuitions such as isolated changes and changes referred by other changes in 10 open source projects. Then we re-categorize all identified changes through an open-card sorting process. Subsequently, we conduct a survey with 89 developers to confirm our influential change categories. Finally, from our ground truth we extract features, including metrics such as the complexity of changes, terms in commit logs and file centrality in co-change graphs, to build machine learning classifiers. The experiment results show that our prediction model achieves overall with random samples 86.8\% precision, 74\% recall and 80.4\% F-measure respectively.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Li, Daoyuan and Li, Li and Kim, Dongsun and Bissyandé, Tegawendé F. and Lo, David and Traon, Yves Le},
	month = jun,
	year = {2016},
	note = {arXiv:1606.03266 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/XIULP824/Li et al. - 2016 - Watch out for This Commit! A Study of Influential .pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/G489YH6K/1606.html:text/html},
}

@inproceedings{ji_identifying_2018,
	title = {Identifying {Supplementary} {Bug}-fix {Commits}},
	volume = {01},
	doi = {10.1109/COMPSAC.2018.00031},
	abstract = {Real-world bugs and the bug-fix activities are essential in many fields such as bug prediction and automatic program repair. Identifying bug-fix commits from version histories has received much recent attention. Linking commits to bug reports and analyzing the commits individually are common practice. However, considering the one-to-many relationship between the bug report and the bug-fix commits, analyzing commits individually will miss the relevance between commits, since several commits might fix the same bug together. In addition, some supplementary bug-fix commits which supplement or correct the identified bug-fix commit may be neglected. For empirical studies on bug-fix commits, it is important to study all the relevant commits as a whole, otherwise we will fail to understand the complete real bug-fix activities. In this paper, we investigate the relevance between bug-fix commits that are linked to the same bug-fix pull request, and utilize machine learning techniques to determine supplementary bug-fix commits for an identified bug-fix commit. Experimental results show that there indeed exist supplementary bug-fix commits (i.e., 19.8\% on average) that are neglected when analyzing commits individually. The performance of our tool SupBCFinder is much better than that of using a sliding window of one hour and that of analyzing the local change. Moreover, inspired by our learning-based approach and extracted features, we propose one effective heuristic as an alternative for the cases when there are not enough pull requests for training.},
	booktitle = {2018 {IEEE} 42nd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Ji, Tao and Pan, Jinkun and Chen, Liqian and Mao, Xiaoguang},
	month = jul,
	year = {2018},
	note = {ISSN: 0730-3157},
	keywords = {Software, Computer bugs, Feature extraction, Training, Maintenance engineering, data quality, Git, Prediction algorithms, Predictive models, supplementary bug fixes},
	pages = {184--193},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/59JXBVPF/8377655.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/NIVPLXY6/Ji et al. - 2018 - Identifying Supplementary Bug-fix Commits.pdf:application/pdf},
}

@inproceedings{sobreira_dissection_2018,
	title = {Dissection of a {Bug} {Dataset}: {Anatomy} of 395 {Patches} from {Defects4J}},
	shorttitle = {Dissection of a {Bug} {Dataset}},
	url = {http://arxiv.org/abs/1801.06393},
	doi = {10.1109/SANER.2018.8330203},
	abstract = {Well-designed and publicly available datasets of bugs are an invaluable asset to advance research fields such as fault localization and program repair as they allow directly and fairly comparison between competing techniques and also the replication of experiments. These datasets need to be deeply understood by researchers: the answer for questions like "which bugs can my technique handle?" and "for which bugs is my technique effective?" depends on the comprehension of properties related to bugs and their patches. However, such properties are usually not included in the datasets, and there is still no widely adopted methodology for characterizing bugs and patches. In this work, we deeply study 395 patches of the Defects4J dataset. Quantitative properties (patch size and spreading) were automatically extracted, whereas qualitative ones (repair actions and patterns) were manually extracted using a thematic analysis-based approach. We found that 1) the median size of Defects4J patches is four lines, and almost 30\% of the patches contain only addition of lines; 2) 92\% of the patches change only one file, and 38\% has no spreading at all; 3) the top-3 most applied repair actions are addition of method calls, conditionals, and assignments, occurring in 77\% of the patches; and 4) nine repair patterns were found for 95\% of the patches, where the most prevalent, appearing in 43\% of the patches, is on conditional blocks. These results are useful for researchers to perform advanced analysis on their techniques' results based on Defects4J. Moreover, our set of properties can be used to characterize and compare different bug datasets.},
	urldate = {2023-02-25},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Sobreira, Victor and Durieux, Thomas and Madeiral, Fernanda and Monperrus, Martin and Maia, Marcelo A.},
	month = mar,
	year = {2018},
	note = {arXiv:1801.06393 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {130--140},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/JLZ8FW6Y/Sobreira et al. - 2018 - Dissection of a Bug Dataset Anatomy of 395 Patche.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/U5HFJHSY/1801.html:text/html},
}

@misc{zhou_docprompting_2023,
	title = {{DocPrompting}: {Generating} {Code} by {Retrieving} the {Docs}},
	shorttitle = {{DocPrompting}},
	url = {http://arxiv.org/abs/2207.05987},
	abstract = {Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85\% in pass@1 (52\% relative gain) and 4.39\% in pass@10 (30\% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9\% exact match.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Zhou, Shuyan and Alon, Uri and Xu, Frank F. and Wang, Zhiruo and Jiang, Zhengbao and Neubig, Graham},
	month = feb,
	year = {2023},
	note = {arXiv:2207.05987 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/ZZNA7BNI/Zhou et al. - 2023 - DocPrompting Generating Code by Retrieving the Do.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/J7FC8ZYG/2207.html:text/html},
}

@misc{kruse_loop_2021,
	title = {Loop {Transformations} using {Clang}'s {Abstract} {Syntax} {Tree}},
	url = {http://arxiv.org/abs/2107.08132},
	abstract = {OpenMP 5.1 introduced the first loop nest transformation directives unroll and tile, and more are expected to be included in OpenMP 6.0. We discuss the two Abstract Syntax Tree (AST) representations used by Clang's implementation that is currently under development. The first representation is designed for compatibility with the existing implementation and stores the transformed loop nest in a shadow AST next to the syntactical AST. The second representation introduces a new meta AST-node OMPCanonicalLoop that guarantees that the semantic requirements of an OpenMP loop are met, and a CanonicalLoopInfo type that the OpenMPIRBuilder uses to represent literal and transformed loops. This second approach provides a better abstraction of loop semantics, removes the need for shadow AST nodes that are only relevant for code generation, allows sharing the implementation with other front-ends such as flang, but depends on the OpenMPIRBuilder which is currently under development.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Kruse, Michael},
	month = jul,
	year = {2021},
	note = {arXiv:2107.08132 [cs]},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/HUR5S5V8/Kruse - 2021 - Loop Transformations using Clang's Abstract Syntax.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/V3INVX2I/2107.html:text/html},
}

@article{moreau_taxonomy_2018,
	title = {A {Taxonomy} of {General} {Purpose} {Approximate} {Computing} {Techniques}},
	volume = {10},
	issn = {1943-0663, 1943-0671},
	url = {http://ieeexplore.ieee.org/document/8054698/},
	doi = {10.1109/LES.2017.2758679},
	abstract = {Approximate computing is the idea that systems can gain performance and energy efﬁciency if they expend less effort on producing a “perfect” answer. Approximate computing techniques propose various ways of exposing and exploiting accuracy–efﬁciency trade-offs. We present a taxonomy that classiﬁes approximate computing techniques according to salient features: visibility, determinism, and coarseness. These axes allow us to address questions about the correctability, reproducibility, and control over accuracy–efﬁciency tradeoffs of different techniques. We use this taxonomy to inform research challenges in approximate architectures, compilers, and applications.},
	language = {en},
	number = {1},
	urldate = {2023-02-28},
	journal = {IEEE Embedded Systems Letters},
	author = {Moreau, Thierry and San Miguel, Joshua and Wyse, Mark and Bornholt, James and Alaghi, Armin and Ceze, Luis and Enright Jerger, Natalie and Sampson, Adrian},
	month = mar,
	year = {2018},
	pages = {2--5},
	file = {Moreau et al. - 2018 - A Taxonomy of General Purpose Approximate Computin.pdf:/Users/akazad/Zotero/storage/E2HURMMX/Moreau et al. - 2018 - A Taxonomy of General Purpose Approximate Computin.pdf:application/pdf},
}

@article{moreau_taxonomy_nodate,
	title = {A {Taxonomy} of {Approximate} {Computing} {Techniques}},
	abstract = {Approximate computing is the idea that systems can gain performance and energy efﬁciency if they expend less effort on producing a “perfect” answer. Approximate computing techniques propose various ways of exposing and exploiting accuracy–efﬁciency trade-offs. We present a taxonomy that classiﬁes approximate computing techniques according to their most salient features: compute vs. data, deterministic vs. nondeterministic and coarse- vs. ﬁne-grained. These axes allow us to address questions about the visibility, testability and ﬂexibility of different techniques. We use this taxonomy to inform future research in approximate architectures, compilers and applications that will catalyze mainstream adoption of approximate computing.},
	language = {en},
	author = {Moreau, Thierry and Miguel, Joshua San and Wyse, Mark and Bornholt, James and Ceze, Luis and Jerger, Natalie Enright and Sampson, Adrian},
	file = {Moreau et al. - A Taxonomy of Approximate Computing Techniques.pdf:/Users/akazad/Zotero/storage/GQCPVVLT/Moreau et al. - A Taxonomy of Approximate Computing Techniques.pdf:application/pdf},
}

@book{noauthor_text_nodate,
	title = {Text {Mining} {Book}},
	file = {_.pdf:/Users/akazad/Zotero/storage/VN66DGM9/_.pdf:application/pdf},
}

@misc{frieder_mathematical_2023,
	title = {Mathematical {Capabilities} of {ChatGPT}},
	url = {http://arxiv.org/abs/2301.13867},
	abstract = {We investigate the mathematical capabilities of ChatGPT by testing it on publicly available datasets, as well as hand-crafted ones, and measuring its performance against other models trained on a mathematical corpus, such as Minerva. We also test whether ChatGPT can be a useful assistant to professional mathematicians by emulating various use cases that come up in the daily professional activities of mathematicians (question answering, theorem searching). In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, only cover elementary mathematics. We address this issue by introducing a new dataset: GHOSTS. It is the first natural-language dataset made and curated by working researchers in mathematics that (1) aims to cover graduate-level mathematics and (2) provides a holistic overview of the mathematical capabilities of language models. We benchmark ChatGPT on GHOSTS and evaluate performance against fine-grained criteria. We make this new dataset publicly available to assist a community-driven comparison of ChatGPT with (future) large language models in terms of advanced mathematical comprehension. We conclude that contrary to many positive reports in the media (a potential case of selection bias), ChatGPT's mathematical abilities are significantly below those of an average mathematics graduate student. Our results show that ChatGPT often understands the question but fails to provide correct solutions. Hence, if your goal is to use it to pass a university exam, you would be better off copying from your average peer!},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Frieder, Simon and Pinchetti, Luca and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp Christian and Chevalier, Alexis and Berner, Julius},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13867 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/I9VXQX68/Frieder et al. - 2023 - Mathematical Capabilities of ChatGPT.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/ARZG7TKK/2301.html:text/html},
}

@book{reynolds_presentation_2012,
	address = {Berkeley, CA},
	edition = {2nd ed., rev. \& updated},
	series = {Voices that matter},
	title = {Presentation zen: simple ideas on presentation design and delivery},
	isbn = {978-0-321-81198-1},
	shorttitle = {Presentation zen},
	abstract = {Provides lessons to help users design and deliver creative presentations using Microsoft PowerPoint},
	language = {en},
	publisher = {New Riders},
	author = {Reynolds, Garr},
	year = {2012},
	keywords = {Business presentations, Graphic methods Computer programs, Microsoft PowerPoint (Computer file), Multimedia systems in business presentations},
}

@article{davidson_tips_nodate,
	title = {Tips for {Presenting} {Your} {Research} at {Conferences}},
	language = {en},
	author = {Davidson, Marlina},
	keywords = {presentation},
	file = {Davidson - Tips for Presenting Your Research at Conferences.pdf:/Users/akazad/Zotero/storage/6F62WPNN/Davidson - Tips for Presenting Your Research at Conferences.pdf:application/pdf},
}

@misc{sandoval_lost_2023,
	title = {Lost at {C}: {A} {User} {Study} on the {Security} {Implications} of {Large} {Language} {Model} {Code} {Assistants}},
	shorttitle = {Lost at {C}},
	url = {http://arxiv.org/abs/2208.09727},
	abstract = {Large Language Models (LLMs) such as OpenAI Codex are increasingly being used as AI-based coding assistants. Understanding the impact of these tools on developers' code is paramount, especially as recent work showed that LLMs may suggest cybersecurity vulnerabilities. We conduct a security-driven user study (N=58) to assess code written by student programmers when assisted by LLMs. Given the potential severity of low-level bugs as well as their relative frequency in real-world projects, we tasked participants with implementing a singly-linked 'shopping list' structure in C. Our results indicate that the security impact in this setting (low-level C with pointer and array manipulations) is small: AI-assisted users produce critical security bugs at a rate no greater than 10\% more than the control, indicating the use of LLMs does not introduce new security risks.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Sandoval, Gustavo and Pearce, Hammond and Nys, Teo and Karri, Ramesh and Garg, Siddharth and Dolan-Gavitt, Brendan},
	month = feb,
	year = {2023},
	note = {arXiv:2208.09727 [cs]
version: 4},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/ZBNIQ8XV/Sandoval et al. - 2023 - Lost at C A User Study on the Security Implicatio.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/7QW7PCHN/2208.html:text/html},
}

@misc{nie_learning_2023,
	title = {Learning {Deep} {Semantics} for {Test} {Completion}},
	url = {http://arxiv.org/abs/2302.10166},
	abstract = {Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TeCo -- a deep learning model using code semantics for test completion. The key insight underlying TeCo is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. TeCo extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TeCo, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that TeCo achieves an exact-match accuracy of 18, which is 29\% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, TeCo can generate runnable code in 29\% of the cases compared to 18\% obtained by the best baseline. Moreover, TeCo is significantly better than prior work on test oracle generation.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
	month = mar,
	year = {2023},
	note = {arXiv:2302.10166 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/GSXQRTNC/Nie et al. - 2023 - Learning Deep Semantics for Test Completion.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/R2XH2J2P/2302.html:text/html},
}

@misc{noauthor_inbox_nodate,
	title = {Inbox (645) - akazad@umich.edu - {University} of {Michigan} {Mail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2023-03-26},
}

@misc{noauthor_inbox_nodate-1,
	title = {Inbox (645) - akazad@umich.edu - {University} of {Michigan} {Mail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2023-03-26},
}

@misc{rathi_complete_nodate,
	title = {A {Complete} {Guide} to {LLVM} for {Programming} {Language} {Creators}},
	url = {https://mukulrathi.com/create-your-own-programming-language/llvm-ir-cpp-api-tutorial/},
	abstract = {We'll explain the concepts underlying LLVM IR, and how you can use the LLVM C++ API in your programming language's compiler.},
	language = {en},
	urldate = {2023-03-26},
	author = {Rathi, Mukul},
	file = {Snapshot:/Users/akazad/Zotero/storage/Z2RW9QVA/Rathi - A Complete Guide to LLVM for Programming Language .html:text/html},
}

@article{noauthor_2022_nodate,
	title = {2022  {Michigan} {Schedule} 1 {Additions} and {Subtractions}},
	language = {en},
	file = {2022  Michigan Schedule 1 Additions and Subtractio.pdf:/Users/akazad/Zotero/storage/HBEHAGSD/2022  Michigan Schedule 1 Additions and Subtractio.pdf:application/pdf},
}

@inproceedings{pei_neudep_2022,
	address = {Singapore Singapore},
	title = {{NeuDep}: neural binary memory dependence analysis},
	isbn = {978-1-4503-9413-0},
	shorttitle = {{NeuDep}},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549147},
	doi = {10.1145/3540250.3549147},
	language = {en},
	urldate = {2023-04-14},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Pei, Kexin and She, Dongdong and Wang, Michael and Geng, Scott and Xuan, Zhou and David, Yaniv and Yang, Junfeng and Jana, Suman and Ray, Baishakhi},
	month = nov,
	year = {2022},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Programming Languages},
	pages = {747--759},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/82YGAP3X/2210.html:text/html;Full Text:/Users/akazad/Zotero/storage/TRRUHJBW/Pei et al. - 2022 - NeuDep neural binary memory dependence analysis.pdf:application/pdf;Full Text PDF:/Users/akazad/Zotero/storage/WI87KQW5/Pei et al. - 2022 - NeuDep Neural Binary Memory Dependence Analysis.pdf:application/pdf},
}

@misc{noauthor_msr23_presentation_draft_nodate,
	title = {{MSR23}\_presentation\_draft},
	url = {https://docs.google.com/presentation/d/1iltkgbiA0g4jostK32dYYLq_r6LaEsTrJCPYhgu3UdE/edit?ouid=115848223568547580847&usp=slides_home&ths=true&usp=embed_facebook},
	abstract = {An Empirical Study of High Performance Computing (HPC) Performance Bugs Md Abul Kalam Azad, Nafees Iqbal, Foyzul Hassan, Probir Roy Department of Computer and Information Science University of Michigan - Dearborn May 1, 2023 Hello Everyone, Good Morning, My name is , I am currently a phd student ...},
	language = {en},
	urldate = {2023-05-05},
	journal = {Google Docs},
	file = {Snapshot:/Users/akazad/Zotero/storage/8DNZ278Z/edit.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/LYELVZUI/1706.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/667MXEW8/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate-2,
	url = {https://conferences.computer.org/icsepub/pdfs/ICSE2023-bO1MXfEUhGxrfTK3k5jSh/570100b434/570100b434.pdf},
}

@article{xia_automated_nodate,
	title = {Automated {Program} {Repair} in the {Era} of {Large} {Pre}-trained {Language} {Models}},
	abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bugfixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.},
	language = {en},
	author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
	file = {Xia et al. - Automated Program Repair in the Era of Large Pre-t.pdf:/Users/akazad/Zotero/storage/6YR3P7TM/Xia et al. - Automated Program Repair in the Era of Large Pre-t.pdf:application/pdf},
}

@misc{fan_automated_2023,
	title = {Automated {Repair} of {Programs} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10583},
	abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
	month = jan,
	year = {2023},
	note = {arXiv:2205.10583 [cs]},
	keywords = {Computer Science - Software Engineering, 2023},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/7S3QR66Z/2205.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/3CDTKYZX/Fan et al. - 2023 - Automated Repair of Programs from Large Language M.pdf:application/pdf},
}

@inproceedings{karmakar_what_2021,
	address = {Melbourne, Australia},
	title = {What do pre-trained code models know about code?},
	isbn = {978-1-66540-337-5},
	url = {https://ieeexplore.ieee.org/document/9678927/},
	doi = {10.1109/ASE51524.2021.9678927},
	urldate = {2023-06-15},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Karmakar, Anjan and Robbes, Romain},
	month = nov,
	year = {2021},
	pages = {1332--1336},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/D7KGA5YV/Karmakar and Robbes - 2021 - What do pre-trained code models know about code.pdf:application/pdf},
}

@misc{guo_graphcodebert_2021,
	title = {{GraphCodeBERT}: {Pre}-training {Code} {Representations} with {Data} {Flow}},
	shorttitle = {{GraphCodeBERT}},
	url = {http://arxiv.org/abs/2009.08366},
	abstract = {Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "where-the-value-comes-from" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and Tufano, Michele and Deng, Shao Kun and Clement, Colin and Drain, Dawn and Sundaresan, Neel and Yin, Jian and Jiang, Daxin and Zhou, Ming},
	month = sep,
	year = {2021},
	note = {arXiv:2009.08366 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EZGMLYF5/2009.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/YTCWHIA5/Guo et al. - 2021 - GraphCodeBERT Pre-training Code Representations w.pdf:application/pdf},
}

@inproceedings{ciniselli_empirical_2021-1,
	address = {Madrid, Spain},
	title = {An {Empirical} {Study} on the {Usage} of {BERT} {Models} for {Code} {Completion}},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463129/},
	doi = {10.1109/MSR52588.2021.00024},
	abstract = {Code completion is one of the main features of modern Integrated Development Environments (IDEs). Its objective is to speed up code writing by predicting the next code token(s) the developer is likely to write. Research in this area has substantially bolstered the predictive performance of these techniques. However, the support to developers is still limited to the prediction of the next few tokens to type. In this work, we take a step further in this direction by presenting a large-scale empirical study aimed at exploring the capabilities of state-ofthe-art deep learning (DL) models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). To this aim, we train and test several adapted variants of the recently proposed RoBERTa model, and evaluate its predictions from several perspectives, including: (i) metrics usually adopted when assessing DL generative models (i.e., BLEU score and Levenshtein distance); (ii) the percentage of perfect predictions (i.e., the predicted code snippets that match those written by developers); and (iii) the “semantic” equivalence of the generated code as compared to the one written by developers. The achieved results show that BERT models represent a viable solution for code completion, with perfect predictions ranging from ∼7\%, obtained when asking the model to guess entire blocks, up to ∼58\%, reached in the simpler scenario of few tokens masked from the same code statement.},
	language = {en},
	urldate = {2023-06-21},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	month = may,
	year = {2021},
	pages = {108--119},
	file = {Ciniselli et al. - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:/Users/akazad/Zotero/storage/SIXBZT6K/Ciniselli et al. - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:application/pdf},
}

@misc{ahmed_recommending_2023,
	title = {Recommending {Root}-{Cause} and {Mitigation} {Steps} for {Cloud} {Incidents} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.03797},
	abstract = {Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
	month = feb,
	year = {2023},
	note = {arXiv:2301.03797 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/VWSZDVWM/2301.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/ZIQDTCTW/Ahmed et al. - 2023 - Recommending Root-Cause and Mitigation Steps for C.pdf:application/pdf},
}

@misc{noauthor_unreasonable_nodate,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2023-06-21},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/akazad/Zotero/storage/2238DAHI/rnn-effectiveness.html:text/html;The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/akazad/Zotero/storage/82Z8AUT3/rnn-effectiveness.html:text/html;The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/akazad/Zotero/storage/E8NFL76C/rnn-effectiveness.html:text/html;The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/akazad/Zotero/storage/5H5MHDJ8/rnn-effectiveness.html:text/html;The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/akazad/Zotero/storage/L2M9839J/rnn-effectiveness.html:text/html},
}

@misc{bouzenia_tracefixer_2023,
	title = {{TraceFixer}: {Execution} {Trace}-{Driven} {Program} {Repair}},
	shorttitle = {{TraceFixer}},
	url = {http://arxiv.org/abs/2304.12743},
	abstract = {When debugging unintended program behavior, developers can often identify the point in the execution where the actual behavior diverges from the desired behavior. For example, a variable may get assigned a wrong value, which then negatively influences the remaining computation. Once a developer identifies such a divergence, how to fix the code so that it provides the desired behavior? This paper presents TraceFixer, a technique for predicting how to edit source code so that it does not diverge from the expected behavior anymore. The key idea is to train a neural program repair model that not only learns from source code edits but also exploits excerpts of runtime traces. The input to the model is a partial execution trace of the incorrect code, which can be obtained automatically through code instrumentation, and the correct state that the program should reach at the divergence point, which the user provides, e.g., in an interactive debugger. Our approach fundamentally differs from current program repair techniques, which share a similar goal but exploit neither execution traces nor information about the desired program state. We evaluate TraceFixer on single-line mistakes in Python code. After training the model on hundreds of thousands of code edits created by a neural model that mimics real-world bugs, we find that exploiting execution traces improves the bug-fixing ability by 13\% to 20\% (depending on the dataset, within the top-10 predictions) compared to a baseline that learns from source code edits only. Applying TraceFixer to 20 real-world Python bugs shows that the approach successfully fixes 10 of them.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Bouzenia, Islem and Ding, Yangruibo and Pei, Kexin and Ray, Baishakhi and Pradel, Michael},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12743 [cs]},
	keywords = {Computer Science - Software Engineering, ray},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/PEVD8WAR/2304.html:text/html;arXiv.org Snapshot:/Users/akazad/Zotero/storage/MBFANHWG/2304.html:text/html;arXiv.org Snapshot:/Users/akazad/Zotero/storage/982IL4J8/2304.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/Q2X6GH48/Bouzenia et al. - 2023 - TraceFixer Execution Trace-Driven Program Repair.pdf:application/pdf},
}

@misc{van_dam_enriching_2023,
	title = {Enriching {Source} {Code} with {Contextual} {Data} for {Code} {Completion} {Models}: {An} {Empirical} {Study}},
	shorttitle = {Enriching {Source} {Code} with {Contextual} {Data} for {Code} {Completion} {Models}},
	url = {http://arxiv.org/abs/2304.12269},
	abstract = {Transformer-based pre-trained models have recently achieved great results in solving many software engineering tasks including automatic code completion which is a staple in a developer's toolkit. While many have striven to improve the code-understanding abilities of such models, the opposite -- making the code easier to understand -- has not been properly investigated. In this study, we aim to answer whether making code easier to understand through using contextual data improves the performance of pre-trained code language models for the task of code completion. We consider type annotations and comments as two common forms of additional contextual information that often help developers understand code better. For the experiments, we study code completion in two granularity levels; token and line completion and take three recent and large-scale language models for source code: UniXcoder, CodeGPT, and InCoder with five evaluation metrics. Finally, we perform the Wilcoxon Signed Rank test to gauge significance and measure the effect size. Contrary to our expectations, all models perform better if type annotations are removed (albeit the effect sizes are small). For comments, we find that the models perform better in the presence of multi-line comments (again with small effect sizes). Based on our observations, we recommend making proper design choices when training, fine-tuning, or simply selecting such models given the intended data and application. Better evaluations and multi-modal techniques can also be further investigated to improve the practicality and accuracy of auto-completions.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {van Dam, Tim and Izadi, Maliheh and van Deursen, Arie},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12269 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/KTU94YY2/2304.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/9MNI74PK/van Dam et al. - 2023 - Enriching Source Code with Contextual Data for Cod.pdf:application/pdf},
}

@inproceedings{chirkova_empirical_2021,
	address = {Athens Greece},
	title = {Empirical study of transformers for source code},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468611},
	doi = {10.1145/3468264.3468611},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Chirkova, Nadezhda and Troshin, Sergey},
	month = aug,
	year = {2021},
	pages = {703--715},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/WAXSUZPC/Chirkova and Troshin - 2021 - Empirical study of transformers for source code.pdf:application/pdf},
}

@article{sohil_introduction_2022,
	title = {An introduction to statistical learning with applications in {R}: by {Gareth} {James}, {Daniela} {Witten}, {Trevor} {Hastie}, and {Robert} {Tibshirani}, {New} {York}, {Springer} {Science} and {Business} {Media}, 2013, \$41.98, {eISBN}: 978-1-4614-7137-7},
	volume = {6},
	issn = {2475-4269, 2475-4277},
	shorttitle = {An introduction to statistical learning with applications in {R}},
	url = {https://www.tandfonline.com/doi/full/10.1080/24754269.2021.1980261},
	doi = {10.1080/24754269.2021.1980261},
	language = {en},
	number = {1},
	urldate = {2023-06-23},
	journal = {Statistical Theory and Related Fields},
	author = {Sohil, Fariha and Sohali, Muhammad Umair and Shabbir, Javid},
	month = jan,
	year = {2022},
	pages = {87--87},
	file = {Sohil et al. - 2022 - An introduction to statistical learning with appli.pdf:/Users/akazad/Zotero/storage/FPEPHCYC/Sohil et al. - 2022 - An introduction to statistical learning with appli.pdf:application/pdf},
}

@article{arora_checkpointing_nodate,
	title = {Checkpointing and {Saving} the {States} of {AI} {Models}},
	language = {en},
	author = {Arora, Ritu},
	file = {Arora - Checkpointing and Saving the States of AI Models.pdf:/Users/akazad/Zotero/storage/PRTU8G6L/Arora - Checkpointing and Saving the States of AI Models.pdf:application/pdf},
}

@misc{noauthor_notitle_nodate-3,
	url = {https://www.deeplearningbook.org/contents/convnets.html},
	urldate = {2023-06-26},
	keywords = {CNN},
	file = {deeplearningbook.org/contents/convnets.html:/Users/akazad/Zotero/storage/NTMZGDGV/convnets.html:text/html},
}

@article{rael_exploring_nodate,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Uniﬁed} {Text}-to-{Text} {Transformer}},
	language = {en},
	author = {Raﬀel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	file = {Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:/Users/akazad/Zotero/storage/6GADSWGR/Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@misc{bui_codetf_2023,
	title = {{CodeTF}: {One}-stop {Transformer} {Library} for {State}-of-the-art {Code} {LLM}},
	shorttitle = {{CodeTF}},
	url = {http://arxiv.org/abs/2306.00029},
	abstract = {Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Bui, Nghi D. Q. and Le, Hung and Wang, Yue and Li, Junnan and Gotmare, Akhilesh Deepak and Hoi, Steven C. H.},
	month = may,
	year = {2023},
	note = {arXiv:2306.00029 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/I8KRA5EP/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/QXIBTSYR/Bui et al. - 2023 - CodeTF One-stop Transformer Library for State-of-.pdf:application/pdf},
}

@article{parmar_course_nodate,
	title = {Course {Instructor}: {Prof}. {Jonathan} {Rose}},
	language = {en},
	author = {Parmar, Dhairya and Dai, Yuchen},
	file = {Parmar and Dai - Course Instructor Prof. Jonathan Rose.pdf:/Users/akazad/Zotero/storage/DLUT4PDD/Parmar and Dai - Course Instructor Prof. Jonathan Rose.pdf:application/pdf},
}

@misc{ahmed_towards_2023,
	title = {Towards {Understanding} {What} {Code} {Language} {Models} {Learned}},
	url = {http://arxiv.org/abs/2306.11943},
	abstract = {Pre-trained language models are effective in a variety of natural language tasks, but it has been argued their capabilities fall short of fully learning meaning or understanding language. To understand the extent to which language models can learn some form of meaning, we investigate their ability to capture semantics of code beyond superficial frequency and co-occurrence. In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics. In this paper, we examine whether such models capture the semantics of code, which is precisely and formally defined. Through experiments involving the manipulation of code fragments, we show that code pre-trained models of code learn a robust representation of the computational semantics of code that goes beyond superficial features of form alone},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Ahmed, Toufique and Yu, Dian and Huang, Chengxuan and Wang, Cathy and Devanbu, Prem and Sagae, Kenji},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11943 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/9C9BC6I4/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NM58EAAI/Ahmed et al. - 2023 - Towards Understanding What Code Language Models Le.pdf:application/pdf},
}

@inproceedings{zeng_extensive_2022,
	address = {Virtual South Korea},
	title = {An extensive study on pre-trained models for program understanding and generation},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534390},
	doi = {10.1145/3533767.3534390},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zeng, Zhengran and Tan, Hanzhuo and Zhang, Haotian and Li, Jing and Zhang, Yuqun and Zhang, Lingming},
	month = jul,
	year = {2022},
	pages = {39--51},
	file = {Zeng et al. - 2022 - An extensive study on pre-trained models for progr.pdf:/Users/akazad/Zotero/storage/UXNBILI9/Zeng et al. - 2022 - An extensive study on pre-trained models for progr.pdf:application/pdf},
}

@article{pei_can_nodate,
	title = {Can {Large} {Language} {Models} {Reason} about {Program} {Invariants}?},
	abstract = {Identifying invariants is an important program analysis task with applications towards program understanding, bug ﬁnding, vulnerability analysis, and formal veriﬁcation. Existing tools for identifying program invariants rely on dynamic analysis, requiring traces collected from multiple executions in order to produce reliable invariants. We study the application of large language models to invariant prediction, ﬁnding that models trained on source code and ﬁne-tuned for invariant generation can perform invariant prediction as static rather than dynamic analysis. Using a scratchpad approach where invariants are predicted sequentially through a program gives the best performance, ﬁnding invariants statically of quality comparable to those obtained by a dynamic analysis tool with access to ﬁve program traces.},
	language = {en},
	author = {Pei, Kexin and Bieber, David and Shi, Kensen and Sutton, Charles and Yin, Pengcheng},
	file = {Pei et al. - Can Large Language Models Reason about Program Inv.pdf:/Users/akazad/Zotero/storage/D7Y67RLR/Pei et al. - Can Large Language Models Reason about Program Inv.pdf:application/pdf},
}

@misc{ding_traced_2023,
	title = {{TRACED}: {Execution}-aware {Pre}-training for {Source} {Code}},
	shorttitle = {{TRACED}},
	url = {http://arxiv.org/abs/2306.07487},
	abstract = {Most existing pre-trained language models for source code focus on learning the static code text, typically augmented with static code structures (abstract syntax tree, dependency graphs, etc.). However, program semantics will not be fully exposed before the real execution. Without an understanding of the program execution, statically pre-trained models fail to comprehensively capture the dynamic code properties, such as the branch coverage and the runtime variable values, and they are consequently less effective at code understanding tasks, such as retrieving semantic clones and detecting software vulnerabilities. To close the gap between the static nature of language models and the dynamic characteristics of programs, we introduce TRACED, an execution-aware pre-training strategy for source code. Specifically, we pre-train code language models with a combination of source code, executable inputs, and corresponding execution traces. Our goal is to teach code models the complicated execution logic during the pre-training, enabling the model to statically estimate the dynamic code properties without repeatedly executing code during task-specific fine-tuning. To illustrate the effectiveness of our proposed approach, we fine-tune and evaluate TRACED on three downstream tasks: static execution estimation, clone retrieval, and vulnerability detection. The empirical results show that TRACED relatively improves the statically pre-trained code models by 12.4\% for complete execution path prediction and by 25.2\% for runtime variable value predictions. TRACED also significantly outperforms statically pre-trained models in clone retrieval and vulnerability detection across four public benchmarks.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Steenhoek, Ben and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07487 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/RC7ZZ97R/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NL9QPWFB/Ding et al. - 2023 - TRACED Execution-aware Pre-training for Source Co.pdf:application/pdf},
}

@misc{pradel_neural_2021,
	title = {Neural {Software} {Analysis}},
	url = {http://arxiv.org/abs/2011.07986},
	abstract = {Many software development problems can be addressed by program analysis tools, which traditionally are based on precise, logical reasoning and heuristics to ensure that the tools are practical. Recent work has shown tremendous success through an alternative way of creating developer tools, which we call neural software analysis. The key idea is to train a neural machine learning model on numerous code examples, which, once trained, makes predictions about previously unseen code. In contrast to traditional program analysis, neural software analysis naturally handles fuzzy information, such as coding conventions and natural language embedded in code, without relying on manually encoded heuristics. This article gives an overview of neural software analysis, discusses when to (not) use it, and presents three example analyses. The analyses address challenging software development problems: bug detection, type prediction, and code completion. The resulting tools complement and outperform traditional program analyses, and are used in industrial practice.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Pradel, Michael and Chandra, Satish},
	month = apr,
	year = {2021},
	note = {arXiv:2011.07986 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/ZFBG3IFS/2011.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/J6FC6U2Q/Pradel and Chandra - 2021 - Neural Software Analysis.pdf:application/pdf},
}

@misc{wang_learning_2019,
	title = {Learning {Blended}, {Precise} {Semantic} {Program} {Embeddings}},
	url = {http://arxiv.org/abs/1907.02136},
	abstract = {Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, {\textbackslash}liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated {\textbackslash}liger on {\textbackslash}coset, a recently proposed benchmark suite for evaluating neural program embeddings. Results show {\textbackslash}liger (1) is significantly more accurate than the state-of-the-art syntax-based models Gated Graph Neural Network and code2vec in classifying program semantics, and (2) requires on average 10x fewer executions covering 74{\textbackslash}\% fewer paths than the state-of-the-art dynamic model {\textbackslash}dypro. Furthermore, we extend {\textbackslash}liger to predict the name for a method from its body's vector representation. Learning on the same set of functions (more than 170K in total), {\textbackslash}liger significantly outperforms code2seq, the previous state-of-the-art for method name prediction.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Wang, Ke and Su, Zhendong},
	month = jul,
	year = {2019},
	note = {arXiv:1907.02136 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/SVBX8HT6/1907.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/AS7SDJ2J/Wang and Su - 2019 - Learning Blended, Precise Semantic Program Embeddi.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/DP9CBXVT/1810.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/6XMEVJQA/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{aldweesh_deep_2020,
	title = {Deep learning approaches for anomaly-based intrusion detection systems: {A} survey, taxonomy, and open issues},
	volume = {189},
	issn = {09507051},
	shorttitle = {Deep learning approaches for anomaly-based intrusion detection systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705119304897},
	doi = {10.1016/j.knosys.2019.105124},
	abstract = {The massive growth of data that are transmitted through a variety of devices and communication protocols have raised serious security concerns, which have increased the importance of developing advanced intrusion detection systems (IDSs). Deep learning is an advanced branch of machine learning, composed of multiple layers of neurons that represent the learning process. Deep learning can cope with large-scale data and has shown success in different fields. Therefore, researchers have paid more attention to investigating deep learning for intrusion detection. This survey comprehensively reviews and compares the key previous deep learning-focused cybersecurity surveys. Through an extensive review, this survey provides a novel fine-grained taxonomy that categorizes the current state-of-the-art deep learning-based IDSs with respect to different facets, including input data, detection, deployment, and evaluation strategies. Each facet is further classified according to different criteria. This survey also compares and discusses the related experimental solutions proposed as deep learning-based IDSs.},
	language = {en},
	urldate = {2023-07-27},
	journal = {Knowledge-Based Systems},
	author = {Aldweesh, Arwa and Derhab, Abdelouahid and Emam, Ahmed Z.},
	month = feb,
	year = {2020},
	pages = {105124},
	file = {Aldweesh et al. - 2020 - Deep learning approaches for anomaly-based intrusi.pdf:/Users/akazad/Zotero/storage/UTGRDCBB/Aldweesh et al. - 2020 - Deep learning approaches for anomaly-based intrusi.pdf:application/pdf},
}

@article{lee_learning_2017,
	title = {Learning {Binary} {Code} with {Deep} {Learning} to {Detect} {Software} {Weakness}},
	abstract = {As more software are being developed, the importance of automated vulnerability analysis tools is increasing. In this paper, we propose a method of learning assembly code using deep learning to find software weaknesses. Unlike prior studies based on API function call sequence, our method starts by adding the assembly code to an immutable vector to learn the assembly language through deep learning. When modeling assembly code, we propose Instruction2vec, which is effective in vectorizing the assembly code. After learning the assembly code of the existing functions through the vector created by Instruction2vec, we classify whether the new functions have software weaknesses or not. In order to demonstrate the effectiveness of Instruction2vec, we train the vectors with text-convolutional neural network model (Text-CNN) and compare the results with those of Word2vec. We also used the Juliet test suite from National Institute of Standards and Technology as a dataset. As a result, we identify up to 96.1\% accuracy in classifying whether the function has software weaknesses.},
	language = {en},
	author = {Lee, Young Jun and Choi, Sang-Hoon and Kim, Chulwoo and Lim, Seung-Ho and Park, Ki-Woong},
	year = {2017},
	file = {Lee et al. - 2017 - Learning Binary Code with Deep Learning to Detect .pdf:/Users/akazad/Zotero/storage/FC7E7B4J/Lee et al. - 2017 - Learning Binary Code with Deep Learning to Detect .pdf:application/pdf},
}

@misc{patra_nalin_2021,
	title = {Nalin: {Learning} from {Runtime} {Behavior} to {Find} {Name}-{Value} {Inconsistencies} in {Jupyter} {Notebooks}},
	shorttitle = {Nalin},
	url = {http://arxiv.org/abs/2112.06186},
	abstract = {Variable names are important to understand and maintain code. If a variable name and the value stored in the variable do not match, then the program suffers from a name-value inconsistency, which is due to one of two situations that developers may want to fix: Either a correct value is referred to through a misleading name, which negatively affects code understandability and maintainability, or the correct name is bound to a wrong value, which may cause unexpected runtime behavior. Finding name-value inconsistencies is hard because it requires an understanding of the meaning of names and knowledge about the values assigned to a variable at runtime. This paper presents Nalin, a technique to automatically detect name-value inconsistencies. The approach combines a dynamic analysis that tracks assignments of values to names with a neural machine learning model that predicts whether a name and a value fit together. To the best of our knowledge, this is the first work to formulate the problem of finding coding issues as a classification problem over names and runtime values. We apply Nalin to 106,652 real-world Python programs, where meaningful names are particularly important due to the absence of statically declared types. Our results show that the classifier detects name-value inconsistencies with high accuracy, that the warnings reported by Nalin have a precision of 80\% and a recall of 76\% w.r.t. a ground truth created in a user study, and that our approach complements existing techniques for finding coding issues.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Patra, Jibesh and Pradel, Michael},
	month = dec,
	year = {2021},
	note = {arXiv:2112.06186 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/F5QRY8R2/2112.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/8ILPKJNV/Patra and Pradel - 2021 - Nalin Learning from Runtime Behavior to Find Name.pdf:application/pdf},
}

@inproceedings{mir_type4py_2022,
	title = {{Type4Py}: {Practical} {Deep} {Similarity} {Learning}-{Based} {Type} {Inference} for {Python}},
	shorttitle = {{Type4Py}},
	url = {http://arxiv.org/abs/2101.04470},
	doi = {10.1145/3510003.3510124},
	abstract = {Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing codebases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type inference based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present Type4Py, a deep similarity learning-based hierarchical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neighbor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that Type4Py achieves an MRR of 77.1\%, which is a substantial improvement of 8.1\% and 16.7\% over the state-of-the-art approaches Typilus and TypeWriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Studio Code extension, which uses Type4Py to provide ML-based type auto-completion for Python.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Mir, Amir M. and Latoskinas, Evaldas and Proksch, Sebastian and Gousios, Georgios},
	month = may,
	year = {2022},
	note = {arXiv:2101.04470 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Programming Languages},
	pages = {2241--2252},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/57VWWFE8/2101.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/HH5KUACU/Mir et al. - 2022 - Type4Py Practical Deep Similarity Learning-Based .pdf:application/pdf},
}

@misc{ma_graphcode2vec_2022,
	title = {{GraphCode2Vec}: {Generic} {Code} {Embedding} via {Lexical} and {Program} {Dependence} {Analyses}},
	shorttitle = {{GraphCode2Vec}},
	url = {http://arxiv.org/abs/2112.01218},
	abstract = {Code embedding is a keystone in the application of machine learning on several Software Engineering (SE) tasks. To effectively support a plethora of SE tasks, the embedding needs to capture program syntax and semantics in a way that is generic. To this end, we propose the first self-supervised pre-training approach (called GraphCode2Vec) which produces task-agnostic embedding of lexical and program dependence features. GraphCode2Vec achieves this via a synergistic combination of code analysis and Graph Neural Networks. GraphCode2Vec is generic, it allows pre-training, and it is applicable to several SE downstream tasks. We evaluate the effectiveness of GraphCode2Vec on four (4) tasks (method name prediction, solution classification, mutation testing and overfitted patch classification), and compare it with four (4) similarly generic code embedding baselines (Code2Seq, Code2Vec, CodeBERT, GraphCodeBERT) and 7 task-specific, learning-based methods. In particular, GraphCode2Vec is more effective than both generic and task-specific learning-based baselines. It is also complementary and comparable to GraphCodeBERT (a larger and more complex model). We also demonstrate through a probing and ablation study that GraphCode2Vec learns lexical and program dependence features and that self-supervised pre-training improves effectiveness.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Ma, Wei and Zhao, Mengjie and Soremekun, Ezekiel and Hu, Qiang and Zhang, Jie and Papadakis, Mike and Cordy, Maxime and Xie, Xiaofei and Traon, Yves Le},
	month = jan,
	year = {2022},
	note = {arXiv:2112.01218 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/WBIT9RZ6/2112.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/UGGTNK9E/Ma et al. - 2022 - GraphCode2Vec Generic Code Embedding via Lexical .pdf:application/pdf},
}

@inproceedings{li_palmtree_2021,
	title = {{PalmTree}: {Learning} an {Assembly} {Language} {Model} for {Instruction} {Embedding}},
	shorttitle = {{PalmTree}},
	url = {http://arxiv.org/abs/2103.03809},
	doi = {10.1145/3460120.3484587},
	abstract = {Deep learning has demonstrated its strengths in numerous binary analysis tasks, including function boundary detection, binary code search, function prototype inference, value set analysis, etc. When applying deep learning to binary analysis tasks, we need to decide what input should be fed into the neural network model. More specifically, we need to answer how to represent an instruction in a fixed-length vector. The idea of automatically learning instruction representations is intriguing, however the existing schemes fail to capture the unique characteristics of disassembly. These schemes ignore the complex intra-instruction structures and mainly rely on control flow in which the contextual information is noisy and can be influenced by compiler optimizations. In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora. PalmTree utilizes three pre-training tasks to capture various characteristics of assembly language. These training tasks overcome the problems in existing schemes, thus can help to generate high-quality representations. We conduct both intrinsic and extrinsic evaluations, and compare PalmTree with other instruction embedding schemes. PalmTree has the best performance for intrinsic metrics, and outperforms the other instruction embedding schemes for all downstream tasks.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Li, Xuezixiang and Yu, Qu and Yin, Heng},
	month = nov,
	year = {2021},
	note = {arXiv:2103.03809 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	pages = {3236--3251},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/NRYFHPFA/2103.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/S68XMBZ6/Li et al. - 2021 - PalmTree Learning an Assembly Language Model for .pdf:application/pdf},
}

@misc{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2023-08-01},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, t5},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/IZ873ZDU/1910.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/YX38TKNB/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@article{devanbu_deep_nodate,
	title = {Deep {Learning} \& {Software} {Engineering}: {State} of {Research} and {Future} {Directions}},
	language = {en},
	author = {Devanbu, Prem and Dwyer, Matthew and Elbaum, Sebastian and Lowry, Michael and Moran, Kevin and Poshyvanyk, Denys and Ray, Baishakhi and Singh, Rishabh and Zhang, Xiangyu},
	file = {Devanbu et al. - Deep Learning & Software Engineering State of Res.pdf:/Users/akazad/Zotero/storage/CDTDSARI/Devanbu et al. - Deep Learning & Software Engineering State of Res.pdf:application/pdf},
}

@article{zhou_automated_2022,
	title = {An {Automated} {Tool} for {Analysis} and {Tuning} of {GPU}-{Accelerated} {Code} in {HPC} {Applications}},
	volume = {33},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2021.3094169},
	abstract = {The US Department of Energy’s fastest supercomputers and forthcoming exascale systems employ Graphics Processing Units (GPUs) to increase the computational performance of compute nodes. However, the complexity of GPU architectures makes tailoring sophisticated applications to achieve high performance on GPU-accelerated systems a major challenge. At best, prior performance tools for GPU code only provide coarse-grained tuning advice at the kernel level. In this article, we describe GPA, a performance advisor that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To gather the fine-grained measurements needed to produce such insights, GPA uses instruction sampling and binary instrumentation to monitor execution of GPU code. At the time of this writing, GPU instruction sampling is only available on NVIDIA GPUs. To understand performance losses, GPA uses data flow analysis to approximately attribute measured instruction stalls back to their causes. GPA then analyzes patterns of stalls using information about a program’s structure and the GPU architecture to identify optimization strategies that address inefficiencies observed. GPA then employs detailed performance models to estimate the potential speedup that each optimization might provide. Experiments with benchmarks and applications show that GPA provides useful advice for tuning GPU code. We applied GPA to analyze and tune a collection of codes on NVIDIA V100 and A100 GPUs. GPA suggested optimizations that it estimates will accelerate performance across the set of codes by a geometric mean of 1.21×. Applying these optimizations suggested by GPA accelerated these codes by a geometric mean of 1.19×.},
	number = {4},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhou, Keren and Meng, Xiaozhu and Sai, Ryuichi and Grubisic, Dejan and Mellor-Crummey, John},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Measurement, Tools, Graphics processing units, High performance computing, Instruments, Optimization, parallel architectures, parallel programming, performance analysis, Registers, Tuning},
	pages = {854--865},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/QB86DPKI/9470950.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/FW3YPMAM/Zhou et al. - 2022 - An Automated Tool for Analysis and Tuning of GPU-A.pdf:application/pdf},
}

@misc{clement_pymt5_2020,
	title = {{PyMT5}: multi-mode translation of natural language and {Python} code with transformers},
	shorttitle = {{PyMT5}},
	url = {http://arxiv.org/abs/2010.03150},
	abstract = {Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1\% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Clement, Colin B. and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03150 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/HQBYRTMW/2010.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/G67YBD4W/Clement et al. - 2020 - PyMT5 multi-mode translation of natural language .pdf:application/pdf},
}

@misc{turzo_towards_2023,
	title = {Towards {Automated} {Classification} of {Code} {Review} {Feedback} to {Support} {Analytics}},
	url = {http://arxiv.org/abs/2307.03852},
	abstract = {Background: As improving code review (CR) effectiveness is a priority for many software development organizations, projects have deployed CR analytics platforms to identify potential improvement areas. The number of issues identified, which is a crucial metric to measure CR effectiveness, can be misleading if all issues are placed in the same bin. Therefore, a finer-grained classification of issues identified during CRs can provide actionable insights to improve CR effectiveness. Although a recent work by Fregnan et al. proposed automated models to classify CR-induced changes, we have noticed two potential improvement areas -- i) classifying comments that do not induce changes and ii) using deep neural networks (DNN) in conjunction with code context to improve performances. Aims: This study aims to develop an automated CR comment classifier that leverages DNN models to achieve a more reliable performance than Fregnan et al. Method: Using a manually labeled dataset of 1,828 CR comments, we trained and evaluated supervised learning-based DNN models leveraging code context, comment text, and a set of code metrics to classify CR comments into one of the five high-level categories proposed by Turzo and Bosu. Results: Based on our 10-fold cross-validation-based evaluations of multiple combinations of tokenization approaches, we found a model using CodeBERT achieving the best accuracy of 59.3\%. Our approach outperforms Fregnan et al.'s approach by achieving 18.7\% higher accuracy. Conclusion: Besides facilitating improved CR analytics, our proposed model can be useful for developers in prioritizing code review feedback and selecting reviewers.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Turzo, Asif Kamal and Faysal, Fahim and Poddar, Ovi and Sarker, Jaydeb and Iqbal, Anindya and Bosu, Amiangshu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03852 [cs]},
	keywords = {Computer Science - Software Engineering, bosu},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/L9Y7TR3S/2307.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/LY27QFDX/Turzo et al. - 2023 - Towards Automated Classification of Code Review Fe.pdf:application/pdf},
}

@inproceedings{liu_automatic_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Automatic detection of performance bugs in database systems using equivalent queries},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510093},
	doi = {10.1145/3510003.3510093},
	language = {en},
	urldate = {2023-08-08},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Xinyu and Zhou, Qi and Arulraj, Joy and Orso, Alessandro},
	month = may,
	year = {2022},
	keywords = {model paper},
	pages = {225--236},
	file = {Liu et al. - 2022 - Automatic detection of performance bugs in databas.pdf:/Users/akazad/Zotero/storage/Y42WUMZ3/Liu et al. - 2022 - Automatic detection of performance bugs in databas.pdf:application/pdf},
}

@inproceedings{song_performance_2017,
	title = {Performance {Diagnosis} for {Inefficient} {Loops}},
	doi = {10.1109/ICSE.2017.41},
	abstract = {Writing efficient software is difficult. Design and implementation defects can cause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-cause and fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, to provide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-time overhead without degrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performance problems shows that LDoctor can provide better coverage and accuracy than existing techniques, with low overhead.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Song, Linhai and Lu, Shan},
	month = may,
	year = {2017},
	note = {ISSN: 1558-1225},
	keywords = {Tools, Taxonomy, Software, Computer bugs, Anodes, debugging, Debugging, loop inefficiency, performance diagnosis, Redundancy, idea paper},
	pages = {370--380},
	file = {IEEE Xplore Abstract Record:/Users/akazad/Zotero/storage/5HYXIEV6/7985677.html:text/html;IEEE Xplore Full Text PDF:/Users/akazad/Zotero/storage/CZLX2KTZ/Song and Lu - 2017 - Performance Diagnosis for Inefficient Loops.pdf:application/pdf},
}

@article{tufano_learning_nodate,
	title = {Learning {Code} {Transformations} via {Neural} {Machine} {Translation}},
	language = {en},
	author = {Tufano, Michele},
	keywords = {PhD thesis},
	file = {Tufano - Learning Code Transformations via Neural Machine T.pdf:/Users/akazad/Zotero/storage/CQKYGLKG/Tufano - Learning Code Transformations via Neural Machine T.pdf:application/pdf},
}

@misc{tufano_unit_2021,
	title = {Unit {Test} {Case} {Generation} with {Transformers} and {Focal} {Context}},
	url = {http://arxiv.org/abs/2009.05617},
	abstract = {Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose AthenaTest, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25\% relative improvement and focal context providing additional 11.1\% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from GitHub. We evaluate AthenaTest on five defects4j projects, generating 25K passing test cases covering 43.7\% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3, finding that our approach outperforms GPT-3 and has comparable coverage w.r.t. EvoSuite. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards AthenaTest.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
	month = may,
	year = {2021},
	note = {arXiv:2009.05617 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/TAKDJT7E/2009.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NMF58HB3/Tufano et al. - 2021 - Unit Test Case Generation with Transformers and Fo.pdf:application/pdf},
}

@inproceedings{tufano_generating_2022,
	title = {Generating {Accurate} {Assert} {Statements} for {Unit} {Test} {Cases} using {Pretrained} {Transformers}},
	url = {http://arxiv.org/abs/2009.05634},
	doi = {10.1145/3524481.3527220},
	abstract = {Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task. In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests. The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62\% of the cases in the first attempt. The results show 80\% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.},
	urldate = {2023-08-08},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} {International} {Conference} on {Automation} of {Software} {Test}},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Sundaresan, Neel},
	month = may,
	year = {2022},
	note = {arXiv:2009.05634 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	pages = {54--64},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/N8MXFZBT/2009.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/DCAZIFVW/Tufano et al. - 2022 - Generating Accurate Assert Statements for Unit Tes.pdf:application/pdf},
}

@misc{tufano_empirical_2019,
	title = {An {Empirical} {Study} on {Learning} {Bug}-{Fixing} {Patches} in the {Wild} via {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1812.08693},
	abstract = {Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub, in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50\% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
	month = may,
	year = {2019},
	note = {arXiv:1812.08693 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/A4NE427W/1812.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/EDBZ49YJ/Tufano et al. - 2019 - An Empirical Study on Learning Bug-Fixing Patches .pdf:application/pdf},
}

@misc{madaan_learning_2023,
	title = {Learning {Performance}-{Improving} {Code} {Edits}},
	url = {http://arxiv.org/abs/2302.07867},
	abstract = {The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25\% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Madaan, Aman and Shypula, Alexander and Alon, Uri and Hashemi, Milad and Ranganathan, Parthasarathy and Yang, Yiming and Neubig, Graham and Yazdanbakhsh, Amir},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07867 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JD76NBFL/2302.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/TILKZNYC/Madaan et al. - 2023 - Learning Performance-Improving Code Edits.pdf:application/pdf},
}

@inproceedings{bui_detect-localize-repair_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Detect-{Localize}-{Repair}: {A} {Unified} {Framework} for {Learning} to {Debug} with {CodeT5}},
	shorttitle = {Detect-{Localize}-{Repair}},
	url = {https://aclanthology.org/2022.findings-emnlp.57},
	doi = {10.18653/v1/2022.findings-emnlp.57},
	abstract = {Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either one of them or approach them in a stage-wise manner, ignoring the mutual benefits between them. In this work, we propose a novel unified Detect-Localize-Repair framework based on a pretrained programming language model CodeT5 to seamlessly address these tasks, named CodeT5-DLR. Specifically, we propose three objectives to adapt the generic CodeT5 for debugging: a bug detection objective to determine whether a given code snippet is buggy or not, a bug localization objective to identify the buggy lines, and a program repair objective to translate the buggy code to its fixed version. We evaluate it on each of these tasks and their combined setting on two newly collected line-level debugging datasets in Java and Python. Extensive results show that our model significantly outperforms existing baselines from both NLP and software engineering domains.},
	urldate = {2023-08-09},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Bui, Nghi and Wang, Yue and Hoi, Steven C.H.},
	month = dec,
	year = {2022},
	pages = {812--823},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/GH2P4ATK/Bui et al. - 2022 - Detect-Localize-Repair A Unified Framework for Le.pdf:application/pdf},
}

@inproceedings{karampatsis_big_2020,
	title = {Big {Code} != {Big} {Vocabulary}: {Open}-{Vocabulary} {Models} for {Source} {Code}},
	shorttitle = {Big {Code} != {Big} {Vocabulary}},
	url = {http://arxiv.org/abs/2003.07914},
	doi = {10.1145/3377811.3380342},
	abstract = {Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	author = {Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
	month = jun,
	year = {2020},
	note = {arXiv:2003.07914 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {1073--1085},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/8ATILFSI/2003.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/DB9G7PUV/Karampatsis et al. - 2020 - Big Code != Big Vocabulary Open-Vocabulary Models.pdf:application/pdf},
}

@misc{xie_emotion_2023,
	title = {Emotion and {Sentiment} {Guided} {Paraphrasing}},
	url = {http://arxiv.org/abs/2306.05556},
	abstract = {Paraphrase generation, a.k.a. paraphrasing, is a common and important task in natural language processing. Emotional paraphrasing, which changes the emotion embodied in a piece of text while preserving its meaning, has many potential applications, including moderating online dialogues and preventing cyberbullying. We introduce a new task of fine-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in fine-grained settings following smooth variations in affective dimensions while preserving the meaning of the original text. We reconstruct several widely used paraphrasing datasets by augmenting the input and target texts with their fine-grained emotion labels. Then, we propose a framework for emotion and sentiment guided paraphrasing by leveraging pre-trained language models for conditioned text generation. Extensive evaluation of the fine-tuned models suggests that including fine-grained emotion labels in the paraphrase task significantly improves the likelihood of obtaining high-quality paraphrases that reflect the desired emotions while achieving consistently better scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Xie, Justin J. and Agrawal, Ameeta},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, paraphrase},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/SYXICZWZ/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/K3CICIAQ/Xie and Agrawal - 2023 - Emotion and Sentiment Guided Paraphrasing.pdf:application/pdf},
}

@misc{feng_survey_2021,
	title = {A {Survey} of {Data} {Augmentation} {Approaches} for {NLP}},
	url = {http://arxiv.org/abs/2105.03075},
	abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
	month = dec,
	year = {2021},
	note = {arXiv:2105.03075 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EUQIQHU9/2105.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/3GU4LQS6/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf:application/pdf},
}

@article{kumar_data_nodate,
	title = {Data {Augmentation} {Using} {Pre}-trained {Transformer} {Models}},
	abstract = {Language model based pre-trained models such as BERT have provided signiﬁcant gains across different NLP tasks. In this paper, we study different types of transformer based pretrained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classiﬁcation benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different data augmentation methods using pre-trained model differ in-terms of data diversity, and how well such methods preserve the class-label information.},
	language = {en},
	author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
	file = {Kumar et al. - Data Augmentation Using Pre-trained Transformer Mo.pdf:/Users/akazad/Zotero/storage/KXINIM7V/Kumar et al. - Data Augmentation Using Pre-trained Transformer Mo.pdf:application/pdf},
}

@inproceedings{nema_analyzing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Analyzing user perspectives on mobile app privacy at scale},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510079},
	doi = {10.1145/3510003.3510079},
	language = {en},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Nema, Preksha and Anthonysamy, Pauline and Taft, Nina and Peddinti, Sai Teja},
	month = may,
	year = {2022},
	pages = {112--124},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/QBBNDCMG/Nema et al. - 2022 - Analyzing user perspectives on mobile app privacy .pdf:application/pdf},
}

@article{souza_lexecutor_2023,
	title = {{LExecutor}: {Learning}-{Guided} {Execution}},
	abstract = {Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5\% and 98.2\%, allowing LExecutor to closely mimic real executions. As a result, the approach successfully executes significantly more code than any available technique, such as simply executing the code as-is. For example, executing the open-source code snippets as-is covers only 4.1\% of all lines, because the code crashes early on, whereas LExecutor achieves a coverage of 51.6\%.},
	language = {en},
	author = {Souza, Beatriz and Pradel, Michael},
	year = {2023},
	file = {Souza and Pradel - 2023 - LExecutor Learning-Guided Execution.pdf:/Users/akazad/Zotero/storage/6FL7E465/Souza and Pradel - 2023 - LExecutor Learning-Guided Execution.pdf:application/pdf},
}

@misc{li_hitchhikers_2023,
	title = {The {Hitchhiker}'s {Guide} to {Program} {Analysis}: {A} {Journey} with {Large} {Language} {Models}},
	shorttitle = {The {Hitchhiker}'s {Guide} to {Program} {Analysis}},
	url = {http://arxiv.org/abs/2308.00245},
	abstract = {Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated agent that interfaces with both a static analysis tool and an LLM. By carefully designing the agent and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates an extremely potent capability, showcasing a high precision (50\%) and recall rate (100\%). It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in the use of LLMs for bug discovery in extensive, real-world datasets.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
	month = jul,
	year = {2023},
	note = {arXiv:2308.00245 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/6U4WMBBS/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/IILB3UVS/Li et al. - 2023 - The Hitchhiker's Guide to Program Analysis A Jour.pdf:application/pdf},
}

@inproceedings{mahbub_defectors_2023,
	title = {Defectors: {A} {Large}, {Diverse} {Python} {Dataset} for {Defect} {Prediction}},
	shorttitle = {Defectors},
	url = {http://arxiv.org/abs/2303.04738},
	doi = {10.1109/MSR59073.2023.00085},
	abstract = {Defect prediction has been a popular research topic where machine learning (ML) and deep learning (DL) have found numerous applications. However, these ML/DL-based defect prediction models are often limited by the quality and size of their datasets. In this paper, we present Defectors, a large dataset for just-in-time and line-level defect prediction. Defectors consists of \${\textbackslash}approx\$ 213K source code files (\${\textbackslash}approx\$ 93K defective and \${\textbackslash}approx\$ 120K defect-free) that span across 24 popular Python projects. These projects come from 18 different domains, including machine learning, automation, and internet-of-things. Such a scale and diversity make Defectors a suitable dataset for training ML/DL models, especially transformer models that require large and diverse datasets. We also foresee several application areas of our dataset including defect prediction and defect explanation. Dataset link: https://doi.org/10.5281/zenodo.7708984},
	urldate = {2023-09-06},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur},
	month = may,
	year = {2023},
	note = {arXiv:2303.04738 [cs]},
	keywords = {Computer Science - Software Engineering, parvez},
	pages = {393--397},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/TDJ6CTI9/2303.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/MFP3I5U7/Mahbub et al. - 2023 - Defectors A Large, Diverse Python Dataset for Def.pdf:application/pdf},
}

@inproceedings{zeng_deep_2021,
	address = {Virtual Denmark},
	title = {Deep just-in-time defect prediction: how far are we?},
	isbn = {978-1-4503-8459-9},
	shorttitle = {Deep just-in-time defect prediction},
	url = {https://dl.acm.org/doi/10.1145/3460319.3464819},
	doi = {10.1145/3460319.3464819},
	abstract = {Defect prediction aims to automatically identify potential defective code with minimal human intervention and has been widely studied in the literature. Just-in-Time (JIT) defect prediction focuses on program changes rather than whole programs, and has been widely adopted in continuous testing. CC2Vec, state-of-the-art JIT defect prediction tool, first constructs a hierarchical attention network (HAN) to learn distributed vector representations of both code additions and deletions, and then concatenates them with two other embedding vectors representing commit messages and overall code changes extracted by the existing DeepJIT approach to train a model for predicting whether a given commit is defective. Although CC2Vec has been shown to be the state of the art for JIT defect prediction, it was only evaluated on a limited dataset and not compared with all representative baselines. Therefore, to further investigate the efficacy and limitations of CC2Vec, this paper performs an extensive study of CC2Vec on a large-scale dataset with over 310,370 changes (8.3 X larger than the original CC2Vec dataset). More specifically, we also empirically compare CC2Vec against DeepJIT and representative traditional JIT defect prediction techniques. The experimental results show that CC2Vec cannot consistently outperform DeepJIT, and neither of them can consistently outperform traditional JIT defect prediction. We also investigate the impact of individual traditional defect prediction features and find that the added-line-number feature outperforms other traditional features. Inspired by this finding, we construct a simplistic JIT defect prediction approach which simply adopts the added-linenumber feature with the logistic regression classifier. Surprisingly, such a simplistic approach can outperform CC2Vec and DeepJIT in defect prediction, and can be 81k X/120k X faster in training/testing. Furthermore, the paper also provides various practical guidelines for advancing JIT defect prediction in the near future.},
	language = {en},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zeng, Zhengran and Zhang, Yuqun and Zhang, Haotian and Zhang, Lingming},
	month = jul,
	year = {2021},
	pages = {427--438},
	file = {Zeng et al. - 2021 - Deep just-in-time defect prediction how far are w.pdf:/Users/akazad/Zotero/storage/WN5DIWGI/Zeng et al. - 2021 - Deep just-in-time defect prediction how far are w.pdf:application/pdf},
}

@misc{gururangan_dont_2020,
	title = {Don't {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}},
	shorttitle = {Don't {Stop} {Pretraining}},
	url = {http://arxiv.org/abs/2004.10964},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	urldate = {2023-09-09},
	publisher = {arXiv},
	author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	month = may,
	year = {2020},
	note = {arXiv:2004.10964 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/W49AINBX/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/RUUG7HFD/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf:application/pdf},
}

@inproceedings{su_hotgpt_2023,
	address = {Providence RI USA},
	title = {{HotGPT}: {How} to {Make} {Software} {Documentation} {More} {Useful} with a {Large} {Language} {Model}?},
	isbn = {9798400701955},
	shorttitle = {{HotGPT}},
	url = {https://dl.acm.org/doi/10.1145/3593856.3595910},
	doi = {10.1145/3593856.3595910},
	language = {en},
	urldate = {2023-09-14},
	booktitle = {Proceedings of the 19th {Workshop} on {Hot} {Topics} in {Operating} {Systems}},
	publisher = {ACM},
	author = {Su, Yiming and Wan, Chengcheng and Sethi, Utsav and Lu, Shan and Musuvathi, Madan and Nath, Suman},
	month = jun,
	year = {2023},
	pages = {87--93},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/Q8JW73YX/Su et al. - 2023 - HotGPT How to Make Software Documentation More Us.pdf:application/pdf},
}

@misc{di_grazia_diffsearch_2022,
	title = {{DiffSearch}: {A} {Scalable} and {Precise} {Search} {Engine} for {Code} {Changes}},
	shorttitle = {{DiffSearch}},
	url = {http://arxiv.org/abs/2204.02787},
	abstract = {The source code of successful projects is evolving all the time, resulting in hundreds of thousands of code changes stored in source code repositories. This wealth of data can be useful, e.g., to find changes similar to a planned code change or examples of recurring code improvements. This paper presents DiffSearch, a search engine that, given a query that describes a code change, returns a set of changes that match the query. The approach is enabled by three key contributions. First, we present a query language that extends the underlying programming language with wildcards and placeholders, providing an intuitive way of formulating queries that is easy to adapt to different programming languages. Second, to ensure scalability, the approach indexes code changes in a one-time preprocessing step, mapping them into a feature space, and then performs an efficient search in the feature space for each query. Third, to guarantee precision, i.e., that any returned code change indeed matches the given query, we present a tree-based matching algorithm that checks whether a query can be expanded to a concrete code change. We present implementations for Java, JavaScript, and Python, and show that the approach responds within seconds to queries across one million code changes, has a recall of 80.7\% for Java, 89.6\% for Python, and 90.4\% for JavaScript, enables users to find relevant code changes more effectively than a regular expression-based search, and is helpful for gathering a large-scale dataset of real-world bug fixes.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Di Grazia, Luca and Bredl, Paul and Pradel, Michael},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02787 [cs]},
	keywords = {Computer Science - Software Engineering, pradel},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/7CPU4NUR/2204.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/K362I8PA/Di Grazia et al. - 2022 - DiffSearch A Scalable and Precise Search Engine f.pdf:application/pdf},
}

@misc{li_starcoder_2023,
	title = {{StarCoder}: may the source be with you!},
	shorttitle = {{StarCoder}},
	url = {http://arxiv.org/abs/2305.06161},
	abstract = {The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40{\textbackslash}\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Davaadorj, Mishig and Lamy-Poirier, Joel and Monteiro, João and Shliazhko, Oleh and Gontier, Nicolas and Meade, Nicholas and Zebaze, Armel and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Benjamin and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Fahmy, Nour and Bhattacharyya, Urvashi and Yu, Wenhao and Singh, Swayam and Luccioni, Sasha and Villegas, Paulo and Kunakov, Maxim and Zhdanov, Fedor and Romero, Manuel and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Robinson, Jennifer and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Muñoz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	month = may,
	year = {2023},
	note = {arXiv:2305.06161 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/ATPR5J22/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/EWQ5RW59/Li et al. - 2023 - StarCoder may the source be with you!.pdf:application/pdf},
}

@misc{murali_codecompose_2023,
	title = {{CodeCompose}: {A} {Large}-{Scale} {Industrial} {Deployment} of {AI}-assisted {Code} {Authoring}},
	shorttitle = {{CodeCompose}},
	url = {http://arxiv.org/abs/2305.12050},
	abstract = {The rise of large language models (LLMs) has unlocked various applications of this technology in software development. In particular, generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces. We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. Finally, we present metrics from our large-scale deployment of CodeCompose that shows its impact on Meta's internal code authoring experience over a 15-day time window, where 4.5 million suggestions were made by CodeCompose. Quantitative metrics reveal that (i) CodeCompose has an acceptance rate of 22\% across several languages, and (ii) 8\% of the code typed by users of CodeCompose is through accepting code suggestions from CodeCompose. Qualitative feedback indicates an overwhelming 91.5\% positive reception for CodeCompose. In addition to assisting with code authoring, CodeCompose is also introducing other positive side effects such as encouraging developers to generate more in-code documentation, helping them with the discovery of new APIs, etc.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan},
	month = may,
	year = {2023},
	note = {arXiv:2305.12050 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/YDHP6VEX/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NANII7QB/Murali et al. - 2023 - CodeCompose A Large-Scale Industrial Deployment o.pdf:application/pdf},
}

@misc{cummins_large_2023,
	title = {Large {Language} {Models} for {Compiler} {Optimization}},
	url = {http://arxiv.org/abs/2309.07062},
	abstract = {We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0\% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91\% of the time and perfectly emulating the output of the compiler 70\% of the time.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Elhoushi, Mostafa and Liang, Youwei and Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Hazelwood, Kim and Synnaeve, Gabriel and Leather, Hugh},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/9E6XC8D7/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/N9XEPPIP/Cummins et al. - 2023 - Large Language Models for Compiler Optimization.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, gpt4},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/CHPQECRV/2303.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/VYBQPR4U/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2023-09-29},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/Z4XIXJZG/1310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/RWIPN9TC/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@misc{allen-zhu_physics_2023,
	title = {Physics of {Language} {Models}: {Part} 3.2, {Knowledge} {Manipulation}},
	shorttitle = {Physics of {Language} {Models}},
	url = {http://arxiv.org/abs/2309.14402},
	abstract = {Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?") We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored and fully extractable in the models, and despite adequate instruct fine-tuning.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.14402 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/KG8QKJGL/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/BEAU2SE7/Allen-Zhu and Li - 2023 - Physics of Language Models Part 3.2, Knowledge Ma.pdf:application/pdf},
}

@misc{chen_reconcile_2023,
	title = {{ReConcile}: {Round}-{Table} {Conference} {Improves} {Reasoning} via {Consensus} among {Diverse} {LLMs}},
	shorttitle = {{ReConcile}},
	url = {http://arxiv.org/abs/2309.13007},
	abstract = {Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcile determines the final answer by leveraging the confidence of each agent in a weighted voting scheme. We implement ReConcile with ChatGPT, Bard, and Claude2 as the three agents. Our experimental results on various benchmarks demonstrate that ReConcile significantly enhances the reasoning performance of the agents (both individually and as a team), surpassing prior single-agent and multi-agent baselines by 7.7\% and also outperforming GPT-4 on some of these datasets. We also experiment with GPT-4 itself as one of the agents in ReConcile and demonstrate that its initial performance also improves by absolute 10.0\% through discussion and feedback from other agents. Finally, we also analyze the accuracy after every round and observe that ReConcile achieves better and faster consensus between agents, compared to a multi-agent debate baseline. Our code is available at: https://github.com/dinobby/ReConcile},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Chen, Justin Chih-Yao and Saha, Swarnadeep and Bansal, Mohit},
	month = sep,
	year = {2023},
	note = {arXiv:2309.13007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/RXV5XBL9/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/HY6K4U24/Chen et al. - 2023 - ReConcile Round-Table Conference Improves Reasoni.pdf:application/pdf},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9794014},
	urldate = {2023-10-06},
	file = {IEEE Xplore Full-Text PDF\::/Users/akazad/Zotero/storage/ZWEJDZVF/stamp.html:text/html},
}

@misc{noauthor_ieee_nodate-2,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9794014},
	urldate = {2023-10-06},
	file = {IEEE Xplore Full-Text PDF\::/Users/akazad/Zotero/storage/VHC8JUL3/stamp.html:text/html},
}

@article{wu_how_2023,
	title = {How {Effective} {Are} {Neural} {Networks} for {Fixing} {Security} {Vulnerabilities}},
	abstract = {Security vulnerability repair is a di cult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically x software bugs.},
	language = {en},
	author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
	year = {2023},
	file = {Wu et al. - 2023 - How Effective Are Neural Networks for Fixing Secur.pdf:/Users/akazad/Zotero/storage/6XQJSNZX/Wu et al. - 2023 - How Effective Are Neural Networks for Fixing Secur.pdf:application/pdf},
}

@misc{seff_motionlm_2023,
	title = {{MotionLM}: {Multi}-{Agent} {Motion} {Forecasting} as {Language} {Modeling}},
	shorttitle = {{MotionLM}},
	url = {http://arxiv.org/abs/2309.16534},
	abstract = {Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-the-art performance for multi-agent motion prediction on the Waymo Open Motion Dataset, ranking 1st on the interactive challenge leaderboard.},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Seff, Ari and Cera, Brian and Chen, Dian and Ng, Mason and Zhou, Aurick and Nayakanti, Nigamaa and Refaat, Khaled S. and Al-Rfou, Rami and Sapp, Benjamin},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16534 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, multimodal},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/BD6VTFX5/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/AMY8K7QR/Seff et al. - 2023 - MotionLM Multi-Agent Motion Forecasting as Langua.pdf:application/pdf},
}

@inproceedings{cheng_efficient_2023,
	address = {Singapore Singapore},
	title = {Efficient {Data} {Representation} {Learning} in {Google}-scale {Systems}},
	isbn = {9798400702419},
	url = {https://dl.acm.org/doi/10.1145/3604915.3608882},
	doi = {10.1145/3604915.3608882},
	language = {en},
	urldate = {2023-10-08},
	booktitle = {Proceedings of the 17th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Cheng, Derek Zhiyuan and Wang, Ruoxi and Kang, Wang-Cheng and Coleman, Benjamin and Zhang, Yin and Ni, Jianmo and Valverde, Jonathan and Hong, Lichan and Chi, Ed},
	month = sep,
	year = {2023},
	pages = {267--271},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/BCZ7WH54/Cheng et al. - 2023 - Efficient Data Representation Learning in Google-s.pdf:application/pdf},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception, ML},
	file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/Users/akazad/Zotero/storage/GKHZ3J3P/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{noauthor_dataflow_2024,
	title = {Dataflow {Analysis}-{Inspired} {Deep} {Learning} for {Efficient} {Vulnerability} {Detection}},
	abstract = {Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysisinspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.},
	language = {en},
	journal = {Dataflow Analysis},
	year = {2024},
	file = {2024 - Dataflow Analysis-Inspired Deep Learning for Effic.pdf:/Users/akazad/Zotero/storage/27NDCQI2/2024 - Dataflow Analysis-Inspired Deep Learning for Effic.pdf:application/pdf},
}

@misc{steenhoek_dataflow_2023,
	title = {Dataflow {Analysis}-{Inspired} {Deep} {Learning} for {Efficient} {Vulnerability} {Detection}},
	url = {http://arxiv.org/abs/2212.08108},
	abstract = {Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysisinspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.},
	language = {en},
	urldate = {2023-10-10},
	publisher = {arXiv},
	author = {Steenhoek, Benjamin and Gao, Hongyang and Le, Wei},
	month = oct,
	year = {2023},
	note = {arXiv:2212.08108 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Steenhoek et al. - 2023 - Dataflow Analysis-Inspired Deep Learning for Effic.pdf:/Users/akazad/Zotero/storage/VYTFD7B7/Steenhoek et al. - 2023 - Dataflow Analysis-Inspired Deep Learning for Effic.pdf:application/pdf},
}

@misc{tehranijamsaz_perfograph_2023,
	title = {{PERFOGRAPH}: {A} {Numerical} {Aware} {Program} {Graph} {Representation} for {Performance} {Optimization} and {Program} {Analysis}},
	shorttitle = {{PERFOGRAPH}},
	url = {http://arxiv.org/abs/2306.00210},
	abstract = {The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages, which directly impacts the ability of machine learning methods to reason about programs. The absence of numerical awareness, composite data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the composite data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness. These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture program intricate dependencies and semantics. Consequently, it serves as a powerful tool for various applications such as program analysis, performance optimization, and parallelism discovery. Our experimental results demonstrate that PERFOGRAPH outperforms existing representations and sets new state-of-the-art results by reducing the error rate by 7.4\% (AMD dataset) and 10\% (NVIDIA dataset) in the well-known Device Mapping challenge. It also sets new state-of-the-art results in various performance optimization tasks like Parallelism Discovery and Numa and Prefetchers Configuration prediction.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {TehraniJamsaz, Ali and Mahmud, Quazi Ishtiaque and Chen, Le and Ahmed, Nasreen K. and Jannesari, Ali},
	month = may,
	year = {2023},
	note = {arXiv:2306.00210 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/HCH7BJLK/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/H3YUDDHY/TehraniJamsaz et al. - 2023 - PERFOGRAPH A Numerical Aware Program Graph Repres.pdf:application/pdf},
}

@article{hassan_uniloc_2023,
	title = {{UniLoc}: {Unified} {Fault} {Localization} of {Continuous} {Integration} {Failures}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	shorttitle = {{UniLoc}},
	url = {https://dl.acm.org/doi/10.1145/3593799},
	doi = {10.1145/3593799},
	abstract = {Continuous integration (CI) practices encourage developers to frequently integrate code into a shared repository. Each integration is validated by automatic build and testing such that errors are revealed as early as possible. When CI failures or integration errors are reported, existing techniques are insufficient to automatically locate the root causes for two reasons. First, a CI failure may be triggered by faults in source code and/or build scripts, whereas current approaches consider only source code. Second, a tentative integration can fail because of build failures and/or test failures, whereas existing tools focus on test failures only. This article presents UniLoc, the first unified technique to localize faults in
              both
              source code and build scripts given a CI failure log, without assuming the failure’s location (source code or build scripts) and nature (a test failure or not). Adopting the information retrieval (IR) strategy, UniLoc locates buggy files by treating source code and build scripts as documents to search and by considering build logs as search queries. However, instead of naïvely applying an off-the-shelf IR technique to these software artifacts, for more accurate fault localization, UniLoc applies various domain-specific heuristics to optimize the search queries, search space, and ranking formulas. To evaluate UniLoc, we gathered 700 CI failure fixes in 72 open source projects that are built with Gradle. UniLoc could effectively locate bugs with the average mean reciprocal rank value as 0.49, mean average precision value as 0.36, and normalized discounted cumulative gain value as 0.54. UniLoc outperformed the state-of-the-art IR-based tool BLUiR and Locus. UniLoc has the potential to help developers diagnose root causes for CI failures more accurately and efficiently.},
	language = {en},
	number = {6},
	urldate = {2023-10-12},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Hassan, Foyzul and Meng, Na and Wang, Xiaoyin},
	month = nov,
	year = {2023},
	pages = {1--31},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/452R455J/Hassan et al. - 2023 - UniLoc Unified Fault Localization of Continuous I.pdf:application/pdf},
}

@article{ramadan_novel_nodate,
	title = {{NOVEL} {REPRESENTATION} {LEARNING} {TECHNIQUE} {USING} {GRAPHS} {FOR} {PERFORMANCE} {ANALYTICS}},
	language = {en},
	author = {Ramadan, Tarek},
	file = {Ramadan - NOVEL REPRESENTATION LEARNING TECHNIQUE USING GRAP.pdf:/Users/akazad/Zotero/storage/LABI68QB/Ramadan - NOVEL REPRESENTATION LEARNING TECHNIQUE USING GRAP.pdf:application/pdf},
}

@inproceedings{maneriker_urltran_2021,
	address = {San Diego, CA, USA},
	title = {{URLTran}: {Improving} {Phishing} {URL} {Detection} {Using} {Transformers}},
	isbn = {978-1-66543-956-5},
	shorttitle = {{URLTran}},
	url = {https://ieeexplore.ieee.org/document/9653028/},
	doi = {10.1109/MILCOM52596.2021.9653028},
	abstract = {Browsers often include security features to detect phishing web pages. In the past, some browsers evaluated an unknown URL for inclusion in a list of known phishing pages. However, as the number of URLs and known phishing pages continued to increase at a rapid pace, browsers started to include one or more machine learning classifiers as part of their security services that aim to better protect end users from harm. While additional information could be used, browsers typically evaluate every unknown URL using some classifier in order to quickly detect these phishing pages. Early phishing detection used standard machine learning classifiers, but recent research has instead proposed the use of deep learning models for the phishing URL detection task. Concurrently, text embedding research using transformers has led to state-of-the-art results in many natural language processing tasks. In this work, we perform a comprehensive analysis of transformer models on the phishing URL detection task. We consider standard masked language model and additional domain-specific pre-training tasks, and compare these models to fine-tuned BERT and RoBERTa models. Combining the insights from these experiments, we propose URLTran which uses transformers to significantly improve the performance of phishing URL detection over a wide range of very low false positive rates (FPRs) compared to other deep learning-based methods. For example, URLTran yields a true positive rate (TPR) of 86.80\% compared to 71.20\% for the next best baseline at an FPR of 0.01\%, resulting in a relative improvement of over 21.9\%. Further, we consider some classical adversarial black-box phishing attacks such as those based on homoglyphs and compound word splits to improve the robustness of URLTran. We consider additional fine tuning with these adversarial samples and demonstrate that URLTran can maintain low FPRs under these scenarios.},
	language = {en},
	urldate = {2023-10-17},
	booktitle = {{MILCOM} 2021 - 2021 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	publisher = {IEEE},
	author = {Maneriker, Pranav and Stokes, Jack W. and Lazo, Edir Garcia and Carutasu, Diana and Tajaddodianfar, Farid and Gururajan, Arun},
	month = nov,
	year = {2021},
	pages = {197--204},
	file = {Maneriker et al. - 2021 - URLTran Improving Phishing URL Detection Using Tr.pdf:/Users/akazad/Zotero/storage/3R7XLEVY/Maneriker et al. - 2021 - URLTran Improving Phishing URL Detection Using Tr.pdf:application/pdf},
}

@article{lee_catbert_2020,
	title = {{CATBERT}: {CONTEXT}-{AWARE} {TINY} {BERT} {FOR} {DETECTING} {SOCIAL} {ENGINEERING} {EMAILS}},
	abstract = {Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached ﬁles or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don’t contain any malicious code and don’t share word choices with known attacks. To tackle this problem, we ﬁne-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efﬁciently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email’s content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87\% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83\%, 79\%, and 54\% detection rates at false positive rates of 1\%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.},
	language = {en},
	author = {Lee, Younghoo and Saxe, Joshua and Harang, Richard},
	year = {2020},
	file = {Lee et al. - 2020 - CATBERT CONTEXT-AWARE TINY BERT FOR DETECTING SOC.pdf:/Users/akazad/Zotero/storage/7XRMC49D/Lee et al. - 2020 - CATBERT CONTEXT-AWARE TINY BERT FOR DETECTING SOC.pdf:application/pdf},
}

@misc{lee_catbert_2020-1,
	title = {{CATBERT}: {Context}-{Aware} {Tiny} {BERT} for {Detecting} {Social} {Engineering} {Emails}},
	shorttitle = {{CATBERT}},
	url = {http://arxiv.org/abs/2010.03484},
	abstract = {Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached files or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don't contain any malicious code and don't share word choices with known attacks. To tackle this problem, we fine-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efficiently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email's content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87\% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83\%, 79\%, and 54\% detection rates at false positive rates of 1\%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Lee, Younghoo and Saxe, Joshua and Harang, Richard},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03484 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/Q8C49V9A/2010.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/VQAN4FEJ/Lee et al. - 2020 - CATBERT Context-Aware Tiny BERT for Detecting Soc.pdf:application/pdf},
}

@article{baral_optimizing_nodate,
	title = {Optimizing {Continuous} {Development} {By} {Detecting} and {Preventing} {Unnecessary} {Content} {Generation}},
	abstract = {Continuous development (CD) helps developers quickly release and update their software. To enact CD, developers customize their CD builds to perform several tasks, including compiling, testing, static analysis checks, etc. However, as developers add more tasks to their builds, the builds take longer to run, therefore slowing down the entire CD process. Furthermore, developers may unknowingly include tasks into their builds whose results are not used (e.g., generating coverage ﬁles that are never read or uploaded anywhere), therefore wasting build runtime doing unnecessary tasks.},
	language = {en},
	author = {Baral, Talank and Rahman, Shanto and Chanumolu, Bala Naren and Balcı, Basak and Tuncer, Tuna and Shi, August and Lam, Wing},
	file = {Baral et al. - Optimizing Continuous Development By Detecting and.pdf:/Users/akazad/Zotero/storage/FEP4N5C8/Baral et al. - Optimizing Continuous Development By Detecting and.pdf:application/pdf},
}

@article{rahman_source_2019,
	title = {Source code properties of defective infrastructure as code scripts},
	volume = {112},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584919300965},
	doi = {10.1016/j.infsof.2019.04.013},
	abstract = {Objective: The objective of this paper is to help practitioners in increasing the quality of infrastructure as code (IaC) scripts through an empirical study that identiﬁes source code properties of defective IaC scripts.
Methodology: We apply qualitative analysis on defect-related commits mined from open source software repositories to identify source code properties that correlate with defective IaC scripts. Next, we survey practitioners to assess the practitioner’s agreement level with the identiﬁed properties. We also construct defect prediction models using the identiﬁed properties for 2439 scripts collected from four datasets.
Results: We identify 10 source code properties that correlate with defective IaC scripts. Of the identiﬁed 10 properties we observe lines of code and hard-coded string i.e. putting strings as conﬁguration values, to show the strongest correlation with defective IaC scripts. According to our survey analysis, majority of the practitioners show agreement for two properties: include, the property of executing external modules or scripts, and hard-coded string. Using the identiﬁed properties, our constructed defect prediction models show a precision of 0.70∼0.78, and a recall of 0.54∼0.67.
Conclusion: Based on our ﬁndings, we recommend practitioners to allocate suﬃcient inspection and testing eﬀorts on IaC scripts that include any of the identiﬁed 10 source code properties of IaC scripts.},
	language = {en},
	urldate = {2023-10-26},
	journal = {Information and Software Technology},
	author = {Rahman, Akond and Williams, Laurie},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Software Engineering},
	pages = {148--163},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JRLKGE28/1810.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/L234WZK5/Rahman and Williams - 2019 - Source Code Properties of Defective Infrastructure.pdf:application/pdf;Rahman and Williams - 2019 - Source code properties of defective infrastructure.pdf:/Users/akazad/Zotero/storage/4NKYNL3G/Rahman and Williams - 2019 - Source code properties of defective infrastructure.pdf:application/pdf},
}

@misc{nichols_modeling_2023,
	title = {Modeling {Parallel} {Programs} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.17281},
	abstract = {Parallel software codes in high performance computing (HPC) continue to grow in complexity and scale as we enter the exascale era. A diverse set of emerging hardware and programming paradigms make developing, optimizing, and maintaining parallel software burdensome for developers. One way to alleviate some of these burdens is with automated development and analysis tools. Such tools can perform complex and/or remedial tasks for developers that increase their productivity and decrease the chance for error. So far, such tools for code development and performance analysis have been limited in the complexity of tasks they can perform. However, with recent advancements in language modeling, and the wealth of code related data that is now available online, these tools have started to utilize predictive language models to automate more complex tasks. In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes. We train LLMs using code and performance data that is specific to parallel codes. We compare several recent LLMs on HPC related tasks and introduce a new model, HPC-Coder, trained on parallel code. In our experiments we show that this model can auto-complete HPC functions where general models cannot, decorate for loops with OpenMP pragmas, and model performance changes in two scientific application repositories.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Nichols, Daniel and Marathe, Aniruddha and Menon, Harshitha and Gamblin, Todd and Bhatele, Abhinav},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17281 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/TJP9WA97/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/MBXCWIKP/Nichols et al. - 2023 - Modeling Parallel Programs using Large Language Mo.pdf:application/pdf},
}

@article{spadini_pydriller_2018,
	title = {{PyDriller}: {Python} {Framework} for {Mining} {Software} {Repositories}},
	abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present Pydriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that Pydriller can achieve the same results with, on average, 50\% less LOC and significantly lower complexity.},
	language = {en},
	author = {Spadini, Davide and Aniche, Maurício and Bacchelli, Alberto},
	year = {2018},
	file = {Spadini et al. - 2018 - PyDriller Python Framework for Mining Software Re.pdf:/Users/akazad/Zotero/storage/2AEZVJ75/Spadini et al. - 2018 - PyDriller Python Framework for Mining Software Re.pdf:application/pdf},
}

@article{ratner_accelerating_nodate,
	title = {{ACCELERATING} {MACHINE} {LEARNING} {WITH} {TRAINING} {DATA} {MANAGEMENT}},
	language = {en},
	author = {Ratner, Alexander Jason},
	file = {Ratner - ACCELERATING MACHINE LEARNING WITH TRAINING DATA M.pdf:/Users/akazad/Zotero/storage/UZLN6AQT/Ratner - ACCELERATING MACHINE LEARNING WITH TRAINING DATA M.pdf:application/pdf},
}

@misc{ratner_data_2017,
	title = {Data {Programming}: {Creating} {Large} {Training} {Sets}, {Quickly}},
	shorttitle = {Data {Programming}},
	url = {http://arxiv.org/abs/1605.07723},
	abstract = {Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can "denoise" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Ratner, Alexander and De Sa, Christopher and Wu, Sen and Selsam, Daniel and Ré, Christopher},
	month = jan,
	year = {2017},
	note = {arXiv:1605.07723 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/TZYY5KHT/1605.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/QMAKKA7V/Ratner et al. - 2017 - Data Programming Creating Large Training Sets, Qu.pdf:application/pdf},
}

@article{tufano_learning_nodate-1,
	title = {Learning {Code} {Transformations} via {Neural} {Machine} {Translation}},
	language = {en},
	author = {Tufano, Michele},
	file = {Tufano - Learning Code Transformations via Neural Machine T.pdf:/Users/akazad/Zotero/storage/BYD7D9HE/Tufano - Learning Code Transformations via Neural Machine T.pdf:application/pdf},
}

@misc{zhang_binary_2022,
	title = {Binary {Classification} with {Positive} {Labeling} {Sources}},
	url = {http://arxiv.org/abs/2208.01704},
	abstract = {To create a large amount of training labels for machine learning models effectively and efficiently, researchers have turned to Weak Supervision (WS), which uses programmatic labeling sources rather than manual annotation. Existing works of WS for binary classification typically assume the presence of labeling sources that are able to assign both positive and negative labels to data in roughly balanced proportions. However, for many tasks of interest where there is a minority positive class, negative examples could be too diverse for developers to generate indicative labeling sources. Thus, in this work, we study the application of WS on binary classification tasks with positive labeling sources only. We propose WEAPO, a simple yet competitive WS method for producing training labels without negative labeling sources. On 10 benchmark datasets, we show WEAPO achieves the highest averaged performance in terms of both the quality of synthesized labels and the performance of the final classifier supervised with these labels. We incorporated the implementation of {\textbackslash}method into WRENCH, an existing benchmarking platform.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Zhang, Jieyu and Wang, Yujing and Yang, Yaming and Luo, Yang and Ratner, Alexander},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01704 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/BCN37MRC/2208.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/XW6HLTI5/Zhang et al. - 2022 - Binary Classification with Positive Labeling Sourc.pdf:application/pdf},
}

@article{fang_weak_nodate,
	title = {Weak {Supervision} in the {Age} of {Transfer} {Learning} for {Natural} {Language} {Processing}},
	language = {en},
	author = {Fang, Fei and Xie, Zihan},
	file = {Fang and Xie - Weak Supervision in the Age of Transfer Learning f.pdf:/Users/akazad/Zotero/storage/X4PJXQ5Z/Fang and Xie - Weak Supervision in the Age of Transfer Learning f.pdf:application/pdf},
}

@article{zhang_lancet_2021,
	title = {{LANCET}: labeling complex data at scale},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {{LANCET}},
	url = {https://dl.acm.org/doi/10.14778/3476249.3476269},
	doi = {10.14778/3476249.3476269},
	abstract = {Cutting-edge machine learning techniques often require millions of labeled data objects to train a robust model. Because relying on humans to supply such a huge number of labels is rarely practical, automated methods for label generation are needed. Unfortunately, critical challenges in auto-labeling remain unsolved, including the following research questions: (1) which objects to ask humans to label, (2) how to automatically propagate labels to other objects, and (3) when to stop labeling. These three questions are not only each challenging in their own right, but they also correspond to tightly interdependent problems. Yet existing techniques provide at best isolated solutions to a subset of these challenges. In this work, we propose the first approach, called LANCET, that successfully addresses all three challenges in an integrated framework. LANCET is based on a theoretical foundation characterizing the properties that the labeled dataset must satisfy to train an effective prediction model, namely the Covariate-shift and the Continuity conditions. First, guided by the Covariate-shift condition, LANCET maps raw input data into a semantic feature space, where an unlabeled object is expected to share the same label with its near-by labeled neighbor. Next, guided by the Continuity condition, LANCET selects objects for labeling, aiming to ensure that unlabeled objects always have some sufficiently close labeled neighbors. These two strategies jointly maximize the accuracy of the automatically produced labels and the prediction accuracy of the machine learning models trained on these labels. Lastly, LANCET uses a distribution matching network to verify whether both the Covariate-shift and Continuity conditions hold, in which case it would be safe to terminate the labeling process. Our experiments on diverse public data sets demonstrate that LANCET consistently outperforms the stateof-the-art methods from Snuba to GOGGLES and other baselines by a large margin – up to 30 percentage points increase in accuracy.},
	language = {en},
	number = {11},
	urldate = {2023-11-21},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zhang, Huayi and Cao, Lei and Madden, Samuel and Rundensteiner, Elke},
	month = jul,
	year = {2021},
	pages = {2154--2166},
	file = {Zhang et al. - 2021 - LANCET labeling complex data at scale.pdf:/Users/akazad/Zotero/storage/LJCKNK7Y/Zhang et al. - 2021 - LANCET labeling complex data at scale.pdf:application/pdf},
}

@misc{ratner_training_2018,
	title = {Training {Complex} {Models} with {Multi}-{Task} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/1810.02840},
	abstract = {As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Ratner, Alexander and Hancock, Braden and Dunnmon, Jared and Sala, Frederic and Pandey, Shreyash and Ré, Christopher},
	month = dec,
	year = {2018},
	note = {arXiv:1810.02840 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/ZQMRPWXM/1810.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/892Q5WW8/Ratner et al. - 2018 - Training Complex Models with Multi-Task Weak Super.pdf:application/pdf},
}

@misc{gao_making_2021,
	title = {Making {Pre}-trained {Language} {Models} {Better} {Few}-shot {Learners}},
	url = {http://arxiv.org/abs/2012.15723},
	abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
	month = jun,
	year = {2021},
	note = {arXiv:2012.15723 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EUS53UVQ/2012.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/P9PNA5V5/Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf:application/pdf},
}

@misc{boecking_interactive_2021,
	title = {Interactive {Weak} {Supervision}: {Learning} {Useful} {Heuristics} for {Data} {Labeling}},
	shorttitle = {Interactive {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2012.06046},
	abstract = {Obtaining large annotated datasets is critical for training successful machine learning models and it is often a bottleneck in practice. Weak supervision offers a promising alternative for producing labeled datasets without ground truth annotations by generating probabilistic labels using multiple noisy heuristics. This process can scale to large datasets and has demonstrated state of the art performance in diverse domains such as healthcare and e-commerce. One practical issue with learning from user-generated heuristics is that their creation requires creativity, foresight, and domain expertise from those who hand-craft them, a process which can be tedious and subjective. We develop the ﬁrst framework for interactive weak supervision in which a method proposes heuristics and learns from user feedback given on each proposed heuristic. Our experiments demonstrate that only a small number of feedback iterations are needed to train models that achieve highly competitive test set performance without access to ground truth training labels. We conduct user studies, which show that users are able to effectively provide feedback on heuristics and that test set results track the performance of simulated oracles.},
	language = {en},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Boecking, Benedikt and Neiswanger, Willie and Xing, Eric and Dubrawski, Artur},
	month = jan,
	year = {2021},
	note = {arXiv:2012.06046 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Boecking et al. - 2021 - Interactive Weak Supervision Learning Useful Heur.pdf:/Users/akazad/Zotero/storage/6D6W7RAB/Boecking et al. - 2021 - Interactive Weak Supervision Learning Useful Heur.pdf:application/pdf},
}

@article{varma_snuba_2018,
	title = {Snuba: automating weak supervision to label training data},
	volume = {12},
	issn = {2150-8097},
	shorttitle = {Snuba},
	url = {https://dl.acm.org/doi/10.14778/3291264.3291268},
	doi = {10.14778/3291264.3291268},
	abstract = {As deep learning models are applied to increasingly diverse problems, a key bottleneck is gathering enough high-quality training labels tailored to each task. Users therefore turn to weak supervision, relying on imperfect sources of labels like pattern matching and user-deﬁned heuristics. Unfortunately, users have to design these sources for each task. This process can be time consuming and expensive: domain experts often perform repetitive steps like guessing optimal numerical thresholds and developing informative text patterns. To address these challenges, we present Snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. Snuba generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data. We develop a statistical measure that guarantees the iterative process will automatically terminate before it degrades training label quality. Snuba automatically generates heuristics in under ﬁve minutes and performs up to 9.74 F1 points better than the best known user-deﬁned heuristics developed over many days. In collaborations with users at research labs, Stanford Hospital, and on open source datasets, Snuba outperforms other automated approaches like semisupervised learning by up to 14.35 F1 points.},
	language = {en},
	number = {3},
	urldate = {2023-11-29},
	journal = {Proceedings of the VLDB Endowment},
	author = {Varma, Paroma and Ré, Christopher},
	month = nov,
	year = {2018},
	pages = {223--236},
	file = {Varma and Ré - 2018 - Snuba automating weak supervision to label traini.pdf:/Users/akazad/Zotero/storage/43PVD4VN/Varma and Ré - 2018 - Snuba automating weak supervision to label traini.pdf:application/pdf},
}

@inproceedings{alberto_tubespam_2015,
	address = {Miami, FL},
	title = {{TubeSpam}: {Comment} {Spam} {Filtering} on {YouTube}},
	isbn = {978-1-5090-0287-0},
	shorttitle = {{TubeSpam}},
	url = {http://ieeexplore.ieee.org/document/7424299/},
	doi = {10.1109/ICMLA.2015.37},
	abstract = {The proﬁtability promoted by Google in its brand new video distribution platform YouTube has attracted an increasing number of users. However, such success has also attracted malicious users, which aim to self-promote their videos or disseminate viruses and malwares. Since YouTube offers limited tools for comment moderation, the spam volume is shockingly increasing which lead owners of famous channels to disable the comments section in their videos. Automatic comment spam ﬁltering on YouTube is a challenge even for established classiﬁcation methods, since the messages are very short and often rife with slangs, symbols and abbreviations. In this work, we have evaluated several top-performance classiﬁcation techniques for such purpose. The statistical analysis of results indicate that, with 99.9\% of conﬁdence level, decision trees, logistic regression, Bernoulli Na¨ıve Bayes, random forests, linear and Gaussian SVMs are statistically equivalent. Based on this, we have also offered the TubeSpam – an accurate online system to ﬁlter comments posted on YouTube.},
	language = {en},
	urldate = {2023-11-30},
	booktitle = {2015 {IEEE} 14th {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Alberto, Tulio C. and Lochter, Johannes V. and Almeida, Tiago A.},
	month = dec,
	year = {2015},
	pages = {138--143},
	file = {Alberto et al. - 2015 - TubeSpam Comment Spam Filtering on YouTube.pdf:/Users/akazad/Zotero/storage/3ZYYEJGP/Alberto et al. - 2015 - TubeSpam Comment Spam Filtering on YouTube.pdf:application/pdf},
}

@misc{chen_chatgpts_2023,
	title = {{ChatGPT}'s {One}-year {Anniversary}: {Are} {Open}-{Source} {Large} {Language} {Models} {Catching} up?},
	shorttitle = {{ChatGPT}'s {One}-year {Anniversary}},
	url = {http://arxiv.org/abs/2311.16989},
	abstract = {Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Chen, Hailin and Jiao, Fangkai and Li, Xingxuan and Qin, Chengwei and Ravaut, Mathieu and Zhao, Ruochen and Xiong, Caiming and Joty, Shafiq},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16989 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/NREPLQYQ/2311.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/27RB4NR9/Chen et al. - 2023 - ChatGPT's One-year Anniversary Are Open-Source La.pdf:application/pdf},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/Z2WCPENB/2002.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/V6THT5HF/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf},
}

@inproceedings{celik_regression_2017,
	address = {Paderborn Germany},
	title = {Regression test selection across {JVM} boundaries},
	isbn = {978-1-4503-5105-8},
	url = {https://dl.acm.org/doi/10.1145/3106237.3106297},
	doi = {10.1145/3106237.3106297},
	abstract = {Modern software development processes recommend that changes be integrated into the main development line of a project multiple times a day. Before a new revision may be integrated, developers practice regression testing to ensure that the latest changes do not break any previously established functionality. The cost of regression testing is high, due to an increase in the number of revisions that are introduced per day, as well as the number of tests developers write per revision. Regression test selection (RTS) optimizes regression testing by skipping tests that are not affected by recent project changes. Existing dynamic RTS techniques support only projects written in a single programming language, which is unfortunate knowing that an open-source project is on average written in several programming languages. We present the first dynamic RTS technique that does not stop at predefined language boundaries. Our technique dynamically detects, at the operating system level, all file artifacts a test depends on. Our technique is, hence, oblivious to the specific means the test uses to actually access the files: be it through spawning a new process, invoking a system call, invoking a library written in a different language, invoking a library that spawns a process which makes a system call, etc. We also provide a set of extension points which allow for a smooth integration with testing frameworks and build systems. We implemented our technique in a tool called RTSLinux as a loadable Linux kernel module and evaluated it on 21 Java projects that escape the JVM by spawning new processes or invoking native code, totaling 2,050,791 lines of code. Our results show that RTSLinux, on average, skips 74.17\% of tests and saves 52.83\% of test execution time compared to executing all tests.},
	language = {en},
	urldate = {2023-12-09},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Celik, Ahmet and Vasic, Marko and Milicevic, Aleksandar and Gligoric, Milos},
	month = aug,
	year = {2017},
	pages = {809--820},
	file = {Celik et al. - 2017 - Regression test selection across JVM boundaries.pdf:/Users/akazad/Zotero/storage/CI7K4XLK/Celik et al. - 2017 - Regression test selection across JVM boundaries.pdf:application/pdf},
}

@article{ren_chianti_nodate,
	title = {Chianti: {A} {Tool} for {Change} {Impact} {Analysis} of {Java} {Programs}},
	abstract = {This paper reports on the design and implementation of Chianti, a change impact analysis tool for Java that is implemented in the context of the Eclipse environment. Chianti analyzes two versions of an application and decomposes their diﬀerence into a set of atomic changes. Change impact is then reported in terms of aﬀected (regression or unit) tests whose execution behavior may have been modiﬁed by the applied changes. For each aﬀected test, Chianti also determines a set of aﬀecting changes that were responsible for the test’s modiﬁed behavior. This latter step of isolating the changes that induce the failure of one speciﬁc test from those changes that only aﬀect other tests can be used as a debugging technique in situations where a test fails unexpectedly after a long editing session.},
	language = {en},
	author = {Ren, Xiaoxia and Shah, Fenil and Tip, Frank and Ryder, Barbara G and Chesley, Ophelia},
	file = {Ren et al. - Chianti A Tool for Change Impact Analysis of Java.pdf:/Users/akazad/Zotero/storage/33FRWAYE/Ren et al. - Chianti A Tool for Change Impact Analysis of Java.pdf:application/pdf},
}

@misc{sharma_survey_2022,
	title = {A {Survey} on {Machine} {Learning} {Techniques} for {Source} {Code} {Analysis}},
	url = {http://arxiv.org/abs/2110.09610},
	abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 479 primary studies published between 2011 and 2021. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Sharma, Tushar and Kechagia, Maria and Georgiou, Stefanos and Tiwari, Rohit and Vats, Indira and Moazen, Hadi and Sarro, Federica},
	month = sep,
	year = {2022},
	note = {arXiv:2110.09610 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, survey},
	file = {arXiv Fulltext PDF:/Users/akazad/Zotero/storage/EYSQ2CMS/Sharma et al. - 2022 - A Survey on Machine Learning Techniques for Source.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/F9DRRQ8I/2110.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/6UY8CX7D/Sharma et al. - 2022 - A Survey on Machine Learning Techniques for Source.pdf:application/pdf},
}

@article{le_goues_genprog_2012,
	title = {{GenProg}: {A} {Generic} {Method} for {Automatic} {Software} {Repair}},
	volume = {38},
	issn = {0098-5589},
	shorttitle = {{GenProg}},
	url = {http://ieeexplore.ieee.org/document/6035728/},
	doi = {10.1109/TSE.2011.104},
	abstract = {This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.},
	language = {en},
	number = {1},
	urldate = {2023-12-15},
	journal = {IEEE Transactions on Software Engineering},
	author = {Le Goues, Claire and Nguyen, ThanhVu and Forrest, Stephanie and Weimer, Westley},
	month = jan,
	year = {2012},
	pages = {54--72},
	file = {Le Goues et al. - 2012 - GenProg A Generic Method for Automatic Software R.pdf:/Users/akazad/Zotero/storage/3CVF5CQ7/Le Goues et al. - 2012 - GenProg A Generic Method for Automatic Software R.pdf:application/pdf},
}

@article{al-albaani_prophets_nodate,
	title = {The {Prophet}’s {Prayer} {Described}},
	language = {en},
	author = {Al-Albaani, Sheikh Muhammed Naasir-ud-Deen},
	file = {Al-Albaani - The Prophet’s Prayer Described.pdf:/Users/akazad/Zotero/storage/LI2U5KZX/Al-Albaani - The Prophet’s Prayer Described.pdf:application/pdf},
}

@article{nichols_modeling_nodate,
	title = {Modeling {Parallel} {Programs} using {Large} {Language} {Models}},
	language = {en},
	author = {Nichols, Daniel and Marathe, Aniruddha and Menon, Harshitha and Gamblin, Todd and Bhatele, Abhinav},
	file = {Nichols et al. - Modeling Parallel Programs using Large Language Mo.pdf:/Users/akazad/Zotero/storage/K9CWIXAG/Nichols et al. - Modeling Parallel Programs using Large Language Mo.pdf:application/pdf},
}

@misc{nichols_can_2024,
	title = {Can {Large} {Language} {Models} {Write} {Parallel} {Code}?},
	url = {http://arxiv.org/abs/2401.12554},
	abstract = {Large Language Models are becoming an increasingly popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for more complex tasks. In this paper, we explore the ability of state-of-the-art language models to generate parallel code. We propose a benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for comparing parallel code generation performance and use them to explore how well each LLM performs on various parallel programming models and computational problem types.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Nichols, Daniel and Davis, Joshua H. and Xie, Zhaojun and Rajaram, Arjun and Bhatele, Abhinav},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12554 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/2JZTACYL/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/UJYVBVGD/Nichols et al. - 2024 - Can Large Language Models Write Parallel Code.pdf:application/pdf},
}

@misc{fatima_flakyfix_2024,
	title = {{FlakyFix}: {Using} {Large} {Language} {Models} for {Predicting} {Flaky} {Test} {Fix} {Categories} and {Test} {Code} {Repair}},
	shorttitle = {{FlakyFix}},
	url = {http://arxiv.org/abs/2307.00012},
	abstract = {Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky test cases where the root cause of flakiness is in the test case itself and not in the production code. Our key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, in addition to informing testers, we augment a Large Language Model (LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions. The results show that our suggested fix category labels significantly enhance the capability of GPT 3.5 Turbo, in generating fixes for flaky tests.},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Fatima, Sakina and Hemmati, Hadi and Briand, Lionel},
	month = jan,
	year = {2024},
	note = {arXiv:2307.00012 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/PXIIZIY4/2307.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/KX9FFUW5/Fatima et al. - 2024 - FlakyFix Using Large Language Models for Predicti.pdf:application/pdf},
}

@inproceedings{liang_how_2019,
	address = {Berlin, Germany},
	title = {How to {Explain} a {Patch}: {An} {Empirical} {Study} of {Patch} {Explanations} in {Open} {Source} {Projects}},
	isbn = {978-1-72814-982-0},
	shorttitle = {How to {Explain} a {Patch}},
	url = {https://ieeexplore.ieee.org/document/8987552/},
	doi = {10.1109/ISSRE.2019.00016},
	abstract = {Bugs are inevitable in software development and maintenance processes. Recently a lot of research efforts have been devoted to automatic program repair, aiming to reduce the efforts of debugging. However, since it is difﬁcult to ensure that the generated patches meet all quality requirements such as correctness, developers still need to review the patch. In addition, current techniques produce only patches without explanation, making it difﬁcult for the developers to understand the patch. Therefore, we believe a more desirable approach should generate not only the patch but also an explanation of the patch.},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Liang, Jingjing and Hou, Yaozong and Zhou, Shurui and Chen, Junjie and Xiong, Yingfei and Huang, Gang},
	month = oct,
	year = {2019},
	keywords = {patch explain},
	pages = {58--69},
	file = {Liang et al. - 2019 - How to Explain a Patch An Empirical Study of Patc.pdf:/Users/akazad/Zotero/storage/QIC66RY4/Liang et al. - 2019 - How to Explain a Patch An Empirical Study of Patc.pdf:application/pdf},
}

@article{tufano_empirical_2019-1,
	title = {An {Empirical} {Study} on {Learning} {Bug}-{Fixing} {Patches} in the {Wild} via {Neural} {Machine} {Translation}},
	volume = {28},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3340544},
	doi = {10.1145/3340544},
	abstract = {Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50\% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},
	language = {en},
	number = {4},
	urldate = {2024-02-02},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
	month = oct,
	year = {2019},
	pages = {1--29},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/DSLDXMGI/Tufano et al. - 2019 - An Empirical Study on Learning Bug-Fixing Patches .pdf:application/pdf},
}

@article{liu_atom_2022,
	title = {{ATOM}: {Commit} {Message} {Generation} {Based} on {Abstract} {Syntax} {Tree} and {Hybrid} {Ranking}},
	volume = {48},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {{ATOM}},
	url = {https://ieeexplore.ieee.org/document/9261989/},
	doi = {10.1109/TSE.2020.3038681},
	language = {en},
	number = {5},
	urldate = {2024-02-02},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liu, Shangqing and Gao, Cuiyun and Chen, Sen and Nie, Lun Yiu and Liu, Yang},
	month = may,
	year = {2022},
	pages = {1800--1817},
	file = {Liu et al. - 2022 - ATOM Commit Message Generation Based on Abstract .pdf:/Users/akazad/Zotero/storage/HXZFDHWD/Liu et al. - 2022 - ATOM Commit Message Generation Based on Abstract .pdf:application/pdf},
}

@inproceedings{johnson_why_2013,
	address = {San Francisco, CA, USA},
	title = {Why don't software developers use static analysis tools to find bugs?},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606613/},
	doi = {10.1109/ICSE.2013.6606613},
	abstract = {Using static analysis tools for automating code inspections can be beneﬁcial for software engineers. Such tools can make ﬁnding bugs, or software defects, faster and cheaper than manual inspections. Despite the beneﬁts of using static analysis tools to ﬁnd bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneﬁcial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers ﬁx defects.},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
	month = may,
	year = {2013},
	pages = {672--681},
	file = {Johnson et al. - 2013 - Why don't software developers use static analysis .pdf:/Users/akazad/Zotero/storage/TUBQSZ3M/Johnson et al. - 2013 - Why don't software developers use static analysis .pdf:application/pdf},
}

@inproceedings{le_dilavrec_hyperdiff_2023,
	address = {San Francisco CA USA},
	title = {{HyperDiff}: {Computing} {Source} {Code} {Diffs} at {Scale}},
	isbn = {9798400703270},
	shorttitle = {{HyperDiff}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616312},
	doi = {10.1145/3611643.3616312},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Le Dilavrec, Quentin and Khelladi, Djamel Eddine and Blouin, Arnaud and Jézéquel, Jean-Marc},
	month = nov,
	year = {2023},
	pages = {288--299},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/X47ZLR3Z/Le Dilavrec et al. - 2023 - HyperDiff Computing Source Code Diffs at Scale.pdf:application/pdf},
}

@article{koyuncu_fixminer_2020,
	title = {{FixMiner}: {Mining} relevant fix patterns for automated program repair},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	shorttitle = {{FixMiner}},
	url = {http://link.springer.com/10.1007/s10664-019-09780-z},
	doi = {10.1007/s10664-019-09780-z},
	abstract = {Patching is a common activity in software development. It is generally performed on a source code base to address bugs or add new functionalities. In this context, given the recurrence of bugs across projects, the associated similar patches can be leveraged to extract generic fix actions. While the literature includes various approaches leveraging similarity among patches to guide program repair, these approaches often do not yield fix patterns that are tractable and reusable as actionable input to APR systems. In this paper, we propose a systematic and automated approach to mining relevant and actionable fix patterns based on an iterative clustering strategy applied to atomic changes within patches. The goal of FixMiner is thus to infer separate and reusable fix patterns that can be leveraged in other patch generation systems. Our technique, FixMiner, leverages Rich Edit Script which is a specialized tree structure of the edit scripts that captures the AST-level context of the code changes. FixMiner uses different tree representations of Rich Edit Scripts for each round of clustering to identify similar changes. These are abstract syntax trees, edit actions trees, and code context trees. We have evaluated FixMiner on thousands of software patches collected from open source projects. Preliminary results show that we are able to mine accurate patterns, efficiently exploiting change information in Rich Edit Scripts. We further integrated the mined patterns to an automated program repair prototype, PARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J benchmark. Beyond this quantitative performance, we show that the mined fix patterns are sufficiently relevant to produce patches with a high probability of correctness: 81\% of PARFixMiner’s generated plausible patches are correct.},
	language = {en},
	number = {3},
	urldate = {2024-02-02},
	journal = {Empirical Software Engineering},
	author = {Koyuncu, Anil and Liu, Kui and Bissyandé, Tegawendé F. and Kim, Dongsun and Klein, Jacques and Monperrus, Martin and Le Traon, Yves},
	month = may,
	year = {2020},
	pages = {1980--2024},
	file = {Koyuncu et al. - 2020 - FixMiner Mining relevant fix patterns for automat.pdf:/Users/akazad/Zotero/storage/G3Z4CK9J/Koyuncu et al. - 2020 - FixMiner Mining relevant fix patterns for automat.pdf:application/pdf},
}

@inproceedings{frick_understanding_2020,
	address = {Seoul South Korea},
	title = {Understanding software changes: extracting, classifying, and presenting fine-grained source code changes},
	isbn = {978-1-4503-7122-3},
	shorttitle = {Understanding software changes},
	url = {https://dl.acm.org/doi/10.1145/3377812.3381400},
	doi = {10.1145/3377812.3381400},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {ACM},
	author = {Frick, Veit},
	month = jun,
	year = {2020},
	pages = {226--229},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/DB4CN5JT/Frick - 2020 - Understanding software changes extracting, classi.pdf:application/pdf},
}

@inproceedings{asaduzzaman_lhdiff_2013,
	address = {Eindhoven, Netherlands},
	title = {{LHDiff}: {A} {Language}-{Independent} {Hybrid} {Approach} for {Tracking} {Source} {Code} {Lines}},
	isbn = {978-0-7695-4981-1},
	shorttitle = {{LHDiff}},
	url = {http://ieeexplore.ieee.org/document/6676894/},
	doi = {10.1109/ICSM.2013.34},
	abstract = {Tracking source code lines between two different versions of a ﬁle is a fundamental step for solving a number of important problems in software maintenance such as locating bug introducing changes, tracking code fragments or defects across versions, merging ﬁle versions, and software evolution analysis. Although a number of such approaches are available in the literature, their performance is sensitive to the kind and degree of source code changes. There is also a marked lack of study on the effect of change types on source location tracking techniques. In this paper, we propose a language-independent technique, LHDiff, for tracking source code lines across versions that leverages simhash technique together with heuristics to improve accuracy. We evaluate our approach against state-of-theart techniques using benchmarks containing different degrees of changes where ﬁles are selected from real world applications. We further evaluate LHDiff with other techniques using a mutation based analysis to understand how different types of changes affect their performance. The results reveal that our technique is more effective than language-independent approaches and no worse than some language-dependent techniques. In our study LHDiff even shows better performance than a state-of-the-art languagedependent approach. In addition, we also discuss limitations of different line tracking techniques including ours and propose future research directions.},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {2013 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE},
	author = {Asaduzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A. and Penta, Massimiliano Di},
	month = sep,
	year = {2013},
	pages = {230--239},
	file = {Asaduzzaman et al. - 2013 - LHDiff A Language-Independent Hybrid Approach for.pdf:/Users/akazad/Zotero/storage/5SIWF9AQ/Asaduzzaman et al. - 2013 - LHDiff A Language-Independent Hybrid Approach for.pdf:application/pdf},
}

@misc{grossman_llvm_2023,
	title = {{LLVM} {IR} {Dataset} {Utils}},
	url = {https://zenodo.org/doi/10.5281/zenodo.10155760},
	abstract = {Code is increasingly becoming a core data modality of modern machine learning research impacting not only the way we write code with conversational agents like OpenAI’s ChatGPT, Google’s Bard, or Anthropic’s Claude, the way we translate code from one language into another, but also the compiler infrastructure underlying the language. While modeling approaches may vary and representations differ, the targeted tasks often remain the same within the individual classes of models. Relying solely on the ability of modern models to extract information from unstructured code does not take advantage of 70 years of programming language and compiler development by not utilizing the structure inherent to programs in the data collection. This detracts from the performance of models working over a tokenized representation of input code and precludes the use of these models in the compiler itself. To work towards better intermediate representation (IR) based models, we fully utilize the LLVM compiler infrastructure, shared by a number of languages, to generate a 182B token dataset of LLVM IR with a 144B token public version 2. We generated this dataset from programming languages built on the shared LLVM infrastructure, including Rust, Swift, Julia, and C/C++, by hooking into LLVM code generation either through the language’s package manager or the compiler directly to extract the dataset of intermediate representations from production grade programs. Our dataset shows great promise for large language model training, and machine-learned compiler components.},
	urldate = {2024-02-05},
	publisher = {Zenodo},
	author = {Grossman, Aiden and Paehler, Ludger and Parasyris, Konstantinos and Ben-Nun, Tal and Hegna, Jacob and Moses, William and Monsalve Diaz, Jose Manuel and Trofin, Mircea and Doerfert, Johannes},
	month = nov,
	year = {2023},
	doi = {10.5281/ZENODO.10155760},
}

@article{grossman_compile_nodate,
	title = {{ComPile}: {A} {Large} {IR} {Dataset} from {Production} {Sources}},
	abstract = {Code is increasingly becoming a core data modality of modern machine learning research impacting not only the way we write code with conversational agents like OpenAI’s ChatGPT, Google’s Bard, or Anthropic’s Claude, the way we translate code from one language into another, but also the compiler infrastructure underlying the language. While modeling approaches may vary and representations differ, the targeted tasks often remain the same within the individual classes of models. Relying solely on the ability of modern models to extract information from unstructured code does not take advantage of 70 years of programming language and compiler development by not utilizing the structure inherent to programs in the data collection. This detracts from the performance of models working over a tokenized representation of input code and precludes the use of these models in the compiler itself. To work towards better intermediate representation (IR) based models, we fully utilize the LLVM compiler infrastructure, shared by a number of languages, to generate a 182B token dataset of LLVM IR with a 144B token public version 2. We generated this dataset from programming languages built on the shared LLVM infrastructure, including Rust, Swift, Julia, and C/C++, by hooking into LLVM code generation either through the language’s package manager or the compiler directly to extract the dataset of intermediate representations from production grade programs. Our dataset shows great promise for large language model training, and machine-learned compiler components.},
	language = {en},
	author = {Grossman, Aiden and Paehler, Ludger and Parasyris, Konstantinos and Ben-Nun, Tal and Hegna, Jacob and Moses, William and Diaz, Jose M Monsalve and Trofin, Mircea and Doerfert, Johannes},
	file = {Grossman et al. - ComPile A Large IR Dataset from Production Source.pdf:/Users/akazad/Zotero/storage/6DH2X5AX/Grossman et al. - ComPile A Large IR Dataset from Production Source.pdf:application/pdf},
}

@article{cummins_programl_nodate,
	title = {{PROGRAML}: {A} {Graph}-based {Program} {Representation} for {Data} {Flow} {Analysis} and {Compiler} {Optimizations}},
	abstract = {Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data ﬂow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufﬁciently able to reason about programs. We formulate data ﬂow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose PROGRAML – Program Graphs for Machine Learning – a languageindependent, portable representation of program semantics. PROGRAML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.},
	language = {en},
	author = {Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoeﬂer, Torsten and O’Boyle, Michael and Leather, Hugh},
	keywords = {programl},
	file = {Cummins et al. - PROGRAML A Graph-based Program Representation for.pdf:/Users/akazad/Zotero/storage/QHGQ8GIA/Cummins et al. - PROGRAML A Graph-based Program Representation for.pdf:application/pdf},
}

@inproceedings{falleri_fine-grained_2014,
	address = {Västeras, Sweden},
	title = {Fine-grained and {Accurate} {Source} {Code} {Differencing}},
	url = {https://hal.science/hal-01054552},
	doi = {10.1145/2642937.2642982},
	abstract = {At the heart of software evolution is a sequence of edit actions, called an "edit script", made to a source code file. Since software systems are stored version by version, the edit script has to be computed from these versions, which is known as a complex task. Existing approaches usually compute edit scripts at the text granularity with only "add line" and "delete line" actions. However, inferring syntactic changes from such an edit script is hard. Since moving code is a frequent action performed when editing code and it should also be taken into account. In this paper, we tackle these issues by introducing an algorithm computing edit scripts at the abstract syntax tree granularity including move actions. Our objective is to compute edit scripts that are short and close to the original developer intent. Our algorithm is implemented in a freely-available and extensible tool that has been intensively validated.},
	urldate = {2024-02-09},
	booktitle = {Proceedings of the {International} {Conference} on {Automated} {Software} {Engineering}},
	author = {Falleri, Jean-Rémy and Morandat, Floréal and Blanc, Xavier and Martinez, Matias and Monperrus, Martin},
	year = {2014},
	keywords = {gumtree},
	pages = {313--324},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/XFU3KTG8/Falleri et al. - 2014 - Fine-grained and Accurate Source Code Differencing.pdf:application/pdf},
}

@article{yaraghi_automated_nodate,
	title = {Automated {Test} {Case} {Repair} {Using} {Language} {Models}},
	abstract = {Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers’ time. To address this challenge, we present TARGET (TEST REPAIR GENERATOR), a novel approach leveraging pre-trained code language models for automated test case repair. TARGET treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TARBENCH, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TARGET’s effectiveness, achieving a 66.1\% exact match accuracy. Furthermore, our study examines the effectiveness of TARGET across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects.},
	language = {en},
	author = {Yaraghi, Ahmadreza Saboor and Holden, Darren and Kahani, Nafiseh and Briand, Lionel},
	file = {Yaraghi et al. - Automated Test Case Repair Using Language Models.pdf:/Users/akazad/Zotero/storage/TYH96EMQ/Yaraghi et al. - Automated Test Case Repair Using Language Models.pdf:application/pdf},
}

@inproceedings{xu_data_2023,
	address = {Melbourne, Australia},
	title = {Data {Quality} {Matters}: {A} {Case} {Study} of {Obsolete} {Comment} {Detection}},
	isbn = {978-1-66545-701-9},
	shorttitle = {Data {Quality} {Matters}},
	url = {https://ieeexplore.ieee.org/document/10172689/},
	doi = {10.1109/ICSE48619.2023.00074},
	abstract = {Machine learning methods have achieved great success in many software engineering tasks. However, as a datadriven paradigm, how would the data quality impact the effectiveness of these methods remains largely unexplored. In this paper, we explore this problem under the context of just-in-time obsolete comment detection. Speciﬁcally, we ﬁrst conduct data cleaning on the existing benchmark dataset, and empirically observe that with only 0.22\% label corrections and even 15.0\% fewer data, the existing obsolete comment detection approaches can achieve up to 10.7\% relative accuracy improvement. To further mitigate the data quality issues, we propose an adversarial learning framework to simultaneously estimate the data quality and make the ﬁnal predictions. Experimental evaluations show that this adversarial learning framework can further improve the relative accuracy by up to 18.1\% compared to the state-of-the-art method. Although our current results are from the obsolete comment detection problem, we believe that the proposed two-phase solution, which handles the data quality issues through both the data aspect and the algorithm aspect, is also generalizable and applicable to other machine learning based software engineering tasks.},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Xu, Shengbin and Yao, Yuan and Xu, Feng and Gu, Tianxiao and Xu, Jingwei and Ma, Xiaoxing},
	month = may,
	year = {2023},
	pages = {781--793},
	file = {Xu et al. - 2023 - Data Quality Matters A Case Study of Obsolete Com.pdf:/Users/akazad/Zotero/storage/4HQT8LS9/Xu et al. - 2023 - Data Quality Matters A Case Study of Obsolete Com.pdf:application/pdf},
}

@article{lin_detecting_nodate,
	title = {Detecting {API} {Post}-{Handling} {Bugs} {Using} {Code} and {Description} in {Patches}},
	abstract = {Program APIs must be used in accordance with their specifications. API post-handling (APH) is a common type of specification that deals with APIs’ return checks, resource releases, etc. Violation of APH specifications (aka, APH bug) could cause serious security problems, including memory corruption, resource leaks, etc. API documents, as a good source of APH specifications, are often analyzed to extract specifications for APH bug detection. However, documents are not always complete, which makes many bugs fail to be detected. In this paper, we find that patches could be another good source of APH specifications. In addition to the code differences introduced by patches, patches also contain descriptions, which help to accurately extract APH specifications. In order to make bug detection accurate and efficient, we design API specification-based graph for reducing the number of paths to be analyzed and perform partial path-sensitive analysis. We implement a prototype named APHP (API Post-Handling bugs detector using Patches) for static detection of APH bugs. We evaluate APHP on four popular open-source programs, including the Linux kernel, QEMU, Git and Redis, and detect 410 new bugs, outperforming existing state-of-the-art work. 216 of the bugs have been confirmed by the maintainers, and 2 CVEs have been assigned. Some bugs have existed for more than 12 years. Till now, many submitted patches have been backported to long-term stable versions of the Linux kernel.},
	language = {en},
	author = {Lin, Miaoqian and Chen, Kai and Xiao, Yang},
	file = {Lin et al. - Detecting API Post-Handling Bugs Using Code and De.pdf:/Users/akazad/Zotero/storage/LLSTDHHM/Lin et al. - Detecting API Post-Handling Bugs Using Code and De.pdf:application/pdf},
}

@inproceedings{an_fonte_2023,
	address = {Melbourne, Australia},
	title = {Fonte: {Finding} {Bug} {Inducing} {Commits} from {Failures}},
	isbn = {978-1-66545-701-9},
	shorttitle = {Fonte},
	url = {https://ieeexplore.ieee.org/document/10172540/},
	doi = {10.1109/ICSE48619.2023.00059},
	abstract = {A Bug Inducing Commit (BIC) is a commit that introduces a software bug into the codebase. Knowing the relevant BIC for a given bug can provide valuable information for debugging as well as bug triaging. However, existing BIC identiﬁcation techniques are either too expensive (because they require the failing tests to be executed against previous versions for bisection) or inapplicable at the debugging time (because they require post hoc artefacts such as bug reports or bug ﬁxes). We propose FONTE, an efﬁcient and accurate BIC identiﬁcation technique that only requires test coverage. FONTE combines Fault Localisation (FL) with BIC identiﬁcation and ranks commits based on the suspiciousness of the code elements that they modiﬁed. FONTE reduces the search space of BICs using failure coverage as well as a ﬁlter that detects commits that are merely style changes. Our empirical evaluation using 130 real-world BICs shows that FONTE signiﬁcantly outperforms state-of-theart BIC identiﬁcation techniques based on Information Retrieval as well as neural code embedding models, achieving at least 39\% higher MRR. We also report that the ranking scores produced by FONTE can be used to perform weighted bisection, further reducing the cost of BIC identiﬁcation. Finally, we apply FONTE to a large-scale industry project with over 10M lines of code, and show that it can rank the actual BIC within the top ﬁve commits for 87\% of the studied real batch-testing failures, and save the BIC inspection cost by 32\% on average.},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {An, Gabin and Hong, Jingun and Kim, Naryeong and Yoo, Shin},
	month = may,
	year = {2023},
	pages = {589--601},
	file = {An et al. - 2023 - Fonte Finding Bug Inducing Commits from Failures.pdf:/Users/akazad/Zotero/storage/D46X9RDQ/An et al. - 2023 - Fonte Finding Bug Inducing Commits from Failures.pdf:application/pdf},
}

@article{silva_repairllama_2017,
	title = {{RepairLLaMA}: {Efficient} {Representations} and {Fine}-{Tuned} {Adapters} for {Program} {Repair}},
	abstract = {Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-theart parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective ‘program repair adapter’ for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameterefficient fine-tuning helps fine-tuning to converge and contributes to the effectiveness of the repair adapter to fix data-points outside the fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 125 Defects4J v2 and 82 HumanEval-Java bugs, outperforming all baselines.},
	language = {en},
	author = {Silva, André and Fang, Sen and Monperrus, Martin},
	year = {2017},
	file = {Silva et al. - 2017 - RepairLLaMA Efficient Representations and Fine-Tu.pdf:/Users/akazad/Zotero/storage/CILMVAIY/Silva et al. - 2017 - RepairLLaMA Efficient Representations and Fine-Tu.pdf:application/pdf},
}

@article{wang_bugpre_2023,
	title = {{BugPre}: an intelligent software version-to-version bug prediction system using graph convolutional neural networks},
	volume = {9},
	issn = {2199-4536, 2198-6053},
	shorttitle = {{BugPre}},
	url = {https://link.springer.com/10.1007/s40747-022-00848-w},
	doi = {10.1007/s40747-022-00848-w},
	abstract = {Since defects in software may cause product fault and ﬁnancial loss, it is essential to conduct software defect prediction (SDP) to identify the potentially defective modules, especially in the early stage of the software development lifecycle. Recently, cross-version defect prediction (CVDP) began to draw increasing research interests, employing the labeled defect data of the prior version within the same project to predict defects in the current version. As software development is a dynamic process, the data distribution (such as defects) during version change may get changed. Recent studies utilize machine learning (ML) techniques to detect software defects. However, due to the close dependencies between the updated and unchanged code, prior ML-based methods fail to model the long and deep dependencies, causing a high false positive. Furthermore, traditional defect detection is performed on the entire project, and the detection efﬁciency is relatively low, especially on large-scale software projects. To this end, we propose BugPre, a CVDP approach to address these two issues. BugPre is a novel framework that only conducts efﬁcient defect prediction on changed modules in the current version. BugPre utilizes variable propagation tree-based associated analysis method to obtain the changed modules in the current version. Besides, BugPre constructs graph leveraging code context dependences and uses a graph convolutional neural network to learn representative characteristics of code, thereby improving defect prediction capability when version changes occur. Through extensive experiments on opensource Apache projects, the experimental results indicate that our BugPre outperforms three state-of-the-art defect detection approaches, and the F1-score has increased by higher than 16\%.},
	language = {en},
	number = {4},
	urldate = {2024-02-09},
	journal = {Complex \& Intelligent Systems},
	author = {Wang, Zixu and Tong, Weiyuan and Li, Peng and Ye, Guixin and Chen, Hao and Gong, Xiaoqing and Tang, Zhanyong},
	month = aug,
	year = {2023},
	pages = {3835--3855},
	file = {Wang et al. - 2023 - BugPre an intelligent software version-to-version.pdf:/Users/akazad/Zotero/storage/LWW65UUZ/Wang et al. - 2023 - BugPre an intelligent software version-to-version.pdf:application/pdf},
}

@inproceedings{sejfia_toward_2024,
	address = {Lisbon Portugal},
	title = {Toward {Improved} {Deep} {Learning}-based {Vulnerability} {Detection}},
	isbn = {9798400702174},
	url = {https://dl.acm.org/doi/10.1145/3597503.3608141},
	doi = {10.1145/3597503.3608141},
	abstract = {Deep learning (DL) has been a common thread across several recent techniques for vulnerability detection. The rise of large, publicly available datasets of vulnerabilities has fueled the learning process underpinning these techniques. While these datasets help the DL-based vulnerability detectors, they also constrain these detectors’ predictive abilities. Vulnerabilities in these datasets have to be represented in a certain way, e.g., code lines, functions, or program slices within which the vulnerabilities exist. We refer to this representation as a base unit. The detectors learn how base units can be vulnerable and then predict whether other base units are vulnerable. We have hypothesized that this focus on individual base units harms the ability of the detectors to properly detect those vulnerabilities that span multiple base units (or MBU vulnerabilities). For vulnerabilities such as these, a correct detection occurs when all comprising base units are detected as vulnerable. Verifying how existing techniques perform in detecting all parts of a vulnerability is important to establish their effectiveness for other downstream tasks. To evaluate our hypothesis, we conducted a study focusing on three prominent DL-based detectors: ReVeal, DeepWukong, and LineVul. Our study shows that all three detectors contain MBU vulnerabilities in their respective datasets. Further, we observed significant accuracy drops when detecting these types of vulnerabilities. We present our study and a framework that can be used to help DL-based detectors toward the proper inclusion of MBU vulnerabilities.},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {Proceedings of the 46th {IEEE}/{ACM} {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Sejfia, Adriana and Das, Satyaki and Shafiq, Saad and Medvidović, Nenad},
	month = feb,
	year = {2024},
	pages = {1--12},
	file = {Sejfia et al. - 2024 - Toward Improved Deep Learning-based Vulnerability .pdf:/Users/akazad/Zotero/storage/WBBS9QY3/Sejfia et al. - 2024 - Toward Improved Deep Learning-based Vulnerability .pdf:application/pdf},
}

@inproceedings{le_dilavrec_hyperdiff_2023-1,
	address = {San Francisco CA USA},
	title = {{HyperDiff}: {Computing} {Source} {Code} {Diffs} at {Scale}},
	isbn = {9798400703270},
	shorttitle = {{HyperDiff}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616312},
	doi = {10.1145/3611643.3616312},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Le Dilavrec, Quentin and Khelladi, Djamel Eddine and Blouin, Arnaud and Jézéquel, Jean-Marc},
	month = nov,
	year = {2023},
	pages = {288--299},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/8FTYJL5F/Le Dilavrec et al. - 2023 - HyperDiff Computing Source Code Diffs at Scale.pdf:application/pdf},
}

@inproceedings{hu_deep_2018,
	address = {Gothenburg Sweden},
	title = {Deep code comment generation},
	isbn = {978-1-4503-5714-2},
	url = {https://dl.acm.org/doi/10.1145/3196321.3196334},
	doi = {10.1145/3196321.3196334},
	language = {en},
	urldate = {2024-02-10},
	booktitle = {Proceedings of the 26th {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
	month = may,
	year = {2018},
	pages = {200--210},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/UPQ2WA5Z/Hu et al. - 2018 - Deep code comment generation.pdf:application/pdf},
}

@misc{noauthor_homework-1_nodate,
	title = {Homework-1, {SpeedGrader}, {Winter} 2024 - {CIS}-310-001-{Computer} {Org} and {Assembly} {Lang}},
	url = {https://canvas.umd.umich.edu/courses/535825/gradebook/speed_grader?assignment_id=4833450&student_id=1878640},
	urldate = {2024-02-12},
	file = {Homework-1, SpeedGrader, Winter 2024 - CIS-310-001-Computer Org and Assembly Lang:/Users/akazad/Zotero/storage/WB9CGQJ3/speed_grader.html:text/html},
}

@inproceedings{li_deepcommenter_2020,
	address = {Virtual Event USA},
	title = {{DeepCommenter}: a deep code comment generation tool with hybrid lexical and syntactical information},
	isbn = {978-1-4503-7043-1},
	shorttitle = {{DeepCommenter}},
	url = {https://dl.acm.org/doi/10.1145/3368089.3417926},
	doi = {10.1145/3368089.3417926},
	abstract = {As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods.},
	language = {en},
	urldate = {2024-02-12},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Li, Boao and Yan, Meng and Xia, Xin and Hu, Xing and Li, Ge and Lo, David},
	month = nov,
	year = {2020},
	pages = {1571--1575},
	file = {Li et al. - 2020 - DeepCommenter a deep code comment generation tool.pdf:/Users/akazad/Zotero/storage/KGMJRWT7/Li et al. - 2020 - DeepCommenter a deep code comment generation tool.pdf:application/pdf},
}

@misc{gao_learning_2024,
	title = {Learning in the {Wild}: {Towards} {Leveraging} {Unlabeled} {Data} for {Effectively} {Tuning} {Pre}-trained {Code} {Models}},
	shorttitle = {Learning in the {Wild}},
	url = {http://arxiv.org/abs/2401.01060},
	doi = {10.1145/3597503.3639216},
	abstract = {Pre-trained code models have recently achieved substantial improvements in many code intelligence tasks. These models are first pre-trained on large-scale unlabeled datasets in a task-agnostic manner using self-supervised learning, and then fine-tuned on labeled datasets in downstream tasks. However, the labeled datasets are usually limited in size (i.e., human intensive efforts), which may hinder the performance of pre-trained code models in specific tasks. To mitigate this, one possible solution is to leverage the large-scale unlabeled data in the tuning stage by pseudo-labeling. However, directly employing the pseudo-labeled data can bring a large amount of noise, i.e., incorrect labels, leading to suboptimal performance. How to effectively leverage the noisy pseudo-labeled data is a challenging yet under-explored problem.In this paper, we propose a novel approach named HINT to improve pre-trained code models with large-scale unlabeled datasets by better utilizing the pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled data selection and Noise-tolerant Training. In the hybrid pseudo-data selection module, considering the robustness issue, apart from directly measuring the quality of pseudo labels through training loss, we further propose to employ a retrieval-based method to filter low-quality pseudo-labeled data. The noise-tolerant training module aims to further mitigate the influence of errors in pseudo labels by training the model with a noise-tolerant loss function and by regularizing the consistency of model predictions.The experimental results show that HINT can better leverage those unlabeled data in a task-specific way and provide complementary benefits for pre-trained models, e.g., improving the best baseline model by 15.33\%, 16.50\%, and 8.98\% on code summarization, defect detection, and assertion generation, respectively.},
	urldate = {2024-02-12},
	author = {Gao, Shuzheng and Mao, Wenxin and Gao, Cuiyun and Li, Li and Hu, Xing and Xia, Xin and Lyu, Michael R.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.01060 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/QIGQVTTH/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/D2HL4Q5M/Gao et al. - 2024 - Learning in the Wild Towards Leveraging Unlabeled.pdf:application/pdf},
}

@article{chen_code_2024,
	title = {Code {Search} {Is} {All} {You} {Need}? {Improving} {Code} {Suggestions} with {Code} {Search}},
	abstract = {Modern integrated development environments (IDEs) provide various automated code suggestion techniques (e.g., code completion and code generation) to help developers improve their efficiency. Such techniques may retrieve similar code snippets from the code base or leverage deep learning models to provide code suggestions. However, how to effectively enhance the code suggestions using code retrieval has not been systematically investigated. In this paper, we study and explore a retrieval-augmented framework for code suggestions. Specifically, our framework leverages different retrieval approaches and search strategies to search similar code snippets. Then the retrieved code is used to further enhance the performance of language models on code suggestions. We conduct experiments by integrating different language models into our framework and compare the results with their original models. We find that our framework noticeably improves the performance of both code completion and code generation by up to 53.8\% and 130.8\% in terms of BLEU-4, respectively. Our study highlights that integrating the retrieval process into code suggestions can improve the performance of code suggestions by a large margin.},
	language = {en},
	author = {Chen, Junkai and Hu, Xing and Li, Zhenhao and Gao, Cuiyun and Xia, Xin and Lo, David},
	year = {2024},
	file = {Chen et al. - 2024 - Code Search Is All You Need Improving Code Sugges.pdf:/Users/akazad/Zotero/storage/SHA8IWMP/Chen et al. - 2024 - Code Search Is All You Need Improving Code Sugges.pdf:application/pdf},
}

@misc{alon_general_2018,
	title = {A {General} {Path}-{Based} {Representation} for {Predicting} {Program} {Properties}},
	url = {http://arxiv.org/abs/1803.09544},
	abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming and increase programmer productivity. A major challenge when learning from programs is \${\textbackslash}textit\{how to represent programs in a way that facilitates effective learning\}\$. We present a \${\textbackslash}textit\{general path-based representation\}\$ for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C{\textbackslash}\#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	month = apr,
	year = {2018},
	note = {arXiv:1803.09544 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/3VPE9LZE/1803.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/XXVCWCDR/Alon et al. - 2018 - A General Path-Based Representation for Predicting.pdf:application/pdf},
}

@misc{liu_codemind_2024,
	title = {{CodeMind}: {A} {Framework} to {Challenge} {Large} {Language} {Models} for {Code} {Reasoning}},
	shorttitle = {{CodeMind}},
	url = {http://arxiv.org/abs/2402.09664},
	abstract = {Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Liu, Changshu and Zhang, Shizhuo Dylan and Jabbarvand, Reyhaneh},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09664 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/YSXW9LXJ/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/MU77GQGZ/Liu et al. - 2024 - CodeMind A Framework to Challenge Large Language .pdf:application/pdf},
}

@misc{noauthor_ieee_nodate-3,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10123596},
	urldate = {2024-02-21},
	file = {IEEE Xplore Full-Text PDF\::/Users/akazad/Zotero/storage/7HZAPUKT/stamp.html:text/html},
}

@inproceedings{ding_hpc-gpt_2023,
	address = {Denver CO USA},
	title = {{HPC}-{GPT}: {Integrating} {Large} {Language} {Model} for {High}-{Performance} {Computing}},
	isbn = {9798400707858},
	shorttitle = {{HPC}-{GPT}},
	url = {https://dl.acm.org/doi/10.1145/3624062.3624172},
	doi = {10.1145/3624062.3624172},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of {The} {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {ACM},
	author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
	month = nov,
	year = {2023},
	pages = {951--960},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/FUCIURVZ/Ding et al. - 2023 - HPC-GPT Integrating Large Language Model for High.pdf:application/pdf},
}

@book{yao_survey_2023,
	title = {A {Survey} on {Large} {Language} {Model} ({LLM}) {Security} and {Privacy}: {The} {Good}, the {Bad}, and the {Ugly}},
	shorttitle = {A {Survey} on {Large} {Language} {Model} ({LLM}) {Security} and {Privacy}},
	abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Eric and Zhang, Yue},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/R32ZVTWI/Yao et al. - 2023 - A Survey on Large Language Model (LLM) Security an.pdf:application/pdf},
}

@inproceedings{chen_data_2023,
	address = {Denver CO USA},
	title = {Data {Race} {Detection} {Using} {Large} {Language} {Models}},
	isbn = {9798400707858},
	url = {https://dl.acm.org/doi/10.1145/3624062.3624088},
	doi = {10.1145/3624062.3624088},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of {The} {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {ACM},
	author = {Chen, Le and Ding, Xianzhong and Emani, Murali and Vanderbruggen, Tristan and Lin, Pei-Hung and Liao, Chunhua},
	month = nov,
	year = {2023},
	pages = {215--223},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/ZX7RBGFD/Chen et al. - 2023 - Data Race Detection Using Large Language Models.pdf:application/pdf},
}

@misc{chen_landscape_2024,
	title = {The {Landscape} and {Challenges} of {HPC} {Research} and {LLMs}},
	url = {http://arxiv.org/abs/2402.02018},
	abstract = {Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Chen, Le and Ahmed, Nesreen K. and Dutta, Akash and Bhattacharjee, Arijit and Yu, Sixing and Mahmud, Quazi Ishtiaque and Abebe, Waqwoya and Phan, Hung and Sarkar, Aishwarya and Butler, Branden and Hasabnis, Niranjan and Oren, Gal and Vo, Vy A. and Munoz, Juan Pablo and Willke, Theodore L. and Mattson, Tim and Jannesari, Ali},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02018 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/XD6KDVA8/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/Q29DZ75M/Chen et al. - 2024 - The Landscape and Challenges of HPC Research and L.pdf:application/pdf},
}

@misc{schneider_mpirigen_2024,
	title = {{MPIrigen}: {MPI} {Code} {Generation} through {Domain}-{Specific} {Language} {Models}},
	shorttitle = {{MPIrigen}},
	url = {http://arxiv.org/abs/2402.09126},
	abstract = {The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Schneider, Nadav and Hasabnis, Niranjan and Vo, Vy A. and Kadosh, Tal and Krien, Neva and Capotă, Mihai and Wasay, Abdul and Tamir, Guy and Willke, Ted and Ahmed, Nesreen and Pinter, Yuval and Mattson, Timothy and Oren, Gal},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09126 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {978-3-031-40744-4.pdf:/Users/akazad/Zotero/storage/DMN84L7A/978-3-031-40744-4.pdf:application/pdf;arXiv.org Snapshot:/Users/akazad/Zotero/storage/NGT4YEQX/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/QR7DIVI4/Schneider et al. - 2024 - MPIrigen MPI Code Generation through Domain-Speci.pdf:application/pdf},
}

@misc{chen_ompgpt_2024,
	title = {{OMPGPT}: {A} {Generative} {Pre}-trained {Transformer} {Model} for {OpenMP}},
	shorttitle = {{OMPGPT}},
	url = {http://arxiv.org/abs/2401.16445},
	abstract = {Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms existing large language models specialized in OpenMP tasks and maintains a notably smaller size, aligning it more closely with the typical hardware constraints of HPC environments. We consider our contribution as a pivotal bridge, connecting the advantage of language models with the specific demands of HPC tasks. The success of OMPGPT lays a solid foundation, suggesting its potential applicability and adaptability to a wider range of HPC tasks, thereby opening new avenues in the field of computational efficiency and effectiveness.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Chen, Le and Bhattacharjee, Arijit and Ahmed, Nesreen and Hasabnis, Niranjan and Oren, Gal and Vo, Vy and Jannesari, Ali},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16445 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/5IE9KQHE/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/L49GHSKP/Chen et al. - 2024 - OMPGPT A Generative Pre-trained Transformer Model.pdf:application/pdf},
}

@misc{yang_unveiling_2024,
	title = {Unveiling {Memorization} in {Code} {Models}},
	url = {http://arxiv.org/abs/2308.09932},
	doi = {10.1145/3597503.3639074},
	abstract = {The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues. This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problems. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.},
	urldate = {2024-02-21},
	author = {Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David},
	month = jan,
	year = {2024},
	note = {arXiv:2308.09932 [cs]},
	keywords = {Computer Science - Software Engineering, 2024 ICSE},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/ZUVWJJKQ/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/RN2FI4MY/Yang et al. - 2024 - Unveiling Memorization in Code Models.pdf:application/pdf},
}

@misc{roziere_code_2024,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	shorttitle = {Code {Llama}},
	url = {http://arxiv.org/abs/2308.12950},
	abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67\% and 65\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	month = jan,
	year = {2024},
	note = {arXiv:2308.12950 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/MZGMIX49/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/XMKNS6BZ/Rozière et al. - 2024 - Code Llama Open Foundation Models for Code.pdf:application/pdf},
}

@misc{weyssow_exploring_2024,
	title = {Exploring {Parameter}-{Efficient} {Fine}-{Tuning} {Techniques} for {Code} {Generation} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10462},
	abstract = {Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs under the automated code generation scenario. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL across a diverse set of LLMs. Additionally, we demonstrate the extended capabilities of PEFT, showcasing its ability to learn from two distinct datasets jointly without compromising performance. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios. Our code is available at https://github.com/martin-wey/peft-llm-code/.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and Lo, David and Sahraoui, Houari},
	month = jan,
	year = {2024},
	note = {arXiv:2308.10462 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/7VNTMCGT/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/37DRV7GL/Weyssow et al. - 2024 - Exploring Parameter-Efficient Fine-Tuning Techniqu.pdf:application/pdf},
}

@inproceedings{eliseeva_commit_2023,
	address = {Luxembourg, Luxembourg},
	title = {From {Commit} {Message} {Generation} to {History}-{Aware} {Commit} {Message} {Completion}},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298339/},
	doi = {10.1109/ASE56229.2023.00078},
	abstract = {Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could signiﬁcantly improve the quality and the personal nature of the resulting commit messages. In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Eliseeva, Aleksandra and Sokolov, Yaroslav and Bogomolov, Egor and Golubev, Yaroslav and Dig, Danny and Bryksin, Timofey},
	month = sep,
	year = {2023},
	pages = {723--735},
	file = {Eliseeva et al. - 2023 - From Commit Message Generation to History-Aware Co.pdf:/Users/akazad/Zotero/storage/FHL847BL/Eliseeva et al. - 2023 - From Commit Message Generation to History-Aware Co.pdf:application/pdf},
}

@book{mcintosh-smith_openmp_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{OpenMP}: {Advanced} {Task}-{Based}, {Device} and {Compiler} {Programming}: 19th {International} {Workshop} on {OpenMP}, {IWOMP} 2023, {Bristol}, {UK}, {September} 13–15, 2023, {Proceedings}},
	volume = {14114},
	isbn = {978-3-031-40743-7 978-3-031-40744-4},
	shorttitle = {{OpenMP}},
	url = {https://link.springer.com/10.1007/978-3-031-40744-4},
	language = {en},
	urldate = {2024-02-22},
	publisher = {Springer Nature Switzerland},
	editor = {McIntosh-Smith, Simon and Klemm, Michael and De Supinski, Bronis R. and Deakin, Tom and Klinkenberg, Jannis},
	year = {2023},
	doi = {10.1007/978-3-031-40744-4},
	file = {McIntosh-Smith et al. - 2023 - OpenMP Advanced Task-Based, Device and Compiler P.pdf:/Users/akazad/Zotero/storage/PG25BWI5/McIntosh-Smith et al. - 2023 - OpenMP Advanced Task-Based, Device and Compiler P.pdf:application/pdf},
}

@inproceedings{huang_empirical_2023,
	address = {Luxembourg, Luxembourg},
	title = {An {Empirical} {Study} on {Fine}-{Tuning} {Large} {Language} {Models} of {Code} for {Automated} {Program} {Repair}},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298532/},
	doi = {10.1109/ASE56229.2023.00181},
	abstract = {The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the ﬁne-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To ﬁll the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the ﬁnetuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then ﬁne-tune them on widely-used datasets and compare them with existing state-of-theart APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the ﬁne-tuning paradigm can signiﬁcantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing},
	month = sep,
	year = {2023},
	pages = {1162--1174},
	file = {Huang et al. - 2023 - An Empirical Study on Fine-Tuning Large Language M.pdf:/Users/akazad/Zotero/storage/9QQHB4BP/Huang et al. - 2023 - An Empirical Study on Fine-Tuning Large Language M.pdf:application/pdf},
}

@article{tipirneni_structcoder_2024,
	title = {{StructCoder}: {Structure}-{Aware} {Transformer} for {Code} {Generation}},
	volume = {18},
	issn = {1556-4681, 1556-472X},
	shorttitle = {{StructCoder}},
	url = {https://dl.acm.org/doi/10.1145/3636430},
	doi = {10.1145/3636430},
	abstract = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at
              https://github.com/reddy-lab-code-research/StructCoder/
              .},
	language = {en},
	number = {3},
	urldate = {2024-02-22},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Tipirneni, Sindhu and Zhu, Ming and Reddy, Chandan K.},
	month = apr,
	year = {2024},
	pages = {1--20},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/DZT6V8EZ/Tipirneni et al. - 2024 - StructCoder Structure-Aware Transformer for Code .pdf:application/pdf},
}

@article{li_span_2023,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{CodeEditor}{\textless}/span{\textgreater} : {Learning} to {Edit} {Source} {Code} with {Pre}-trained {Models}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://dl.acm.org/doi/10.1145/3597207},
	doi = {10.1145/3597207},
	abstract = {Developers often perform repetitive code editing activities (up to 70\%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.
            
              In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named
              CodeEditor
              . Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our
              CodeEditor
              to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained
              CodeEditor
              in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained
              CodeEditor
              with four datasets and evaluate it on the test data.
              CodeEditor
              outperforms the SOTA baselines by 15\%, 25.5\%, 9.4\%, and 26.6\% on four datasets. (2) In the few-shot setting, we train the pre-trained
              CodeEditor
              with limited data and evaluate it on the test data.
              CodeEditor
              substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained
              CodeEditor
              on the test data without training.
              CodeEditor
              correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained
              CodeEditor
              is more effective in automatic code editing.},
	language = {en},
	number = {6},
	urldate = {2024-02-22},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Li, Jia and Li, Ge and Li, Zhuo and Jin, Zhi and Hu, Xing and Zhang, Kechi and Fu, Zhiyi},
	month = nov,
	year = {2023},
	pages = {1--22},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/R82AQXV7/Li et al. - 2023 - CodeEditor.pdf:application/pdf},
}

@inproceedings{peng_generative_2023,
	address = {Luxembourg, Luxembourg},
	title = {Generative {Type} {Inference} for {Python}},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298512/},
	doi = {10.1109/ASE56229.2023.00031},
	abstract = {Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze-style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, highquality annotated datasets and are limited to pre-deﬁned types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a ﬁll-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reﬂect the inference logic. What is more, their predictions are not interpretable, hindering developers’ understanding and veriﬁcation of the results.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Peng, Yun and Wang, Chaozheng and Wang, Wenxuan and Gao, Cuiyun and Lyu, Michael R.},
	month = sep,
	year = {2023},
	pages = {988--999},
	file = {Peng et al. - 2023 - Generative Type Inference for Python.pdf:/Users/akazad/Zotero/storage/YQD9XWN6/Peng et al. - 2023 - Generative Type Inference for Python.pdf:application/pdf},
}

@article{zhang_pre-trained_2024,
	title = {Pre-{Trained} {Model}-{Based} {Automated} {Software} {Vulnerability} {Repair}: {How} {Far} are {We}?},
	issn = {1545-5971, 1941-0018, 2160-9209},
	shorttitle = {Pre-{Trained} {Model}-{Based} {Automated} {Software} {Vulnerability} {Repair}},
	url = {https://ieeexplore.ieee.org/document/10232867/},
	doi = {10.1109/TDSC.2023.3308897},
	abstract = {Various approaches are proposed to help under-resourced security researchers to detect and analyze software vulnerabilities. It is still incredibly time-consuming and labor-intensive for security researchers to fix such reported vulnerabilities due to the increasing size and complexity of modern software systems. The time lag between the reporting and fixing of a security vulnerability causes software systems to suffer from significant exposure to possible attacks. Very recently, some techniques propose to apply pretrained models to fix security vulnerabilities and have proved their success in improving repair accuracy. However, the effectiveness of existing pre-trained models has not been systematically compared and little is known about their advantages and disadvantages.},
	language = {en},
	urldate = {2024-02-22},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Zhang, Quanjun and Fang, Chunrong and Yu, Bowen and Sun, Weisong and Zhang, Tongke and Chen, Zhenyu},
	year = {2024},
	pages = {1--18},
	file = {Zhang et al. - 2024 - Pre-Trained Model-Based Automated Software Vulnera.pdf:/Users/akazad/Zotero/storage/5I9FER53/Zhang et al. - 2024 - Pre-Trained Model-Based Automated Software Vulnera.pdf:application/pdf},
}

@misc{alrashedy_learning_2024,
	title = {Learning {Defect} {Prediction} from {Unrealistic} {Data}},
	url = {http://arxiv.org/abs/2311.00931},
	abstract = {Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial programs using a neural model and scores artificial samples based on their distance to the nearest real-world sample. We show that training on only the nearest, representationally most similar samples while discarding samples that are not at all similar in representations yields consistent improvements across two popular pretrained models of code on two code understanding tasks. Our results are promising, in that they show that training models on a representative subset of an unrealistic dataset can help us harness the power of large-scale synthetic data generation while preserving downstream task performance. Finally, we highlight the limitations of applying AI models for predicting vulnerabilities and bugs in real-world applications},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Alrashedy, Kamel and Hellendoorn, Vincent J. and Orso, Alessandro},
	month = jan,
	year = {2024},
	note = {arXiv:2311.00931 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/26JQALHJ/2311.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/5LLK9VUH/Alrashedy et al. - 2024 - Learning Defect Prediction from Unrealistic Data.pdf:application/pdf},
}

@misc{deligiannis_fixing_2023,
	title = {Fixing {Rust} {Compilation} {Errors} using {LLMs}},
	url = {http://arxiv.org/abs/2308.05177},
	abstract = {The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers. This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration with an LLM to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74\% on real-world compilation errors in popular open-source Rust repositories. We plan to release our dataset of Rust compilation errors to enable further research.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Deligiannis, Pantazis and Lal, Akash and Mehrotra, Nikita and Rastogi, Aseem},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05177 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/FXZRHGMD/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/INGJGQ6J/Deligiannis et al. - 2023 - Fixing Rust Compilation Errors using LLMs.pdf:application/pdf},
}

@inproceedings{wang_natural_2023,
	address = {San Francisco CA USA},
	title = {Natural {Language} to {Code}: {How} {Far} {Are} {We}?},
	isbn = {9798400703270},
	shorttitle = {Natural {Language} to {Code}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616323},
	doi = {10.1145/3611643.3616323},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyandé, Tegawendé F. and Mao, Xiaoguang},
	month = nov,
	year = {2023},
	pages = {375--387},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/5BDQ3P5L/Wang et al. - 2023 - Natural Language to Code How Far Are We.pdf:application/pdf},
}

@misc{mohajer_skipanalyzer_2023,
	title = {{SkipAnalyzer}: {A} {Tool} for {Static} {Code} {Analysis} with {Large} {Language} {Models}},
	shorttitle = {{SkipAnalyzer}},
	url = {http://arxiv.org/abs/2310.18532},
	abstract = {We introduce SkipAnalyzer, a large language model (LLM)-powered tool for static code analysis. SkipAnalyzer has three components: 1) an LLM-based static bug detector that scans source code and reports specific types of bugs, 2) an LLM-based false-positive filter that can identify false-positive bugs in the results of static bug detectors (e.g., the result of step 1) to improve detection accuracy, and 3) an LLM-based patch generator that can generate patches for the detected bugs above. As a proof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited outstanding performance in various software engineering tasks. To evaluate SkipAnalyzer, we focus on two types of typical and critical bugs that are targeted by static bug detection, i.e., Null Dereference and Resource Leak as subjects. We employ Infer to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our study demonstrates that SkipAnalyzer achieves remarkable performance in the mentioned static analysis tasks, including bug detection, false-positive warning removal, and bug repair. In static bug detection, SkipAnalyzer achieves accuracy values of up to 68.37\% for detecting Null Dereference bugs and 76.95\% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer, by 12.86\% and 43.13\%, respectively. For removing false-positive warnings, SkipAnalyzer can reach a precision of up to 93.88\% for Null Dereference bugs and 63.33\% for Resource Leak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive warning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate syntactically correct patches to fix its detected bugs with a success rate of up to 97.30\%.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Mohajer, Mohammad Mahdi and Aleithan, Reem and Harzevili, Nima Shiri and Wei, Moshi and Belle, Alvine Boaye and Pham, Hung Viet and Wang, Song},
	month = dec,
	year = {2023},
	note = {arXiv:2310.18532 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/WW43ZGNV/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/4D276ZJL/Mohajer et al. - 2023 - SkipAnalyzer A Tool for Static Code Analysis with.pdf:application/pdf},
}

@inproceedings{xu_unilog_2024,
	address = {Lisbon Portugal},
	title = {{UniLog}: {Automatic} {Logging} via {LLM} and {In}-{Context} {Learning}},
	isbn = {9798400702174},
	shorttitle = {{UniLog}},
	url = {https://dl.acm.org/doi/10.1145/3597503.3623326},
	doi = {10.1145/3597503.3623326},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the 46th {IEEE}/{ACM} {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei},
	month = feb,
	year = {2024},
	pages = {1--12},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/MJ9RSUYZ/Xu et al. - 2024 - UniLog Automatic Logging via LLM and In-Context L.pdf:application/pdf},
}

@inproceedings{xia_plastic_2023,
	address = {Luxembourg, Luxembourg},
	title = {The {Plastic} {Surgery} {Hypothesis} in the {Era} of {Large} {Language} {Models}},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298499/},
	doi = {10.1109/ASE56229.2023.00047},
	abstract = {Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on speciﬁc bug types and ﬁxes through the use of templates, heuristics, and formal speciﬁcations. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-ofthe-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-speciﬁc information such as unique variable or method names.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming},
	month = sep,
	year = {2023},
	pages = {522--534},
	file = {Xia et al. - 2023 - The Plastic Surgery Hypothesis in the Era of Large.pdf:/Users/akazad/Zotero/storage/8FIXLGMZ/Xia et al. - 2023 - The Plastic Surgery Hypothesis in the Era of Large.pdf:application/pdf},
}

@inproceedings{liu_empirical_2023,
	address = {Luxembourg, Luxembourg},
	title = {An {Empirical} {Study} of {Parameter}-{Efficient} {Fine}-{Tuning} {Methods} for {Pre}-{Trained} {Code} {Models}},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298587/},
	doi = {10.1109/ASE56229.2023.00125},
	abstract = {Pre-trained code models (e.g. CodeBERT and CodeT5) have demonstrated their code intelligence in various software engineering tasks, such as code summarization. And full ﬁne-tuning has become the typical approach to adapting these models to downstream tasks. However, full ﬁne-tuning these large models can be computationally expensive and memory-intensive, particularly when training for multiple tasks. To alleviate this issue, several parameter-efﬁcient ﬁne-tuning methods (e.g. Adapter and LoRA) have been proposed to only train a small number of additional parameters, while keeping the original pre-trained parameters frozen. Although these methods claim superiority over the prior techniques, they seldom make a comprehensive and fair comparison on multiple software engineering tasks. Moreover, besides their potential in reducing ﬁne-tuning costs and maintaining approximate performance, the effectiveness of these methods in low-resource, cross-language, and cross-project scenarios is inadequately studied.},
	language = {en},
	urldate = {2024-02-22},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Liu, Jiaxing and Sha, Chaofeng and Peng, Xin},
	month = sep,
	year = {2023},
	pages = {397--408},
	file = {Liu et al. - 2023 - An Empirical Study of Parameter-Efficient Fine-Tun.pdf:/Users/akazad/Zotero/storage/45R6JM28/Liu et al. - 2023 - An Empirical Study of Parameter-Efficient Fine-Tun.pdf:application/pdf},
}

@misc{huang_code_2023,
	title = {Code {Representation} {Pre}-training with {Complements} from {Program} {Executions}},
	url = {http://arxiv.org/abs/2309.09980},
	abstract = {Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed it into the feature representations of code as complements. The test cases are obtained with the assistance of a customized fuzzer and are only required during pre-training. FuzzPretrain yielded more than 6\%/9\% mAP improvements on code search over its counterparts trained with only source code or AST, respectively. Our extensive experimental results show the benefits of learning discriminative code representations with program executions.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Huang, Jiabo and Zhao, Jianyu and Rong, Yuyang and Guo, Yiwen and He, Yifeng and Chen, Hao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.09980 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/QPWRD6BE/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/M934SQZ4/Huang et al. - 2023 - Code Representation Pre-training with Complements .pdf:application/pdf},
}

@misc{nie_learning_2023-1,
	title = {Learning {Deep} {Semantics} for {Test} {Completion}},
	url = {http://arxiv.org/abs/2302.10166},
	abstract = {Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TeCo -- a deep learning model using code semantics for test completion. The key insight underlying TeCo is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. TeCo extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TeCo, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that TeCo achieves an exact-match accuracy of 18, which is 29\% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, TeCo can generate runnable code in 29\% of the cases compared to 18\% obtained by the best baseline. Moreover, TeCo is significantly better than prior work on test oracle generation.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
	month = mar,
	year = {2023},
	note = {arXiv:2302.10166 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JVWP64YH/2302.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/9RIWC9D8/Nie et al. - 2023 - Learning Deep Semantics for Test Completion.pdf:application/pdf},
}

@inproceedings{mahbub_explaining_2023,
	address = {Melbourne, Australia},
	title = {Explaining {Software} {Bugs} {Leveraging} {Code} {Structures} in {Neural} {Machine} {Translation}},
	isbn = {978-1-66545-701-9},
	url = {https://ieeexplore.ieee.org/document/10172643/},
	doi = {10.1109/ICSE48619.2023.00063},
	urldate = {2024-02-26},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur},
	month = may,
	year = {2023},
	pages = {640--652},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/TG9G2XKH/Mahbub et al. - 2023 - Explaining Software Bugs Leveraging Code Structure.pdf:application/pdf},
}

@article{alon_code2seq_2019,
	title = {{CODE2SEQ}: {GENERATING} {SEQUENCES} {FROM} {STRUCTURED} {REPRESENTATIONS} {OF} {CODE}},
	abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model signiﬁcantly outperforms previous models that were speciﬁcally designed for programming languages, as well as state-of-the-art NMT models. An online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
	language = {en},
	author = {Alon, Uri and Levy, Omer and Brody, Shaked and Yahav, Eran},
	year = {2019},
	file = {Alon et al. - 2019 - CODE2SEQ GENERATING SEQUENCES FROM STRUCTURED REP.pdf:/Users/akazad/Zotero/storage/C4AXINZD/Alon et al. - 2019 - CODE2SEQ GENERATING SEQUENCES FROM STRUCTURED REP.pdf:application/pdf},
}

@misc{gong_ast-t5_2024,
	title = {{AST}-{T5}: {Structure}-{Aware} {Pretraining} for {Code} {Generation} and {Understanding}},
	shorttitle = {{AST}-{T5}},
	url = {http://arxiv.org/abs/2401.03003},
	abstract = {Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C\# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast\_t5.},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Gong, Linyuan and Elhoushi, Mostafa and Cheung, Alvin},
	month = feb,
	year = {2024},
	note = {arXiv:2401.03003 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/Y8EH7PK3/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/72LYP2MC/Gong et al. - 2024 - AST-T5 Structure-Aware Pretraining for Code Gener.pdf:application/pdf},
}

@misc{bhattacharya_exploring_2023,
	title = {Exploring {Large} {Language} {Models} for {Code} {Explanation}},
	url = {http://arxiv.org/abs/2310.16673},
	abstract = {Automating code documentation through explanatory text can prove highly beneficial in code understanding. Large Language Models (LLMs) have made remarkable strides in Natural Language Processing, especially within software engineering tasks such as code generation and code summarization. This study specifically delves into the task of generating natural-language summaries for code snippets, using various LLMs. The findings indicate that Code LLMs outperform their generic counterparts, and zero-shot methods yield superior results when dealing with datasets with dissimilar distributions between training and testing sets.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Bhattacharya, Paheli and Chakraborty, Manojit and Palepu, Kartheek N. S. N. and Pandey, Vikas and Dindorkar, Ishan and Rajpurohit, Rakesh and Gupta, Rishabh},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16673 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, D.2.3, I.7},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/SGDQLNGT/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/H83HUF7X/Bhattacharya et al. - 2023 - Exploring Large Language Models for Code Explanati.pdf:application/pdf},
}

@misc{lin_cct5_2023,
	title = {{CCT5}: {A} {Code}-{Change}-{Oriented} {Pre}-{Trained} {Model}},
	shorttitle = {{CCT5}},
	url = {http://arxiv.org/abs/2305.10785},
	abstract = {Software is constantly changing, requiring developers to perform several derived tasks in a timely manner, such as writing a description for the intention of the code change, or identifying the defect-prone code changes. Considering that the cost of dealing with these tasks can account for a large proportion (typically around 70 percent) of the total development expenditure, automating such processes will significantly lighten the burdens of developers. To achieve such a target, existing approaches mainly rely on training deep learning models from scratch or fine-tuning existing pretrained models on such tasks, both of which have weaknesses. Specifically, the former uses comparatively small-scale labelled data for training, making it difficult to learn and exploit the domain knowledge of programming language hidden in the large-amount unlabelled code in the wild; the latter is hard to fully leverage the learned knowledge of the pre-trained model, as existing pre-trained models are designed to encode a single code snippet rather than a code change (i.e., the difference between two code snippets). We propose to pre-train a model specially designed for code changes to better support developers in software maintenance. To this end, we first collect a large-scale dataset containing 1.5M+ pairwise data of code changes and commit messages. Based on these data, we curate five different tasks for pre-training, which equip the model with diverse domain knowledge about code changes. We fine-tune the pre-trained model, CCT5, on three widely-studied tasks incurred by code changes and two tasks specific to the code review process. Results show that CCT5 outperforms both conventional deep learning approaches and existing pre-trained models on these tasks.},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Lin, Bo and Wang, Shangwen and Liu, Zhongxin and Liu, Yepang and Xia, Xin and Mao, Xiaoguang},
	month = may,
	year = {2023},
	note = {arXiv:2305.10785 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/E36UJS6G/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/KZKGDJAR/Lin et al. - 2023 - CCT5 A Code-Change-Oriented Pre-Trained Model.pdf:application/pdf},
}

@article{mahbub_comprehending_nodate,
	title = {Comprehending {Software} {Bugs} {Leveraging} {Code} {Structures} with {Neural} {Language} {Models}},
	language = {en},
	author = {Mahbub, Parvez},
	file = {Mahbub - Comprehending Software Bugs Leveraging Code Struct.pdf:/Users/akazad/Zotero/storage/8VZTVABB/Mahbub - Comprehending Software Bugs Leveraging Code Struct.pdf:application/pdf},
}

@misc{vu_automated_2024,
	title = {Automated {Description} {Generation} for {Software} {Patches}},
	url = {http://arxiv.org/abs/2402.03805},
	abstract = {Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers. However, manual description creation poses challenges in terms of time consumption and variations in quality and detail. In this paper, we propose PATCHEXPLAINER, an approach that addresses these challenges by framing patch description generation as a machine translation task. In PATCHEXPLAINER, we leverage explicit representations of critical elements, historical context, and syntactic conventions. Moreover, the translation model in PATCHEXPLAINER is designed with an awareness of description similarity. Particularly, the model is explicitly trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches. The dual objectives maximize similarity and accurately predict affiliating groups. Our experimental results on a large dataset of real-world software patches show that PATCHEXPLAINER consistently outperforms existing methods, with improvements up to 189\% in BLEU, 5.7X in Exact Match rate, and 154\% in Semantic Similarity, affirming its effectiveness in generating software patch descriptions.},
	urldate = {2024-03-01},
	publisher = {arXiv},
	author = {Vu, Thanh Trong and Bui, Tuan-Dung and Do, Thanh-Dat and Nguyen, Thu-Trang and Vo, Hieu Dinh and Nguyen, Son},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03805 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/WVTU8IIS/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/TTNE7NGB/Vu et al. - 2024 - Automated Description Generation for Software Patc.pdf:application/pdf},
}

@inproceedings{jiang_impact_2023,
	address = {Melbourne, Australia},
	title = {Impact of {Code} {Language} {Models} on {Automated} {Program} {Repair}},
	isbn = {978-1-66545-701-9},
	url = {https://ieeexplore.ieee.org/document/10172517/},
	doi = {10.1109/ICSE48619.2023.00125},
	abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs’ ﬁxing capabilities and to ﬁne-tune CLMs for the APR task.},
	language = {en},
	urldate = {2024-03-05},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
	month = may,
	year = {2023},
	pages = {1430--1442},
	file = {Jiang et al. - 2023 - Impact of Code Language Models on Automated Progra.pdf:/Users/akazad/Zotero/storage/FZEG9DJK/Jiang et al. - 2023 - Impact of Code Language Models on Automated Progra.pdf:application/pdf},
}

@misc{wang_codet5_2021,
	title = {{CodeT5}: {Identifier}-aware {Unified} {Pre}-trained {Encoder}-{Decoder} {Models} for {Code} {Understanding} and {Generation}},
	shorttitle = {{CodeT5}},
	url = {http://arxiv.org/abs/2109.00859},
	abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven C. H.},
	month = sep,
	year = {2021},
	note = {arXiv:2109.00859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/J6UCN4PV/2109.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/8Z8UBWB9/Wang et al. - 2021 - CodeT5 Identifier-aware Unified Pre-trained Encod.pdf:application/pdf},
}

@misc{wang_codet5_2023,
	title = {{CodeT5}+: {Open} {Code} {Large} {Language} {Models} for {Code} {Understanding} and {Generation}},
	shorttitle = {{CodeT5}+},
	url = {http://arxiv.org/abs/2305.07922},
	abstract = {Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D. Q. and Li, Junnan and Hoi, Steven C. H.},
	month = may,
	year = {2023},
	note = {arXiv:2305.07922 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/HFD5KDED/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/GQQJQISC/Wang et al. - 2023 - CodeT5+ Open Code Large Language Models for Code .pdf:application/pdf},
}

@misc{shi_towards_2023,
	title = {Towards {Efficient} {Fine}-tuning of {Pre}-trained {Code} {Models}: {An} {Experimental} {Study} and {Beyond}},
	shorttitle = {Towards {Efficient} {Fine}-tuning of {Pre}-trained {Code} {Models}},
	url = {http://arxiv.org/abs/2304.05216},
	abstract = {Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better. Replication package including source code, datasets, and online Appendix is available at: {\textbackslash}url\{https://github.com/DeepSoftwareAnalytics/Telly\}.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Shi, Ensheng and Wang, Yanlin and Zhang, Hongyu and Du, Lun and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05216 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/XA4CCBW3/2304.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/GUUWTUZE/Shi et al. - 2023 - Towards Efficient Fine-tuning of Pre-trained Code .pdf:application/pdf},
}

@misc{ayupov_parameter-efficient_2022,
	title = {Parameter-{Efficient} {Finetuning} of {Transformers} for {Source} {Code}},
	url = {http://arxiv.org/abs/2212.05901},
	abstract = {Pretrained Transformers achieve state-of-the-art performance in various code-processing tasks but may be too large to be deployed. As software development tools often incorporate modules for various purposes which may potentially use a single instance of the pretrained model, it appears relevant to utilize parameter-efficient fine-tuning for the pretrained models of code. In this work, we test two widely used approaches, adapters and LoRA, which were initially tested on NLP tasks, on four code-processing tasks. We find that though the efficient fine-tuning approaches may achieve comparable or higher performance than the standard, full, fine-tuning in code understanding tasks, they underperform full fine-tuning in code-generative tasks. These results underline the importance of testing efficient fine-tuning approaches on other domains than NLP and motivate future research in efficient fine-tuning for source code.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Ayupov, Shamil and Chirkova, Nadezhda},
	month = dec,
	year = {2022},
	note = {arXiv:2212.05901 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/BFLPUH9U/2212.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/XR4TCEZF/Ayupov and Chirkova - 2022 - Parameter-Efficient Finetuning of Transformers for.pdf:application/pdf},
}

@inproceedings{sui_table_2024,
	address = {Merida Mexico},
	title = {Table {Meets} {LLM}: {Can} {Large} {Language} {Models} {Understand} {Structured} {Table} {Data}? {A} {Benchmark} and {Empirical} {Study}},
	isbn = {9798400703713},
	shorttitle = {Table {Meets} {LLM}},
	url = {https://dl.acm.org/doi/10.1145/3616855.3635752},
	doi = {10.1145/3616855.3635752},
	abstract = {Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, e.g., cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we propose self-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, e.g., TabFact(↑ 2.31\%), HybridQA(↑ 2.13\%), SQA(↑ 2.72\%), Feverous(↑ 0.84\%), and ToTTo(↑ 5.68\%). We believe that our open source1 benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.},
	language = {en},
	urldate = {2024-03-08},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
	month = mar,
	year = {2024},
	pages = {645--654},
	file = {Sui et al. - 2024 - Table Meets LLM Can Large Language Models Underst.pdf:/Users/akazad/Zotero/storage/Q76EE9NE/Sui et al. - 2024 - Table Meets LLM Can Large Language Models Underst.pdf:application/pdf},
}

@inproceedings{li_assisting_2023,
	address = {San Francisco CA USA},
	title = {Assisting {Static} {Analysis} with {Large} {Language} {Models}: {A} {ChatGPT} {Experiment}},
	isbn = {9798400703270},
	shorttitle = {Assisting {Static} {Analysis} with {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3613078},
	doi = {10.1145/3611643.3613078},
	language = {en},
	urldate = {2024-03-10},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
	month = nov,
	year = {2023},
	pages = {2107--2111},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/WM2EAP8A/Li et al. - 2023 - Assisting Static Analysis with Large Language Mode.pdf:application/pdf},
}

@misc{pister_promptset_2024,
	title = {{PromptSet}: {A} {Programmer}'s {Prompting} {Dataset}},
	shorttitle = {{PromptSet}},
	url = {http://arxiv.org/abs/2402.16932},
	doi = {10.1145/3643795.3648395},
	abstract = {The rise of capabilities expressed by large language models has been quickly followed by the integration of the same complex systems into application level logic. Algorithms, programs, systems, and companies are built around structured prompting to black box models where the majority of the design and implementation lies in capturing and quantifying the `agent mode'. The standard way to shape a closed language model is to prime it for a specific task with a tailored prompt, often initially handwritten by a human. The textual prompts co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be. Unlike traditional code, we find that prompts do not receive effective static testing and linting to prevent runtime issues. In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer prompts used in open source Python programs. We perform analysis on this dataset and introduce the notion of a static linter for prompts. Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name {\textbackslash}texttt\{pisterlabs/promptset\}.},
	urldate = {2024-03-11},
	author = {Pister, Kaiser and Paul, Dhruba Jyoti and Brophy, Patrick and Joshi, Ishan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16932 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/HHLSL3MF/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/C5R6N42L/Pister et al. - 2024 - PromptSet A Programmer's Prompting Dataset.pdf:application/pdf},
}

@misc{khondaker_greenllama_2024,
	title = {{GreenLLaMA}: {A} {Framework} for {Detoxification} with {Explanations}},
	shorttitle = {{GreenLLaMA}},
	url = {http://arxiv.org/abs/2402.15951},
	abstract = {Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Khondaker, Md Tawkat Islam and Abdul-Mageed, Muhammad and Lakshmanan, Laks V. S.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15951 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/I2CH98N7/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/FNKGTPJ5/Khondaker et al. - 2024 - GreenLLaMA A Framework for Detoxification with Ex.pdf:application/pdf},
}

@misc{sukhbaatar_branch-train-mix_2024,
	title = {Branch-{Train}-{MiX}: {Mixing} {Expert} {LLMs} into a {Mixture}-of-{Experts} {LLM}},
	shorttitle = {Branch-{Train}-{MiX}},
	url = {http://arxiv.org/abs/2403.07816},
	abstract = {We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozière, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and Li, Xian},
	month = mar,
	year = {2024},
	note = {arXiv:2403.07816 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JADDZYAL/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/SXNKQIZ4/Sukhbaatar et al. - 2024 - Branch-Train-MiX Mixing Expert LLMs into a Mixtur.pdf:application/pdf},
}

@inproceedings{sadat_scinli_2022,
	address = {Dublin, Ireland},
	title = {{SciNLI}: {A} {Corpus} for {Natural} {Language} {Inference} on {Scientific} {Text}},
	shorttitle = {{SciNLI}},
	url = {https://aclanthology.org/2022.acl-long.511},
	doi = {10.18653/v1/2022.acl-long.511},
	abstract = {Existing Natural Language Inference (NLI) datasets, while being instrumental in the advancement of Natural Language Understanding (NLU) research, are not related to scientific text. In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics. Given that the text used in scientific literature differs vastly from the text used in everyday language both in terms of vocabulary and sentence structure, our dataset is well suited to serve as a benchmark for the evaluation of scientific NLU models. Our experiments show that SciNLI is harder to classify than the existing NLI datasets. Our best performing model with XLNet achieves a Macro F1 score of only 78.18\% and an accuracy of 78.23\% showing that there is substantial room for improvement.},
	urldate = {2024-03-14},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sadat, Mobashir and Caragea, Cornelia},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	keywords = {dataset},
	pages = {7399--7409},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/BGD7MZPH/Sadat and Caragea - 2022 - SciNLI A Corpus for Natural Language Inference on.pdf:application/pdf},
}

@article{singla_sentiment_2017,
	title = {Sentiment {Analysis} of {Customer} {Product} {Reviews} {Using} {Machine} {Learning}},
	abstract = {Today, digital reviews play a pivotal role in enhancing global communications among consumers and influencing consumer buying patterns. E-commerce giants like Amazon, Flipkart, etc. provide a platform to consumers to share their experience and provide real insights about the performance of the product to future buyers. In order to extract valuable insights from a large set of reviews, classification of reviews into positive and negative sentiment is required. Sentiment Analysis is a computational study to extract subjective information from the text. In the proposed work, over 4,000,00 reviews have been classified into positive and negative sentiments using Sentiment Analysis. Out of the various classification models, Naïve Bayes, Support Vector Machine (SVM) and Decision Tree have been employed for classification of reviews. The evaluation of models is done using 10 Fold Cross Validation.},
	language = {en},
	author = {Singla, Zeenia and Randhawa, Sukhchandan and Jain, Sushma},
	year = {2017},
	file = {Singla et al. - 2017 - Sentiment Analysis of Customer Product Reviews Usi.pdf:/Users/akazad/Zotero/storage/SBE4JG2M/Singla et al. - 2017 - Sentiment Analysis of Customer Product Reviews Usi.pdf:application/pdf},
}

@misc{li_rewriting_2024,
	title = {Rewriting the {Code}: {A} {Simple} {Method} for {Large} {Language} {Model} {Augmented} {Code} {Search}},
	shorttitle = {Rewriting the {Code}},
	url = {http://arxiv.org/abs/2401.04514},
	abstract = {In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7\%), zero-shot dense (up to 27.6\%), and fine-tuned dense (up to 23.6\%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Li, Haochen and Zhou, Xin and Shen, Zhiqi},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04514 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval, 2024, LLM},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JU5KMMTF/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/9FPSADUW/Li et al. - 2024 - Rewriting the Code A Simple Method for Large Lang.pdf:application/pdf},
}

@misc{mckinzie_mm1_2024,
	title = {{MM1}: {Methods}, {Analysis} \& {Insights} from {Multimodal} {LLM} {Pre}-training},
	shorttitle = {{MM1}},
	url = {http://arxiv.org/abs/2403.09611},
	abstract = {In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and Belyi, Anton and Zhang, Haotian and Singh, Karanjeet and Kang, Doug and Hè, Hongyu and Schwarzer, Max and Gunter, Tom and Kong, Xiang and Zhang, Aonan and Wang, Jianyu and Wang, Chong and Du, Nan and Lei, Tao and Wiseman, Sam and Lee, Mark and Wang, Zirui and Pang, Ruoming and Grasch, Peter and Toshev, Alexander and Yang, Yinfei},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09611 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, multimodal LLM},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/M8RAZWQX/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/3YZEPTLQ/McKinzie et al. - 2024 - MM1 Methods, Analysis & Insights from Multimodal .pdf:application/pdf},
}

@misc{anonymous_using_2024,
	title = {Using {Large} {Language} {Models} to {Generate} {JUnit} {Tests}: {An} {Empirical} {Study}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Using {Large} {Language} {Models} to {Generate} {JUnit} {Tests}},
	url = {https://zenodo.org/doi/10.5281/zenodo.10530787},
	abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80\% coverage for the HumanEval dataset, but no model had more than 2\% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
	urldate = {2024-03-18},
	publisher = {[object Object]},
	author = {Anonymous},
	month = jan,
	year = {2024},
	doi = {10.5281/ZENODO.10530787},
	keywords = {code generation, large language models, test generation, test smells, unit testing},
}

@article{siddiq_using_2024,
	title = {Using {Large} {Language} {Models} to {Generate} {JUnit} {Tests}: {An} {Empirical} {Study}},
	abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80\% coverage for the HumanEval dataset, but no model had more than 2\% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
	language = {en},
	author = {Siddiq, Mohammed Latif and Santos, Joanna C S and Tanvir, Ridwanul Hasan and Ulfat, Noshin},
	year = {2024},
	file = {Siddiq et al. - 2024 - Using Large Language Models to Generate JUnit Test.pdf:/Users/akazad/Zotero/storage/I82S5P95/Siddiq et al. - 2024 - Using Large Language Models to Generate JUnit Test.pdf:application/pdf},
}

@misc{noauthor_are_nodate,
	title = {Are you sure? {Llm}},
	file = {2311.08596.pdf:/Users/akazad/Zotero/storage/EUIQZPB6/2311.08596.pdf:application/pdf},
}

@misc{venkatesh_emergence_2024,
	title = {The {Emergence} of {Large} {Language} {Models} in {Static} {Analysis}: {A} {First} {Look} through {Micro}-{Benchmarks}},
	shorttitle = {The {Emergence} of {Large} {Language} {Models} in {Static} {Analysis}},
	url = {http://arxiv.org/abs/2402.17679},
	abstract = {The application of Large Language Models (LLMs) in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field. In this paper, we investigate the role that current LLMs can play in improving callgraph analysis and type inference for Python programs. Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs, including OpenAI's GPT series and open-source models such as LLaMA. Our study reveals that LLMs show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis. This contrast emphasizes the need for specialized fine-tuning of LLMs to better suit specific static analysis tasks. Our findings provide a foundation for further research towards integrating LLMs for static analysis tasks.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Venkatesh, Ashwin Prasad Shivarpatna and Sabu, Samkutty and Mir, Amir M. and Reis, Sofia and Bodden, Eric},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17679 [cs]
version: 1},
	keywords = {Computer Science - Software Engineering, 2024},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/XXZ9BYTB/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/PLQA4YVT/Venkatesh et al. - 2024 - The Emergence of Large Language Models in Static A.pdf:application/pdf},
}

@misc{kim_leveraging_2024,
	title = {Leveraging {Large} {Language} {Models} to {Improve} {REST} {API} {Testing}},
	url = {http://arxiv.org/abs/2312.00894},
	abstract = {The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Kim, Myeongsoo and Stennett, Tyler and Shah, Dhruv and Sinha, Saurabh and Orso, Alessandro},
	month = jan,
	year = {2024},
	note = {arXiv:2312.00894 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/VUA3QMKC/2312.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/Z4J2Z9IZ/Kim et al. - 2024 - Leveraging Large Language Models to Improve REST A.pdf:application/pdf},
}

@misc{ding_crosscodeeval_2023,
	title = {{CrossCodeEval}: {A} {Diverse} and {Multilingual} {Benchmark} for {Cross}-{File} {Code} {Completion}},
	shorttitle = {{CrossCodeEval}},
	url = {http://arxiv.org/abs/2310.11248},
	abstract = {Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C\#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
	month = nov,
	year = {2023},
	note = {arXiv:2310.11248 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/VW2MWX42/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/RDCU5TMV/Ding et al. - 2023 - CrossCodeEval A Diverse and Multilingual Benchmar.pdf:application/pdf},
}

@misc{zheng_llamafactory_2024,
	title = {{LlamaFactory}: {Unified} {Efficient} {Fine}-{Tuning} of 100+ {Language} {Models}},
	shorttitle = {{LlamaFactory}},
	url = {http://arxiv.org/abs/2403.13372},
	abstract = {Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Zheng, Yaowei and Zhang, Richong and Zhang, Junhao and Ye, Yanhan and Luo, Zheyan and Ma, Yongqiang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13372 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/RWRRZKZZ/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/XYPUBHHQ/Zheng et al. - 2024 - LlamaFactory Unified Efficient Fine-Tuning of 100.pdf:application/pdf},
}

@misc{yao_hdldebugger_2024,
	title = {{HDLdebugger}: {Streamlining} {HDL} debugging with {Large} {Language} {Models}},
	shorttitle = {{HDLdebugger}},
	url = {http://arxiv.org/abs/2403.11671},
	abstract = {In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Yao, Xufeng and Li, Haoyang and Chan, Tsz Ho and Xiao, Wenyi and Yuan, Mingxuan and Huang, Yu and Chen, Lei and Yu, Bei},
	month = mar,
	year = {2024},
	note = {arXiv:2403.11671 [cs]
version: 1},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, model paper, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Hardware Architecture},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/Q43XHHUR/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/IRBW5FSP/Yao et al. - 2024 - HDLdebugger Streamlining HDL debugging with Large.pdf:application/pdf},
}

@article{ehsani_incivility_2024,
	title = {Incivility in {Open} {Source} {Projects}: {A} {Comprehensive} {Annotated} {Dataset} of {Locked} {GitHub} {Issue} {Threads}},
	abstract = {In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations. This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects. We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility. We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset. The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively. This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior.},
	language = {en},
	author = {Ehsani, Ramtin and Imran, Mia Mohammed and Zita, Robert and Damevski, Kostadin and Chatterjee, Preetha},
	year = {2024},
	file = {Ehsani et al. - 2024 - Incivility in Open Source Projects A Comprehensiv.pdf:/Users/akazad/Zotero/storage/CXLPPQGQ/Ehsani et al. - 2024 - Incivility in Open Source Projects A Comprehensiv.pdf:application/pdf},
}

@article{mishra_exploring_2024,
	title = {Exploring {ChatGPT} for {Toxicity} {Detection} in {GitHub}},
	abstract = {Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses signi�cant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models e�ectively, we need large software engineering-speci�c toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training e�ective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being �ne-tuned speci�cally for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.},
	language = {en},
	author = {Mishra, Shyamal and Chatterjee, Preetha},
	year = {2024},
	file = {Mishra and Chatterjee - 2024 - Exploring ChatGPT for Toxicity Detection in GitHub.pdf:/Users/akazad/Zotero/storage/CKJV2KYR/Mishra and Chatterjee - 2024 - Exploring ChatGPT for Toxicity Detection in GitHub.pdf:application/pdf},
}

@misc{niu_fair_2023,
	title = {{FAIR}: {Flow} {Type}-{Aware} {Pre}-{Training} of {Compiler} {Intermediate} {Representations}},
	shorttitle = {{FAIR}},
	url = {http://arxiv.org/abs/2309.04828},
	abstract = {While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04828 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/IDRFA8NS/2309.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/EE7SXPRS/Niu et al. - 2023 - FAIR Flow Type-Aware Pre-Training of Compiler Int.pdf:application/pdf},
}

@misc{yang_large_2023,
	title = {Large {Language} {Models} for {Test}-{Free} {Fault} {Localization}},
	url = {http://arxiv.org/abs/2310.01726},
	abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%-54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Yang, Aidan Z. H. and Martins, Ruben and Goues, Claire Le and Hellendoorn, Vincent J.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01726 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, ICSE20124},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/IFAHPRB5/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/WNLRHCVG/Yang et al. - 2023 - Large Language Models for Test-Free Fault Localiza.pdf:application/pdf},
}

@article{zhuang_toolqa_nodate,
	title = {{ToolQA}: {A} {Dataset} for {LLM} {Question} {Answering} with {External} {Tools}},
	abstract = {Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs’ question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs’ internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs’ ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs’ pre-training data, enabling a more precise evaluation of LLMs’ tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub 2.},
	language = {en},
	author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
	keywords = {icse2024},
	file = {Zhuang et al. - ToolQA A Dataset for LLM Question Answering with .pdf:/Users/akazad/Zotero/storage/8YDF2LU4/Zhuang et al. - ToolQA A Dataset for LLM Question Answering with .pdf:application/pdf},
}

@misc{feng_prompting_2023,
	title = {Prompting {Is} {All} {You} {Need}: {Automated} {Android} {Bug} {Replay} with {Large} {Language} {Models}},
	shorttitle = {Prompting {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2306.01987},
	abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3\% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Feng, Sidong and Chen, Chunyang},
	month = jul,
	year = {2023},
	note = {arXiv:2306.01987 [cs]},
	keywords = {Computer Science - Software Engineering, icse2024},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/FV9IHQRU/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/5GENJ7SK/Feng and Chen - 2023 - Prompting Is All You Need Automated Android Bug R.pdf:application/pdf},
}

@article{chow_pyty_2024,
	title = {{PyTy}: {Repairing} {Static} {Type} {Errors} in {Python}},
	abstract = {Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4\% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.},
	language = {en},
	author = {Chow, Yiu Wai},
	year = {2024},
	file = {Chow - 2024 - PyTy Repairing Static Type Errors in Python.pdf:/Users/akazad/Zotero/storage/64BPMYG7/Chow - 2024 - PyTy Repairing Static Type Errors in Python.pdf:application/pdf},
}

@misc{gunasekar_textbooks_2023,
	title = {Textbooks {Are} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2306.11644},
	abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11644 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/DRNZVHUY/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/PDUNNZI5/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf:application/pdf},
}

@misc{kadosh_domain-specific_2023,
	title = {Domain-{Specific} {Code} {Language} {Models}: {Unraveling} the {Potential} for {HPC} {Codes} and {Tasks}},
	shorttitle = {Domain-{Specific} {Code} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.13322},
	abstract = {With easier access to powerful compute resources, there is a growing trend in AI for software development to develop larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because these LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need large LMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller LMs for specific domains - we call them domain-specific LMs. Specifically, we start off with HPC as a domain and build an HPC-specific LM, named MonoCoder, that is orders of magnitude smaller than existing LMs but delivers similar, if not better performance, on non-HPC and HPC tasks. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against conventional multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, achieves similar results on normalized-perplexity tests and much better ones in CodeBLEU competence for high-performance and parallel code generations. Furthermore, fine-tuning the base model for the specific task of parallel code generation (OpenMP parallel for pragmas) demonstrates outstanding results compared to GPT, especially when local misleading semantics are removed by our novel pre-processor Tokompiler, showcasing the ability of domain-specific models to assist in HPC-relevant tasks.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Kadosh, Tal and Hasabnis, Niranjan and Vo, Vy A. and Schneider, Nadav and Krien, Neva and Capota, Mihai and Wasay, Abdul and Ahmed, Nesreen and Willke, Ted and Tamir, Guy and Pinter, Yuval and Mattson, Timothy and Oren, Gal},
	month = dec,
	year = {2023},
	note = {arXiv:2312.13322 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/VW2BEME6/2312.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/5XDCULCH/Kadosh et al. - 2023 - Domain-Specific Code Language Models Unraveling t.pdf:application/pdf},
}

@inproceedings{ding_hpc-gpt_2023-1,
	address = {Denver CO USA},
	title = {{HPC}-{GPT}: {Integrating} {Large} {Language} {Model} for {High}-{Performance} {Computing}},
	isbn = {9798400707858},
	shorttitle = {{HPC}-{GPT}},
	url = {https://dl.acm.org/doi/10.1145/3624062.3624172},
	doi = {10.1145/3624062.3624172},
	language = {en},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the {SC} '23 {Workshops} of {The} {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis}},
	publisher = {ACM},
	author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
	month = nov,
	year = {2023},
	pages = {951--960},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/VKSGXEIW/Ding et al. - 2023 - HPC-GPT Integrating Large Language Model for High.pdf:application/pdf},
}

@misc{yin_dynosaur_2023,
	title = {Dynosaur: {A} {Dynamic} {Growth} {Paradigm} for {Instruction}-{Tuning} {Data} {Curation}},
	shorttitle = {Dynosaur},
	url = {http://arxiv.org/abs/2305.14327},
	abstract = {Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than \$12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Yin, Da and Liu, Xiao and Yin, Fan and Zhong, Ming and Bansal, Hritik and Han, Jiawei and Chang, Kai-Wei},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14327 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/XD7AUVI5/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/CY8ITQVY/Yin et al. - 2023 - Dynosaur A Dynamic Growth Paradigm for Instructio.pdf:application/pdf},
}

@misc{gao_pile_2020,
	title = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
	shorttitle = {The {Pile}},
	url = {http://arxiv.org/abs/2101.00027},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	month = dec,
	year = {2020},
	note = {arXiv:2101.00027 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/8HUTHMAM/2101.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/2ZSFU95K/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf},
}

@misc{kocetkov_stack_2022,
	title = {The {Stack}: 3 {TB} of permissively licensed source code},
	shorttitle = {The {Stack}},
	url = {http://arxiv.org/abs/2211.15533},
	abstract = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Muñoz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15533 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, dataset},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/S96NISLZ/2211.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/FTAM6CFQ/Kocetkov et al. - 2022 - The Stack 3 TB of permissively licensed source co.pdf:application/pdf},
}

@misc{austin_program_2021,
	title = {Program {Synthesis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2108.07732},
	abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, dataset},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/2VQ5DR83/2108.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/9Z3TDVUL/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf:application/pdf},
}

@article{ding_crosscodeeval_nodate,
	title = {{CROSSCODEEVAL}: {A} {Diverse} and {Multilingual} {Benchmark} for {Cross}-{File} {Code} {Completion}},
	abstract = {Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.},
	language = {en},
	author = {Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
	keywords = {dataset},
	file = {Ding et al. - CROSSCODEEVAL A Diverse and Multilingual Benchmar.pdf:/Users/akazad/Zotero/storage/XLVMCIGM/Ding et al. - CROSSCODEEVAL A Diverse and Multilingual Benchmar.pdf:application/pdf},
}

@article{colavito_leveraging_2024,
	title = {Leveraging {GPT}-like {LLMs} to {Automate} {Issue} {Labeling}},
	abstract = {Issue labeling is a crucial task for the e�ective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the �ne-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classi�cation, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to �ne-tune BERT-like LLMs.},
	language = {en},
	author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole and Quaranta, Luigi},
	year = {2024},
	file = {Colavito et al. - 2024 - Leveraging GPT-like LLMs to Automate Issue Labelin.pdf:/Users/akazad/Zotero/storage/U7RI8D3E/Colavito et al. - 2024 - Leveraging GPT-like LLMs to Automate Issue Labelin.pdf:application/pdf},
}

@inproceedings{spinellis_dataset_2020,
	address = {Seoul Republic of Korea},
	title = {A {Dataset} for {GitHub} {Repository} {Deduplication}},
	isbn = {978-1-4503-7517-7},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387496},
	doi = {10.1145/3379597.3387496},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Spinellis, Diomidis and Kotti, Zoe and Mockus, Audris},
	month = jun,
	year = {2020},
	pages = {523--527},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/6EHV9VI9/Spinellis et al. - 2020 - A Dataset for GitHub Repository Deduplication.pdf:application/pdf},
}

@misc{sun_dial-insight_2024,
	title = {Dial-insight: {Fine}-tuning {Large} {Language} {Models} with {High}-{Quality} {Domain}-{Specific} {Data} {Preventing} {Capability} {Collapse}},
	shorttitle = {Dial-insight},
	url = {http://arxiv.org/abs/2403.09167},
	abstract = {The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general LLMs can be enhanced through fine-tuning with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for fine-tuning.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Sun, Jianwei and Mei, Chaoyang and Wei, Linlin and Zheng, Kaiyu and Liu, Na and Cui, Ming and Li, Tianyi},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09167 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EU2GHDZE/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/M3G5MJY6/Sun et al. - 2024 - Dial-insight Fine-tuning Large Language Models wi.pdf:application/pdf},
}

@misc{lai_ds-1000_2022,
	title = {{DS}-1000: {A} {Natural} and {Reliable} {Benchmark} for {Data} {Science} {Code} {Generation}},
	shorttitle = {{DS}-1000},
	url = {http://arxiv.org/abs/2211.11501},
	abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11501 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/GHPUTNW8/2211.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/PDPS8GTT/Lai et al. - 2022 - DS-1000 A Natural and Reliable Benchmark for Data.pdf:application/pdf},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/WI3EHGYB/2107.html:text/html;arXiv.org Snapshot:/Users/akazad/Zotero/storage/GMNIXHGS/2107.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/9PWLJIX4/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf},
}

@misc{guo_deepseek-coder_2024,
	title = {{DeepSeek}-{Coder}: {When} the {Large} {Language} {Model} {Meets} {Programming} -- {The} {Rise} of {Code} {Intelligence}},
	shorttitle = {{DeepSeek}-{Coder}},
	url = {http://arxiv.org/abs/2401.14196},
	abstract = {The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y. and Li, Y. K. and Luo, Fuli and Xiong, Yingfei and Liang, Wenfeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14196 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/HCJXBJYX/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/7N2PY7RV/Guo et al. - 2024 - DeepSeek-Coder When the Large Language Model Meet.pdf:application/pdf},
}

@misc{haluptzok_language_2023,
	title = {Language {Models} {Can} {Teach} {Themselves} to {Program} {Better}},
	url = {http://arxiv.org/abs/2207.14502},
	abstract = {Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
	month = apr,
	year = {2023},
	note = {arXiv:2207.14502 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EDSS4G54/2207.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/VUM2HEU8/Haluptzok et al. - 2023 - Language Models Can Teach Themselves to Program Be.pdf:application/pdf},
}

@misc{nguyen_how_2024,
	title = {How {Beginning} {Programmers} and {Code} {LLMs} ({Mis})read {Each} {Other}},
	url = {http://arxiv.org/abs/2401.15232},
	abstract = {Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15232 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/6L6MBZWB/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/UDC2W52K/Nguyen et al. - 2024 - How Beginning Programmers and Code LLMs (Mis)read .pdf:application/pdf},
}

@misc{asai_reliable_2024,
	title = {Reliable, {Adaptable}, and {Attributable} {Language} {Models} with {Retrieval}},
	url = {http://arxiv.org/abs/2403.03187},
	abstract = {Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Asai, Akari and Zhong, Zexuan and Chen, Danqi and Koh, Pang Wei and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Yih, Wen-tau},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03187 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, rag},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/MNC92NIG/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NCEJPNUV/Asai et al. - 2024 - Reliable, Adaptable, and Attributable Language Mod.pdf:application/pdf},
}

@misc{li_blade_2024,
	title = {{BLADE}: {Enhancing} {Black}-box {Large} {Language} {Models} with {Small} {Domain}-{Specific} {Models}},
	shorttitle = {{BLADE}},
	url = {http://arxiv.org/abs/2403.18365},
	abstract = {Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM. Extensive experiments conducted on public legal and medical benchmarks reveal that BLADE significantly outperforms existing approaches. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Li, Haitao and Ai, Qingyao and Chen, Jia and Dong, Qian and Wu, Zhijing and Liu, Yiqun and Chen, Chong and Tian, Qi},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18365 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/22LZY74X/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/TZT99KY9/Li et al. - 2024 - BLADE Enhancing Black-box Large Language Models w.pdf:application/pdf},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/4FQ2ND38/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/MRWR866A/Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf:application/pdf},
}

@misc{zheng_opencodeinterpreter_2024,
	title = {{OpenCodeInterpreter}: {Integrating} {Code} {Generation} with {Execution} and {Refinement}},
	shorttitle = {{OpenCodeInterpreter}},
	url = {http://arxiv.org/abs/2402.14658},
	abstract = {The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14658 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/H6JYI95Q/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/QXIPF42I/Zheng et al. - 2024 - OpenCodeInterpreter Integrating Code Generation w.pdf:application/pdf},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2024-03-30},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/FTEQ2VA2/2310.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/DZV3KNFY/Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf},
}

@misc{amatriain_prompt_2024,
	title = {Prompt {Design} and {Engineering}: {Introduction} and {Advanced} {Methods}},
	shorttitle = {Prompt {Design} and {Engineering}},
	url = {http://arxiv.org/abs/2401.14423},
	abstract = {Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.},
	urldate = {2024-03-31},
	publisher = {arXiv},
	author = {Amatriain, Xavier},
	month = feb,
	year = {2024},
	note = {arXiv:2401.14423 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/JYFR43X9/2401.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/3IDSMZG5/Amatriain - 2024 - Prompt Design and Engineering Introduction and Ad.pdf:application/pdf},
}

@incollection{pena_leveraging_2023,
	title = {Leveraging {Large} {Language} {Models} for {Topic} {Classification} in the {Domain} of {Public} {Affairs}},
	volume = {14193},
	url = {http://arxiv.org/abs/2306.02864},
	abstract = {The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 different topics in the data in different configurations. The results shows that LLMs can be of great use to process domain-specific documents, such as those in the domain of public affairs.},
	urldate = {2024-03-31},
	author = {Peña, Alejandro and Morales, Aythami and Fierrez, Julian and Serna, Ignacio and Ortega-Garcia, Javier and Puente, Iñigo and Cordova, Jorge and Cordova, Gonzalo},
	year = {2023},
	doi = {10.1007/978-3-031-41498-5_2},
	note = {arXiv:2306.02864 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	pages = {20--33},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/AG7KQ6QN/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/35Z2YX2D/Peña et al. - 2023 - Leveraging Large Language Models for Topic Classif.pdf:application/pdf},
}

@misc{mukherjee_orca_2023,
	title = {Orca: {Progressive} {Learning} from {Complex} {Explanation} {Traces} of {GPT}-4},
	shorttitle = {Orca},
	url = {http://arxiv.org/abs/2306.02707},
	abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42\% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02707 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/BLA424H7/2306.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/G6M2KGXM/Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanatio.pdf:application/pdf},
}

@article{mcdowell_189_nodate,
	title = {189 {Programming} {Questions} and {Solutions}},
	language = {en},
	author = {Mcdowell, Gayle Laakmann},
	file = {Mcdowell - 189 Programming Questions and Solutions.pdf:/Users/akazad/Zotero/storage/IU7ZHTYH/Mcdowell - 189 Programming Questions and Solutions.pdf:application/pdf},
}

@misc{mosbach_few-shot_2023,
	title = {Few-shot {Fine}-tuning vs. {In}-context {Learning}: {A} {Fair} {Comparison} and {Evaluation}},
	shorttitle = {Few-shot {Fine}-tuning vs. {In}-context {Learning}},
	url = {http://arxiv.org/abs/2305.16938},
	abstract = {Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
	month = may,
	year = {2023},
	note = {arXiv:2305.16938 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/64VDAZLG/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/NRFGEJ8C/Mosbach et al. - 2023 - Few-shot Fine-tuning vs. In-context Learning A Fa.pdf:application/pdf},
}

@misc{ding_vulnerability_2024,
	title = {Vulnerability {Detection} with {Code} {Language} {Models}: {How} {Far} {Are} {We}?},
	shorttitle = {Vulnerability {Detection} with {Code} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.18624},
	abstract = {In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection. To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions. Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26\% F1 on BigVul but only 3.09\% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Fu, Yanjun and Ibrahim, Omniyyah and Sitawarin, Chawin and Chen, Xinyun and Alomair, Basel and Wagner, David and Ray, Baishakhi and Chen, Yizheng},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18624 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/YT8YHSEG/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/AAD82QD5/Ding et al. - 2024 - Vulnerability Detection with Code Language Models.pdf:application/pdf},
}

@misc{ahmed_studying_2024,
	title = {Studying {LLM} {Performance} on {Closed}- and {Open}-source {Data}},
	url = {http://arxiv.org/abs/2402.15100},
	abstract = {Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C\# and C++. We find that performance for C\# changes little from OSS --{\textgreater} proprietary code, but does significantly reduce for C++; we find that this difference is attributable to differences in identifiers. We also find that some performance degradation, in some cases, can be ameliorated efficiently by in-context learning.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Ahmed, Toufique and Bird, Christian and Devanbu, Premkumar and Chakraborty, Saikat},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15100 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/XBJB7JID/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/GXQ2J7FC/Ahmed et al. - 2024 - Studying LLM Performance on Closed- and Open-sourc.pdf:application/pdf},
}

@misc{spiess_calibration_2024,
	title = {Calibration and {Correctness} of {Language} {Models} for {Code}},
	url = {http://arxiv.org/abs/2402.02047},
	abstract = {Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected. Calibration has so far been studied in mostly non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. In this paper we make several contributions. We develop a framework for evaluating the Calibration of code-generating models. We consider several tasks, correctness criteria, datasets, and approaches, and find that by and large generative code models are not well-calibrated out of the box. We then show how Calibration can be improved, using standard methods such as Platt scaling. Our contributions will lead to better-calibrated decision-making in the current use of code generated by language models, and offers a framework for future research to further improve calibration methods for generative models in Software Engineering.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Spiess, Claudio and Gros, David and Pai, Kunal Suresh and Pradel, Michael and Rabin, Md Rafiqul Islam and Alipour, Amin and Jha, Susmit and Devanbu, Prem and Ahmed, Toufique},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02047 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/PZRVBHID/2402.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/VA9N5WE6/Spiess et al. - 2024 - Calibration and Correctness of Language Models for.pdf:application/pdf},
}

@misc{ding_cycle_2024,
	title = {{CYCLE}: {Learning} to {Self}-{Refine} the {Code} {Generation}},
	shorttitle = {{CYCLE}},
	url = {http://arxiv.org/abs/2403.18746},
	abstract = {Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well. In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs. We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5\%, across benchmarks and varied model sizes. We also notice that CYCLE outperforms code LMs that have 3\${\textbackslash}times\$ more parameters in self-refinement.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Min, Marcus J. and Kaiser, Gail and Ray, Baishakhi},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18746 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/DJYULDMG/2403.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/E7EMPRIE/Ding et al. - 2024 - CYCLE Learning to Self-Refine the Code Generation.pdf:application/pdf},
}

@misc{zhang_tired_2024,
	title = {Tired of {Plugins}? {Large} {Language} {Models} {Can} {Be} {End}-{To}-{End} {Recommenders}},
	shorttitle = {Tired of {Plugins}?},
	url = {http://arxiv.org/abs/2404.00702},
	abstract = {Recommender systems aim to predict user interest based on historical behavioral data. They are mainly designed in sequential pipelines, requiring lots of data to train different sub-systems, and are hard to scale to new domains. Recently, Large Language Models (LLMs) have demonstrated remarkable generalized capabilities, enabling a singular model to tackle diverse recommendation tasks across various scenarios. Nonetheless, existing LLM-based recommendation systems utilize LLM purely for a single task of the recommendation pipeline. Besides, these systems face challenges in presenting large-scale item sets to LLMs in natural language format, due to the constraint of input length. To address these challenges, we introduce an LLM-based end-to-end recommendation framework: UniLLMRec. Specifically, UniLLMRec integrates multi-stage tasks (e.g. recall, ranking, re-ranking) via chain-of-recommendations. To deal with large-scale items, we propose a novel strategy to structure all items into an item tree, which can be dynamically updated and effectively retrieved. UniLLMRec shows promising zero-shot results in comparison with conventional supervised models. Additionally, it boasts high efficiency, reducing the input token need by 86\% compared to existing LLM-based models. Such efficiency not only accelerates task completion but also optimizes resource utilization. To facilitate model understanding and to ensure reproducibility, we have made our code publicly available.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Zhang, Wenlin and Chuhan and Wu and Li, Xiangyang and Wang, Yuhao and Dong, Kuicai and Wang, Yichao and Dai, Xinyi and Zhao, Xiangyu and Guo, Huifeng and Tang, Ruiming},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00702 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/IMI3B2DY/2404.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/WHSU2R7T/Zhang et al. - 2024 - Tired of Plugins Large Language Models Can Be End.pdf:application/pdf},
}

@book{huyen_designing_2022,
	address = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {First edition},
	title = {Designing machine learning systems: an iterative process for production-ready applications},
	isbn = {978-1-09-810796-3},
	shorttitle = {Designing machine learning systems},
	language = {en},
	publisher = {O'Reilly},
	author = {Huyen, Chip},
	year = {2022},
	file = {Huyen - 2022 - Designing machine learning systems an iterative p.pdf:/Users/akazad/Zotero/storage/RRH8Q3J4/Huyen - 2022 - Designing machine learning systems an iterative p.pdf:application/pdf},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, scaling law for LLM},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/FRTL8TTK/2001.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/URT5MVN5/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf},
}

@misc{cotterell_formal_2023,
	title = {Formal {Aspects} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/2311.04329},
	abstract = {Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. Consequently, it is important for both developers and researchers alike to understand the mathematical foundations of large language models, as well as how to implement them. These notes are the accompaniment to the theoretical portion of the ETH Z{\textbackslash}"urich course on large language models, covering what constitutes a language model from a formal, theoretical perspective.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Cotterell, Ryan and Svete, Anej and Meister, Clara and Liu, Tianyu and Du, Li},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04329 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/3VTGI2MJ/2311.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/N9X9A2JS/Cotterell et al. - 2023 - Formal Aspects of Language Modeling.pdf:application/pdf},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = nov,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/IEIJ3Q6A/2303.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/SYEGN9BN/Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf},
}

@misc{lee_can_2024,
	title = {Can {Small} {Language} {Models} {Help} {Large} {Language} {Models} {Reason} {Better}?: {LM}-{Guided} {Chain}-of-{Thought}},
	shorttitle = {Can {Small} {Language} {Models} {Help} {Large} {Language} {Models} {Reason} {Better}?},
	url = {http://arxiv.org/abs/2404.03414},
	abstract = {We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., {\textless}1B) language model (LM) for guiding a black-box large (i.e., {\textgreater}10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Lee, Jooyoung and Yang, Fan and Tran, Thanh and Hu, Qian and Barut, Emre and Chang, Kai-Wei and Su, Chengwei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.03414 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/S8FM3WTC/2404.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/94EU275Q/Lee et al. - 2024 - Can Small Language Models Help Large Language Mode.pdf:application/pdf},
}

@inproceedings{fan_cc_2020,
	address = {Seoul Republic of Korea},
	title = {A {C}/{C}++ {Code} {Vulnerability} {Dataset} with {Code} {Changes} and {CVE} {Summaries}},
	isbn = {978-1-4503-7517-7},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387501},
	doi = {10.1145/3379597.3387501},
	language = {en},
	urldate = {2024-04-12},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Fan, Jiahao and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
	month = jun,
	year = {2020},
	pages = {508--512},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/SRT4RYYJ/Fan et al. - 2020 - A CC++ Code Vulnerability Dataset with Code Chang.pdf:application/pdf},
}

@misc{chakraborty_deep_2020,
	title = {Deep {Learning} based {Vulnerability} {Detection}: {Are} {We} {There} {Yet}?},
	shorttitle = {Deep {Learning} based {Vulnerability} {Detection}},
	url = {http://arxiv.org/abs/2009.07235},
	abstract = {Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95\% at detecting vulnerabilities. In this paper, we ask, "how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?". To our surprise, we find that their performance drops by more than 50\%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57\% boost in precision and 128.38\% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
	month = sep,
	year = {2020},
	note = {arXiv:2009.07235 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/N4CLZUEF/2009.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/CC7JEU54/Chakraborty et al. - 2020 - Deep Learning based Vulnerability Detection Are W.pdf:application/pdf},
}

@inproceedings{bhandari_cvefixes_2021,
	address = {Athens Greece},
	title = {{CVEfixes}: automated collection of vulnerabilities and their fixes from open-source software},
	isbn = {978-1-4503-8680-7},
	shorttitle = {{CVEfixes}},
	url = {https://dl.acm.org/doi/10.1145/3475960.3475985},
	doi = {10.1145/3475960.3475985},
	language = {en},
	urldate = {2024-04-12},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Bhandari, Guru and Naseer, Amara and Moonen, Leon},
	month = aug,
	year = {2021},
	pages = {30--39},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/K25J4NDN/Bhandari et al. - 2021 - CVEfixes automated collection of vulnerabilities .pdf:application/pdf},
}

@inproceedings{nikitopoulos_crossvul_2021,
	address = {Athens Greece},
	title = {{CrossVul}: a cross-language vulnerability dataset with commit data},
	isbn = {978-1-4503-8562-6},
	shorttitle = {{CrossVul}},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473122},
	doi = {10.1145/3468264.3473122},
	language = {en},
	urldate = {2024-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Nikitopoulos, Georgios and Dritsa, Konstantina and Louridas, Panos and Mitropoulos, Dimitris},
	month = aug,
	year = {2021},
	pages = {1565--1569},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/6HQXZI3X/Nikitopoulos et al. - 2021 - CrossVul a cross-language vulnerability dataset w.pdf:application/pdf},
}

@misc{ahmad_fixing_2023,
	title = {Fixing {Hardware} {Security} {Bugs} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.01215},
	abstract = {Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Ahmad, Baleegh and Thakur, Shailja and Tan, Benjamin and Karri, Ramesh and Pearce, Hammond},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01215 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EAKSXILZ/2302.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/96TMJ8FA/Ahmad et al. - 2023 - Fixing Hardware Security Bugs with Large Language .pdf:application/pdf},
}

@misc{liang_large-scale_2023,
	title = {A {Large}-{Scale} {Survey} on the {Usability} of {AI} {Programming} {Assistants}: {Successes} and {Challenges}},
	shorttitle = {A {Large}-{Scale} {Survey} on the {Usability} of {AI} {Programming} {Assistants}},
	url = {http://arxiv.org/abs/2303.17125},
	abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
	month = sep,
	year = {2023},
	note = {arXiv:2303.17125 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/V5KGQUJH/2303.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/646DVDVK/Liang et al. - 2023 - A Large-Scale Survey on the Usability of AI Progra.pdf:application/pdf},
}

@misc{katzy_impact_2023,
	title = {On the {Impact} of {Language} {Selection} for {Training} and {Evaluating} {Programming} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.13354},
	abstract = {The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a diverse set of programming languages when training and evaluating future models.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Katzy, Jonathan and Izadi, Maliheh and van Deursen, Arie},
	month = aug,
	year = {2023},
	note = {arXiv:2308.13354 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/CY73C9MN/2308.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/YTFT2LMN/Katzy et al. - 2023 - On the Impact of Language Selection for Training a.pdf:application/pdf},
}

@misc{huang_survey_2024,
	title = {A {Survey} on {Retrieval}-{Augmented} {Text} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.10981},
	abstract = {Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Huang, Yizheng and Huang, Jimmy},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10981 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/EQJTAPNJ/2404.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/LPDZHSSM/Huang and Huang - 2024 - A Survey on Retrieval-Augmented Text Generation fo.pdf:application/pdf},
}

@misc{huang_autocrawler_2024,
	title = {{AutoCrawler}: {A} {Progressive} {Understanding} {Web} {Agent} for {Web} {Crawler} {Generation}},
	shorttitle = {{AutoCrawler}},
	url = {http://arxiv.org/abs/2404.12753},
	abstract = {Web automation is a significant technique that accomplishes complicated web tasks by automating common web actions, enhancing operational efficiency, and reducing the need for manual intervention. Traditional methods, such as wrappers, suffer from limited adaptability and scalability when faced with a new website. On the other hand, generative agents empowered by large language models (LLMs) exhibit poor performance and reusability in open-world scenarios. In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently. We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding. Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at {\textbackslash}url\{https://github.com/EZ-hwh/AutoCrawler\}},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Huang, Wenhao and Peng, Chenghao and Li, Zhixu and Liang, Jiaqing and Xiao, Yanghua and Wen, Liqian and Chen, Zulong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12753 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/MRWRLDYP/2404.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/4WDEHLLS/Huang et al. - 2024 - AutoCrawler A Progressive Understanding Web Agent.pdf:application/pdf},
}

@misc{bowman_eight_2023,
	title = {Eight {Things} to {Know} about {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.00612},
	abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
	urldate = {2024-04-23},
	publisher = {arXiv},
	author = {Bowman, Samuel R.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00612 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/NGDDYTRR/2304.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/WVWA7VT6/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf:application/pdf},
}

@misc{chen_automatic_2023,
	title = {Automatic {Root} {Cause} {Analysis} via {Large} {Language} {Models} for {Cloud} {Incidents}},
	url = {http://arxiv.org/abs/2305.15778},
	abstract = {Ensuring the reliability and availability of cloud services necessitates efficient root cause analysis (RCA) for cloud incidents. Traditional RCA methods, which rely on manual investigations of data sources such as logs and traces, are often laborious, error-prone, and challenging for on-call engineers. In this paper, we introduce RCACopilot, an innovative on-call system empowered by the large language model for automating RCA of cloud incidents. RCACopilot matches incoming incidents to corresponding incident handlers based on their alert types, aggregates the critical runtime diagnostic information, predicts the incident's root cause category, and provides an explanatory narrative. We evaluate RCACopilot using a real-world dataset consisting of a year's worth of incidents from Microsoft. Our evaluation demonstrates that RCACopilot achieves RCA accuracy up to 0.766. Furthermore, the diagnostic information collection component of RCACopilot has been successfully in use at Microsoft for over four years.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Chen, Yinfang and Xie, Huaibing and Ma, Minghua and Kang, Yu and Gao, Xin and Shi, Liu and Cao, Yunjie and Gao, Xuedong and Fan, Hao and Wen, Ming and Zeng, Jun and Ghosh, Supriyo and Zhang, Xuchao and Zhang, Chaoyun and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and Xu, Tianyin},
	month = nov,
	year = {2023},
	note = {arXiv:2305.15778 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:/Users/akazad/Zotero/storage/WVALK473/2305.html:text/html;Full Text PDF:/Users/akazad/Zotero/storage/EJED7LIF/Chen et al. - 2023 - Automatic Root Cause Analysis via Large Language M.pdf:application/pdf},
}

@misc{fan_large_2023,
	title = {Large {Language} {Models} for {Software} {Engineering}: {Survey} and {Open} {Problems}},
	shorttitle = {Large {Language} {Models} for {Software} {Engineering}},
	url = {http://arxiv.org/abs/2310.03533},
	abstract = {This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs’ emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M.},
	month = nov,
	year = {2023},
	note = {arXiv:2310.03533 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Fan et al. - 2023 - Large Language Models for Software Engineering Su.pdf:/Users/akazad/Zotero/storage/3RTD5F99/Fan et al. - 2023 - Large Language Models for Software Engineering Su.pdf:application/pdf},
}

@inproceedings{ma_llmparser_2024,
	address = {Lisbon Portugal},
	title = {{LLMParser}: {An} {Exploratory} {Study} on {Using} {Large} {Language} {Models} for {Log} {Parsing}},
	isbn = {9798400702174},
	shorttitle = {{LLMParser}},
	url = {https://dl.acm.org/doi/10.1145/3597503.3639150},
	doi = {10.1145/3597503.3639150},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
	month = apr,
	year = {2024},
	pages = {1--13},
	file = {Ma et al. - 2024 - LLMParser An Exploratory Study on Using Large Lan.pdf:/Users/akazad/Zotero/storage/LDU6LCSJ/Ma et al. - 2024 - LLMParser An Exploratory Study on Using Large Lan.pdf:application/pdf},
}

@misc{hossain_deep_2024,
	title = {A {Deep} {Dive} into {Large} {Language} {Models} for {Automated} {Bug} {Localization} and {Repair}},
	url = {http://arxiv.org/abs/2404.11595},
	abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug fixing utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: TokenGranulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment unit, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50, and Top-100 metrics. Additionally, this paper examines Toggle’s generalizability to unseen data, evaluates the effectiveness of various prompts, investigates the impact of additional contextual information such as buggy lines and code comments on bug localization, and explores the importance of the adjustment unit. Our extensive experiments offer valuable insights and answers to critical research questions.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11595 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Hossain et al. - 2024 - A Deep Dive into Large Language Models for Automat.pdf:/Users/akazad/Zotero/storage/4JTRAZ63/Hossain et al. - 2024 - A Deep Dive into Large Language Models for Automat.pdf:application/pdf},
}

@misc{yuan_evaluating_2023,
	title = {Evaluating {Instruction}-{Tuned} {Large} {Language} {Models} on {Code} {Comprehension} and {Generation}},
	url = {http://arxiv.org/abs/2308.01240},
	abstract = {Instruction tuning has been proposed to enhance the generalization capability of large language models (LLMs) on new downstream tasks. To date, many efforts have been dedicated into evaluating instructed LLMs, covering not only general NLP tasks but also specific domains. However, little evaluation of instructed LLMs is diving into the software engineering domain, except the NL-to-Code task (generating a function for the given natural language description), which is only one of the coderelated tasks in software development and maintenance. Although some recent work explores the capability of the instructed models such as ChatGPT on SE tasks, these commercial models are closed-source, thus lacking transparency and reproducibility. Overall, it still remains unclear how the recent open-source instructed LLMs perform on diverse code comprehension and generation tasks.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Yuan, Zhiqiang and Liu, Junwei and Zi, Qiancheng and Liu, Mingwei and Peng, Xin and Lou, Yiling},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01240 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Yuan et al. - 2023 - Evaluating Instruction-Tuned Large Language Models.pdf:/Users/akazad/Zotero/storage/QIFPUMZW/Yuan et al. - 2023 - Evaluating Instruction-Tuned Large Language Models.pdf:application/pdf},
}

@inproceedings{dong_merge_2023,
	address = {Luxembourg, Luxembourg},
	title = {Merge {Conflict} {Resolution}: {Classification} or {Generation}?},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {Merge {Conflict} {Resolution}},
	url = {https://ieeexplore.ieee.org/document/10298509/},
	doi = {10.1109/ASE56229.2023.00155},
	abstract = {Collaborative development is critical to improve the productivity. Multiple contributors work simultaneously on the same project and might make changes to the same code locations. This can cause conflicts and require manual intervention from developers to resolve them. To alleviate the human efforts of manual conflict resolution, researchers have proposed various automatic techniques. More recently, deep learning models have been adopted to solve this problem and achieved state-of-the-art performance. However, these techniques leverage classification to combine the existing elements of input. The classificationbased models cannot generate new tokens or produce flexible combinations, and have a wrong hypothesis that fine-grained conflicts of one single coarse-grained conflict are independent.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Dong, Jinhao and Zhu, Qihao and Sun, Zeyu and Lou, Yiling and Hao, Dan},
	month = sep,
	year = {2023},
	pages = {1652--1663},
	file = {Dong et al. - 2023 - Merge Conflict Resolution Classification or Gener.pdf:/Users/akazad/Zotero/storage/ZQN7385S/Dong et al. - 2023 - Merge Conflict Resolution Classification or Gener.pdf:application/pdf},
}

@inproceedings{wang_mining_2023,
	address = {San Francisco CA USA},
	title = {Mining {Resource}-{Operation} {Knowledge} to {Support} {Resource} {Leak} {Detection}},
	isbn = {9798400703270},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616315},
	doi = {10.1145/3611643.3616315},
	abstract = {Resource leaks, which are caused by acquired resources not being released, often result in performance degradation and system crashes. Resource leak detection relies on two essential components: identifying potential Resource Acquisition and Release (RAR) API pairs, and subsequently analyze code to uncover instances where the corresponding release API call is absent after an acquisition API call. Yet, existing techniques confine themselves to an incomplete pair pool, either pre-defined manually or mined from project-specific code corpus, thus limiting coverage across libraries/APIs and potentially overlooking latent resource leaks. In this work, we propose to represent resource-operation knowledge as abstract resource acquisition/release operation pairs (Abs-RAR pairs for short), and present a novel approach called MiROK to mine such Abs-RAR pairs to construct a better RAR pair pool. Given a large code corpus, MiROK first mines Abs-RAR pairs with rule-based pair expansion and learning-based pair identification strategies, and then instantiates these Abs-RAR pairs into concrete RAR pairs. We implement MiROK and apply it to mine RAR pairs from a large code corpus of 1,454,224 Java methods and 20,000 Maven libraries. We then perform an extensive evaluation to investigate the mining effectiveness of MiROK and the practical usage of its mined RAR pairs for supporting resource leak detection. Our results show that MiROK mines 1,313 new Abs-RAR pairs and instantiates them into 6,314 RAR pairs with a high precision (i.e., 93.3\%). In addition, by feeding our mined RAR pairs, existing approaches detect more resource leak defects in both online code examples and open-source projects.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Chong and Lou, Yiling and Peng, Xin and Liu, Jianan and Zou, Baihan},
	month = nov,
	year = {2023},
	pages = {986--998},
	file = {Wang et al. - 2023 - Mining Resource-Operation Knowledge to Support Res.pdf:/Users/akazad/Zotero/storage/MZ4RCAN9/Wang et al. - 2023 - Mining Resource-Operation Knowledge to Support Res.pdf:application/pdf},
}

@inproceedings{yang_large-scale_2023,
	address = {San Francisco CA USA},
	title = {A {Large}-{Scale} {Empirical} {Review} of {Patch} {Correctness} {Checking} {Approaches}},
	isbn = {9798400703270},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616331},
	doi = {10.1145/3611643.3616331},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Yang, Jun and Wang, Yuehan and Lou, Yiling and Wen, Ming and Zhang, Lingming},
	month = nov,
	year = {2023},
	pages = {1203--1215},
	file = {Yang et al. - 2023 - A Large-Scale Empirical Review of Patch Correctnes.pdf:/Users/akazad/Zotero/storage/9MX8DJDK/Yang et al. - 2023 - A Large-Scale Empirical Review of Patch Correctnes.pdf:application/pdf},
}

@misc{du_classeval_2023,
	title = {{ClassEval}: {A} {Manually}-{Crafted} {Benchmark} for {Evaluating} {LLMs} on {Class}-level {Code} {Generation}},
	shorttitle = {{ClassEval}},
	url = {http://arxiv.org/abs/2308.01861},
	abstract = {In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at https://github.com/FudanSELab/ClassEval.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01861 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Du et al. - 2023 - ClassEval A Manually-Crafted Benchmark for Evalua.pdf:/Users/akazad/Zotero/storage/6W9TC5XR/Du et al. - 2023 - ClassEval A Manually-Crafted Benchmark for Evalua.pdf:application/pdf},
}

@inproceedings{jiao_evaluation_2023,
	address = {Luxembourg, Luxembourg},
	title = {On the {Evaluation} of {Neural} {Code} {Translation}: {Taxonomy} and {Benchmark}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {On the {Evaluation} of {Neural} {Code} {Translation}},
	url = {https://ieeexplore.ieee.org/document/10298408/},
	doi = {10.1109/ASE56229.2023.00114},
	abstract = {In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our ﬁndings indicate that while stateof-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and ﬁner-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful ﬁndings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Jiao, Mingsheng and Yu, Tingrui and Li, Xuan and Qiu, Guanjie and Gu, Xiaodong and Shen, Beijun},
	month = sep,
	year = {2023},
	pages = {1529--1541},
	file = {Jiao et al. - 2023 - On the Evaluation of Neural Code Translation Taxo.pdf:/Users/akazad/Zotero/storage/ASF6QJXF/Jiao et al. - 2023 - On the Evaluation of Neural Code Translation Taxo.pdf:application/pdf},
}

@misc{ramos_melt_2023,
	title = {{MELT}: {Mining} {Effective} {Lightweight} {Transformations} from {Pull} {Requests}},
	shorttitle = {{MELT}},
	url = {http://arxiv.org/abs/2308.14687},
	abstract = {Software developers often struggle to update APIs, leading to manual, time-consuming, and error-prone processes. We introduce MELT, a new approach that generates lightweight API migration rules directly from pull requests in popular library repositories. Our key insight is that pull requests merged into open-source libraries are a rich source of information sufficient to mine API migration rules. By leveraging code examples mined from the library source and automatically generated code examples based on the pull requests, we infer transformation rules in Comby, a language for structural code search and replace. Since inferred rules from single code examples may be too specific, we propose a generalization procedure to make the rules more applicable to client projects. MELT rules are syntax-driven, interpretable, and easily adaptable. Moreover, unlike previous work, our approach enables rule inference to seamlessly integrate into the library workflow, removing the need to wait for client code migrations. We evaluated MELT on pull requests from four popular libraries, successfully mining 461 migration rules from code examples in pull requests and 114 rules from autogenerated code examples. Our generalization procedure increases the number of matches for mined rules by 9×. We applied these rules to client projects and ran their tests, which led to an overall decrease in the number of warnings and fixing some test cases demonstrating MELT’s effectiveness in real-world scenarios.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Ramos, Daniel and Mitchell, Hailie and Lynce, Inês and Manquinho, Vasco and Martins, Ruben and Goues, Claire Le},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14687 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {Ramos et al. - 2023 - MELT Mining Effective Lightweight Transformations.pdf:/Users/akazad/Zotero/storage/ZVBVMMN9/2308.pdf:application/pdf},
}

@inproceedings{malkadi_improving_2023,
	address = {Luxembourg, Luxembourg},
	title = {Improving {Code} {Extraction} from {Coding} {Screencasts} {Using} a {Code}-{Aware} {Encoder}-{Decoder} {Model}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298433/},
	doi = {10.1109/ASE56229.2023.00184},
	abstract = {Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-theart in code extraction techniques from screencasts and images.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Malkadi, Abdulkarim and Tayeb, Ahmad and Haiduc, Sonia},
	month = sep,
	year = {2023},
	pages = {1492--1504},
	file = {Malkadi et al. - 2023 - Improving Code Extraction from Coding Screencasts .pdf:/Users/akazad/Zotero/storage/YQ7X358P/Malkadi et al. - 2023 - Improving Code Extraction from Coding Screencasts .pdf:application/pdf},
}

@inproceedings{shao_information_2023,
	address = {Luxembourg, Luxembourg},
	title = {Information {Retrieval}-{Based} {Fault} {Localization} for {Concurrent} {Programs}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298382/},
	doi = {10.1109/ASE56229.2023.00122},
	abstract = {Information retrieval-based fault localization (IRFL) techniques have been proposed as a solution to identify the files that are likely to contain faults that are root causes of failures reported by users. These techniques have been extensively studied to accurately rank source files, however, none of the existing approaches have focused on the specific case of concurrent programs. This is a critical issue since concurrency bugs are notoriously difficult to identify. To address this problem, this paper presents a novel approach called BLCoiR, which aims to reformulate bug report queries to more accurately localize source files related to concurrency bugs. The key idea of BLCoiR is based on a novel knowledge graph (KG), which represents the domain entities extracted from the concurrency bug reports and their semantic relations. The KG is then transformed into the IR query to perform fault localization. BLCoiR leverages natural language processing (NLP) and concept modeling techniques to construct the knowledge graph. Specifically, NLP techniques are used to extract relevant entities from the bug reports, such as the word entities related to concurrency constructs. These entities are then linked together based on their semantic relationships, forming the KG. We have conducted an empirical study on 692 concurrency bug reports from 44 real-world applications. The results show that BLCoiR outperforms existing IRFL techniques in terms of accuracy and efficiency in localizing concurrency bugs. BLCoiR demonstrates effectiveness of using a knowledge graph to model the domain entities and their relationships, providing a promising direction for future research in this area.},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Shao, Shuai and Yu, Tingting},
	month = sep,
	year = {2023},
	pages = {1467--1479},
	file = {Shao and Yu - 2023 - Information Retrieval-Based Fault Localization for.pdf:/Users/akazad/Zotero/storage/R5TU7CLK/Shao and Yu - 2023 - Information Retrieval-Based Fault Localization for.pdf:application/pdf},
}

@inproceedings{sun_smt_2023,
	address = {Luxembourg, Luxembourg},
	title = {{SMT} {Solver} {Validation} {Empowered} by {Large} {Pre}-{Trained} {Language} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298442/},
	doi = {10.1109/ASE56229.2023.00180},
	language = {en},
	urldate = {2024-04-26},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Sun, Maolin and Yang, Yibiao and Wang, Yang and Wen, Ming and Jia, Haoxiang and Zhou, Yuming},
	month = sep,
	year = {2023},
	pages = {1288--1300},
	file = {Sun et al. - 2023 - SMT Solver Validation Empowered by Large Pre-Train.pdf:/Users/akazad/Zotero/storage/NKBITTQ7/Sun et al. - 2023 - SMT Solver Validation Empowered by Large Pre-Train.pdf:application/pdf},
}

@inproceedings{malviya_fine-grained_2023,
	address = {Luxembourg, Luxembourg},
	title = {Fine-{Grained} {In}-{Context} {Permission} {Classification} for {Android} {Apps} {Using} {Control}-{Flow} {Graph} {Embedding}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298328/},
	doi = {10.1109/ASE56229.2023.00056},
	abstract = {Android is the most popular operating system for mobile devices nowadays. Permissions are a very important part of Android security architecture. Apps frequently need the users’ permission, but many of them only ask for it once—when the user uses the app for the first time—and then they keep and abuse the given permissions. Longing to enhance Android permission security and users’ private data protection is the driving factor behind our approach to explore fine-grained contextsensitive permission usage analysis and thereby identify misuses in Android apps. In this work, we propose an approach for classifying the fine-grained permission uses for each functionality of Android apps that a user interacts with. Our approach, named DROIDGEM, relies on mainly three technical components to provide an in-context classification for permission (mis)uses by Android apps for each functionality triggered by users: (1) static inter-procedural control-flow graphs and call graphs representing each functionality in an app that may be triggered by users’ or systems’ events through UI-linked event handlers, (2) graph embedding techniques converting graph structures into numerical encoding, and (3) supervised machine learning models classifying (mis)uses of permissions based on the embedding. We have implemented a prototype of DROIDGEM and evaluated it on 89 diverse apps. The results show that DROIDGEM can accurately classify whether permission used by the functionality of an app triggered by a UI-linked event handler is a misuse in relation to manually verified decisions, with up to 95\% precision and recall. We believe that such a permission classification mechanism can be helpful in providing fine-grained permission notices in a context related to app users’ actions, and improving their awareness of (mis)uses of permissions and private data in Android apps.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Malviya, Vikas K. and Tun, Yan Naing and Leow, Chee Wei and Xynyn, Ailys Tee and Shar, Lwin Khin and Jiang, Lingxiao},
	month = sep,
	year = {2023},
	pages = {1225--1237},
	file = {Malviya et al. - 2023 - Fine-Grained In-Context Permission Classification .pdf:/Users/akazad/Zotero/storage/5C9GHR42/Malviya et al. - 2023 - Fine-Grained In-Context Permission Classification .pdf:application/pdf},
}

@inproceedings{hu_identify_2023,
	address = {Luxembourg, Luxembourg},
	title = {Identify and {Update} {Test} {Cases} {When} {Production} {Code} {Changes}: {A} {Transformer}-{Based} {Approach}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {Identify and {Update} {Test} {Cases} {When} {Production} {Code} {Changes}},
	url = {https://ieeexplore.ieee.org/document/10298577/},
	doi = {10.1109/ASE56229.2023.00165},
	abstract = {Software testing is one of the most essential parts of the software lifecycle and requires a substantial amount of time and effort. During the software evolution, test cases should coevolve with the production code. However, the co-evolution of test cases often fails due to tight project schedules and other reasons. Obsolete test cases improve the cost of software maintenance and may fail to reveal faults and even lead to future bugs. Therefore, it is essential to detect and update these obsolete test cases in time. In this paper, we propose a novel approach CEPROT (Co-Evolution of Production-Test Code) to identify outdated test cases and update them automatically according to changes in the production code. CEPROT consists of two stages, i.e., obsolete test identiﬁcation and updating. Speciﬁcally, given a production code change and a corresponding test case, CEPROT ﬁrst identiﬁes whether the test case should be updated. If the test is identiﬁed as obsolete, CEPROT will update it to a new version of test case. To evaluate the effectiveness of the two stages, we construct two datasets. Our dataset focuses on method-level production code changes and updates on their obsolete test cases. The experimental results show that CEPROT can effectively identify obsolete test cases with precision and recall of 98.3\% and 90.0\%, respectively. In addition, test cases generated by CEPROT are identical to the ground truth for 12.3\% of samples that are identiﬁed as obsolete by CEPROT. We also conduct dynamic evaluation and human evaluation to measure the effectiveness of the updated test cases by CEPROT. 48.0\% of updated test cases can be compiled and the average coverage of updated cases is 34.2\% which achieves 89\% coverage improvement over the obsolete tests. We believe that this study can motivate the coevolution of production and test code.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Hu, Xing and Liu, Zhuang and Xia, Xin and Liu, Zhongxin and Xu, Tongtong and Yang, Xiaohu},
	month = sep,
	year = {2023},
	pages = {1111--1122},
	file = {Hu et al. - 2023 - Identify and Update Test Cases When Production Cod.pdf:/Users/akazad/Zotero/storage/8X7UCNL5/299600b111.pdf:application/pdf},
}

@inproceedings{richter_how_2023,
	address = {Luxembourg, Luxembourg},
	title = {How to {Train} {Your} {Neural} {Bug} {Detector}: {Artificial} vs {Real} {Bugs}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {How to {Train} {Your} {Neural} {Bug} {Detector}},
	url = {https://ieeexplore.ieee.org/document/10298391/},
	doi = {10.1109/ASE56229.2023.00104},
	abstract = {Real bug ﬁxes found in open source repositories seem to be the perfect source for learning to localize and repair real bugs. Yet, the scale of existing bug ﬁx collections is typically too small for training data-intensive neural approaches. Neural bug detectors are hence almost exclusively trained on artiﬁcial bugs, produced by mutating existing source code and thus easily obtainable at large scales. However, neural bug detectors trained on artiﬁcial bugs usually underperform when faced with real bugs.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Richter, Cedric and Wehrheim, Heike},
	month = sep,
	year = {2023},
	pages = {1036--1048},
	file = {Richter and Wehrheim - 2023 - How to Train Your Neural Bug Detector Artificial .pdf:/Users/akazad/Zotero/storage/4ZBIHR2N/Richter and Wehrheim - 2023 - How to Train Your Neural Bug Detector Artificial .pdf:application/pdf},
}

@inproceedings{tian_code_2023,
	address = {Luxembourg, Luxembourg},
	title = {Code {Difference} {Guided} {Adversarial} {Example} {Generation} for {Deep} {Code} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298520/},
	doi = {10.1109/ASE56229.2023.00149},
	abstract = {Adversarial examples are important to test and enhance the robustness of deep code models. As source code is discrete and has to strictly stick to complex grammar and semantics constraints, the adversarial example generation techniques in other domains are hardly applicable. Moreover, the adversarial example generation techniques specific to deep code models still suffer from unsatisfactory effectiveness due to the enormous ingredient search space. In this work, we propose a novel adversarial example generation technique (i.e., CODA) for testing deep code models. Its key idea is to use code differences between the target input (i.e., a given code snippet as the model input) and reference inputs (i.e., the inputs that have small code differences but different prediction results with the target input) to guide the generation of adversarial examples. It considers both structure differences and identifier differences to preserve the original semantics. Hence, the ingredient search space can be largely reduced as the one constituted by the two kinds of code differences, and thus the testing process can be improved by designing and guiding corresponding equivalent structure transformations and identifier renaming transformations. Our experiments on 15 deep code models demonstrate the effectiveness and efficiency of CODA, the naturalness of its generated examples, and its capability of enhancing model robustness after adversarial fine-tuning. For example, CODA reveals 88.05\% and 72.51\% more faults in models than the state-of-the-art techniques (i.e., CARROT and ALERT) on average, respectively.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Tian, Zhao and Chen, Junjie and Jin, Zhi},
	month = sep,
	year = {2023},
	pages = {850--862},
	file = {Tian et al. - 2023 - Code Difference Guided Adversarial Example Generat.pdf:/Users/akazad/Zotero/storage/LHZH3BLE/Tian et al. - 2023 - Code Difference Guided Adversarial Example Generat.pdf:application/pdf},
}

@inproceedings{baral_optimizing_2023,
	address = {Luxembourg, Luxembourg},
	title = {Optimizing {Continuous} {Development} by {Detecting} and {Preventing} {Unnecessary} {Content} {Generation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298438/},
	doi = {10.1109/ASE56229.2023.00216},
	abstract = {Continuous development (CD) helps developers quickly release and update their software. To enact CD, developers customize their CD builds to perform several tasks, including compiling, testing, static analysis checks, etc. However, as developers add more tasks to their builds, the builds take longer to run, therefore slowing down the entire CD process. Furthermore, developers may unknowingly include tasks into their builds whose results are not used (e.g., generating coverage ﬁles that are never read or uploaded anywhere), therefore wasting build runtime doing unnecessary tasks.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Baral, Talank and Rahman, Shanto and Chanumolu, Bala Naren and Balcı, Başak and Tuncer, Tuna and Shi, August and Lam, Wing},
	month = sep,
	year = {2023},
	keywords = {implication},
	pages = {901--913},
	file = {Baral et al. - 2023 - Optimizing Continuous Development by Detecting and.pdf:/Users/akazad/Zotero/storage/RUYZJB6P/Baral et al. - 2023 - Optimizing Continuous Development by Detecting and.pdf:application/pdf},
}

@inproceedings{jiang_scpatcher_2023,
	address = {Luxembourg, Luxembourg},
	title = {{SCPatcher}: {Mining} {Crowd} {Security} {Discussions} to {Enrich} {Secure} {Coding} {Practices}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {{SCPatcher}},
	url = {https://ieeexplore.ieee.org/document/10298463/},
	doi = {10.1109/ASE56229.2023.00040},
	abstract = {Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically onesentence principles without detailed specifications, e.g., “Properly free allocated memory upon the completion of functions and at all exit points.”, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73\% MLine on average, as well as coding explanations with 3.97\% F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing},
	month = sep,
	year = {2023},
	pages = {358--370},
	file = {Jiang et al. - 2023 - SCPatcher Mining Crowd Security Discussions to En.pdf:/Users/akazad/Zotero/storage/BWX7SNVJ/Jiang et al. - 2023 - SCPatcher Mining Crowd Security Discussions to En.pdf:application/pdf},
}

@inproceedings{wen_when_2023,
	address = {Luxembourg, Luxembourg},
	title = {When {Less} is {Enough}: {Positive} and {Unlabeled} {Learning} {Model} for {Vulnerability} {Detection}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {When {Less} is {Enough}},
	url = {https://ieeexplore.ieee.org/document/10298363/},
	doi = {10.1109/ASE56229.2023.00144},
	abstract = {Automated code vulnerability detection has gained increasing attention in recent years. The deep learning (DL)based methods, which implicitly learn vulnerable code patterns, have proven effective in vulnerability detection. The performance of DL-based methods usually relies on the quantity and quality of labeled data. However, the current labeled data are generally automatically collected, such as crawled from human-generated commits, making it hard to ensure the quality of the labels. Prior studies have demonstrated that the non-vulnerable code (i.e., negative labels) tends to be unreliable in commonly-used datasets, while vulnerable code (i.e., positive labels) is more determined. Considering the large numbers of unlabeled data in practice, it is necessary and worth exploring to leverage the positive data and large numbers of unlabeled data for more accurate vulnerability detection.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Wen, Xin-Cheng and Wang, Xinchen and Gao, Cuiyun and Wang, Shaohua and Liu, Yang and Gu, Zhaoquan},
	month = sep,
	year = {2023},
	pages = {345--357},
	file = {Wen et al. - 2023 - When Less is Enough Positive and Unlabeled Learni.pdf:/Users/akazad/Zotero/storage/8BGZYQJP/Wen et al. - 2023 - When Less is Enough Positive and Unlabeled Learni.pdf:application/pdf},
}

@inproceedings{zhang_learning_2023,
	address = {Luxembourg, Luxembourg},
	title = {Learning to {Locate} and {Describe} {Vulnerabilities}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	url = {https://ieeexplore.ieee.org/document/10298289/},
	doi = {10.1109/ASE56229.2023.00045},
	abstract = {Automatically discovering software vulnerabilities is a long-standing pursuit for software developers and security analysts. Since detection tools usually provide limited information for vulnerability inspection, recent work turns the attention to identify ﬁne-grained vulnerabilities, i.e., vulnerable statements. However, existing work for vulnerability localization struggles to capture long-range and integral dependency information due to the bottleneck of Graph Neural Networks (GNNs). Moreover, little research has been done to help developers understand detected vulnerabilities, leaving vulnerability diagnosis a challenging task. In this paper, we propose VulTeller, a deep learning-based approach that can automatically locate vulnerable statements in a function and more importantly, can describe the vulnerability. Our approach focuses on extracting precise control and data dependencies in the code, achieved through modeling control ﬂow paths and employing taint analysis. We design a novel neural model that encodes the control ﬂows and taint ﬂows which reside in the control ﬂow paths, and decodes them via node classiﬁcation and an attentional decoder for the two tasks respectively. We conduct extensive experiments with real-world vulnerabilities to evaluate the proposed approach. The evaluation results, including quantitative measurement and human evaluation, demonstrate that our approach is highly effective and outperforms state-ofthe-art approaches. Our work for the ﬁrst time formulates the problem of vulnerability description generation, and makes one step further towards automated vulnerability diagnosis.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Zhang, Jian and Liu, Shangqing and Wang, Xu and Li, Tianlin and Liu, Yang},
	month = sep,
	year = {2023},
	pages = {332--344},
	file = {Zhang et al. - 2023 - Learning to Locate and Describe Vulnerabilities.pdf:/Users/akazad/Zotero/storage/IMYCCMNE/Zhang et al. - 2023 - Learning to Locate and Describe Vulnerabilities.pdf:application/pdf},
}

@inproceedings{liang_needle_2023,
	address = {Luxembourg, Luxembourg},
	title = {A {Needle} is an {Outlier} in a {Haystack}: {Hunting} {Malicious} {PyPI} {Packages} with {Code} {Clustering}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {A {Needle} is an {Outlier} in a {Haystack}},
	url = {https://ieeexplore.ieee.org/document/10298315/},
	doi = {10.1109/ASE56229.2023.00085},
	abstract = {As the most popular Python software repository, PyPI has become an indispensable part of the Python ecosystem. Regrettably, the open nature of PyPI exposes end-users to substantial security risks stemming from malicious packages. Consequently, the timely and effective identiﬁcation of malware within the vast number of newly-uploaded PyPI packages has emerged as a pressing concern. Existing detection methods are dependent on difﬁcult-to-obtain explicit knowledge, such as taint sources, sinks, and malicious code patterns, rendering them susceptible to overlooking emergent malicious packages.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Liang, Wentao and Ling, Xiang and Wu, Jingzheng and Luo, Tianyue and Wu, Yanjun},
	month = sep,
	year = {2023},
	pages = {307--318},
	file = {Liang et al. - 2023 - A Needle is an Outlier in a Haystack Hunting Mali.pdf:/Users/akazad/Zotero/storage/TD69NDH6/getPDF.pdf:application/pdf},
}

@inproceedings{zhou_devil_2023,
	address = {Luxembourg, Luxembourg},
	title = {The {Devil} is in the {Tails}: {How} {Long}-{Tailed} {Code} {Distributions} {Impact} {Large} {Language} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {The {Devil} is in the {Tails}},
	url = {https://ieeexplore.ieee.org/document/10298393/},
	doi = {10.1109/ASE56229.2023.00157},
	abstract = {Learning-based techniques, especially advanced Large Language Models (LLMs) for code, have gained considerable popularity in various software engineering (SE) tasks. However, most existing works focus on designing better learningbased models and pay less attention to the properties of datasets. Learning-based models, including popular LLMs for code, heavily rely on data, and the data’s properties (e.g., data distribution) could significantly affect their behavior. We conducted an exploratory study on the distribution of SE data and found that such data usually follows a skewed distribution (i.e., long-tailed distribution) where a small number of classes have an extensive collection of samples, while a large number of classes have very few samples. We investigate three distinct SE tasks and analyze the impacts of long-tailed distribution on the performance of LLMs for code. Our experimental results reveal that the long-tailed distribution has a substantial impact on the effectiveness of LLMs for code. Specifically, LLMs for code perform between 30.0\% and 254.0\% worse on data samples associated with infrequent labels compared to data samples of frequent labels. Our study provides a better understanding of the effects of long-tailed distributions on popular LLMs for code and insights for the future development of SE automation.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Zhou, Xin and Kim, Kisub and Xu, Bowen and Liu, Jiakun and Han, DongGyun and Lo, David},
	month = sep,
	year = {2023},
	pages = {40--52},
	file = {Zhou et al. - 2023 - The Devil is in the Tails How Long-Tailed Code Di.pdf:/Users/akazad/Zotero/storage/HU9WSIKB/Zhou et al. - 2023 - The Devil is in the Tails How Long-Tailed Code Di.pdf:application/pdf},
}

@inproceedings{le_dilavrec_hyperast_2022,
	address = {Rochester MI USA},
	title = {{HyperAST}: {Enabling} {Efficient} {Analysis} of {Software} {Histories} at {Scale}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {{HyperAST}},
	url = {https://dl.acm.org/doi/10.1145/3551349.3560423},
	doi = {10.1145/3551349.3560423},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Le Dilavrec, Quentin and Khelladi, Djamel Eddine and Blouin, Arnaud and Jézéquel, Jean-Marc},
	month = oct,
	year = {2022},
	pages = {1--12},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/MFJCSBDL/Le Dilavrec et al. - 2022 - HyperAST Enabling Efficient Analysis of Software .pdf:application/pdf},
}

@inproceedings{luo_prcbert_2022,
	address = {Rochester MI USA},
	title = {{PRCBERT}: {Prompt} {Learning} for {Requirement} {Classification} using {BERT}-based {Pretrained} {Language} {Models}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {{PRCBERT}},
	url = {https://dl.acm.org/doi/10.1145/3551349.3560417},
	doi = {10.1145/3551349.3560417},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Luo, Xianchang and Xue, Yinxing and Xing, Zhenchang and Sun, Jiamou},
	month = oct,
	year = {2022},
	pages = {1--13},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/YBEBEEGR/Luo et al. - 2022 - PRCBERT Prompt Learning for Requirement Classific.pdf:application/pdf},
}

@article{pan_automating_nodate,
	title = {Automating {Developer} {Chat} {Mining}},
	abstract = {Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q\&A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classiﬁcation approach (named F2CHAT) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2CHAT effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Speciﬁcally, it has two stages with the ﬁrst one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57\%. Experiments also verify the effectiveness of our identiﬁed non-textual features under both intra-project and cross-project validations.},
	language = {en},
	author = {Pan, Shengyi and Ren, Xiaoxue and Xia, Xin and Lo, David and Li, Shanping},
	file = {Pan et al. - Automating Developer Chat Mining.pdf:/Users/akazad/Zotero/storage/9Q32E9IZ/Pan et al. - Automating Developer Chat Mining.pdf:application/pdf},
}

@inproceedings{bavishi_vizsmith_2021,
	address = {Melbourne, Australia},
	title = {{VizSmith}: {Automated} {Visualization} {Synthesis} by {Mining} {Data}-{Science} {Notebooks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-337-5},
	shorttitle = {{VizSmith}},
	url = {https://ieeexplore.ieee.org/document/9678696/},
	doi = {10.1109/ASE51524.2021.9678696},
	abstract = {Visualizations are widely used to communicate ﬁndings and make data-driven decisions. Unfortunately creating bespoke and reproducible visualizations requires the use of procedural tools such as matplotlib. These tools present a steep learning curve as their documentation often lacks sufﬁcient usage examples to help beginners get started or accomplish a speciﬁc task. Forums such as StackOverﬂow have long helped developers search for code online and adapt it for their use. However, developers still have to sift through search results and understand the code before adapting it for their use.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Bavishi, Rohan and Laddad, Shadaj and Yoshida, Hiroaki and Prasad, Mukul R. and Sen, Koushik},
	month = nov,
	year = {2021},
	pages = {129--141},
	file = {Bavishi et al. - 2021 - VizSmith Automated Visualization Synthesis by Min.pdf:/Users/akazad/Zotero/storage/PTPHCVFX/Bavishi et al. - 2021 - VizSmith Automated Visualization Synthesis by Min.pdf:application/pdf},
}

@inproceedings{li_understanding_2021,
	address = {Melbourne, Australia},
	title = {Understanding and {Detecting} {Performance} {Bugs} in {Markdown} {Compilers}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-337-5},
	url = {https://ieeexplore.ieee.org/document/9678611/},
	doi = {10.1109/ASE51524.2021.9678611},
	abstract = {Markdown compilers are widely used for translating plain Markdown text into formatted text, yet they suffer from performance bugs that cause performance degradation and resource exhaustion. Currently, there is little knowledge and understanding about these performance bugs in the wild. In this work, we first conduct a comprehensive study of known performance bugs in Markdown compilers. We identify that the ways Markdown compilers handle the language’s context-sensitive features are the dominant root cause of performance bugs. To detect unknown performance bugs, we develop MDPERFFUZZ, a fuzzing framework with a syntax-tree based mutation strategy to efficiently generate test cases to manifest such bugs. It equips an execution trace similarity algorithm to de-duplicate the bug reports. With MDPERFFUZZ, we successfully identified 216 new performance bugs in real-world Markdown compilers and applications. Our work demonstrates that the performance bugs are a common, severe, yet previously overlooked security problem.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Li, Penghui and Liu, Yinxi and Meng, Wei},
	month = nov,
	year = {2021},
	pages = {892--904},
	file = {Li et al. - 2021 - Understanding and Detecting Performance Bugs in Ma.pdf:/Users/akazad/Zotero/storage/SXC9YQM7/getPDF.pdf:application/pdf},
}

@inproceedings{pornprasit_pyexplainer_2021,
	address = {Melbourne, Australia},
	title = {{PyExplainer}: {Explaining} the {Predictions} of {Just}-{In}-{Time} {Defect} {Models}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-337-5},
	shorttitle = {{PyExplainer}},
	url = {https://ieeexplore.ieee.org/document/9678763/},
	doi = {10.1109/ASE51524.2021.9678763},
	abstract = {Just-In-Time (JIT) defect prediction (i.e., an AI/ML model to predict defect-introducing commits) is proposed to help developers prioritize their limited Software Quality Assurance (SQA) resources on the most risky commits. However, the explainability of JIT defect models remains largely unexplored (i.e., practitioners still do not know why a commit is predicted as defect-introducing). Recently, LIME has been used to generate explanations for any AI/ML models. However, the random perturbation approach used by LIME to generate synthetic neighbors is still suboptimal, i.e., generating synthetic neighbors that may not be similar to an instance to be explained, producing low accuracy of the local models, leading to inaccurate explanations for just-in-time defect models. In this paper, we propose PyExplainer—i.e., a local rule-based model-agnostic technique for generating explanations (i.e., why a commit is predicted as defective) of JIT defect models. Through a case study of two open-source software projects, we ﬁnd that our PyExplainer produces (1) synthetic neighbors that are 41\%-45\% more similar to an instance to be explained; (2) 18\%-38\% more accurate local models; and (3) explanations that are 69\%-98\% more unique and 17\%-54\% more consistent with the actual characteristics of defect-introducing commits in the future than LIME (a state-ofthe-art model-agnostic technique). This could help practitioners focus on the most important aspects of the commits to mitigate the risk of being defect-introducing. Thus, the contributions of this paper build an important step towards Explainable AI for Software Engineering, making software analytics more explainable and actionable. Finally, we publish our PyExplainer as a Python package to support practitioners and researchers (https://github.com/awsm-research/PyExplainer).},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Pornprasit, Chanathip and Tantithamthavorn, Chakkrit and Jiarpakdee, Jirayus and Fu, Michael and Thongtanunam, Patanamon},
	month = nov,
	year = {2021},
	pages = {407--418},
	file = {Pornprasit et al. - 2021 - PyExplainer Explaining the Predictions of Just-In.pdf:/Users/akazad/Zotero/storage/NF9NG2T8/Pornprasit et al. - 2021 - PyExplainer Explaining the Predictions of Just-In.pdf:application/pdf},
}

@misc{shi_ispy_2021,
	title = {{ISPY}: {Automatic} {Issue}-{Solution} {Pair} {Extraction} from {Community} {Live} {Chats}},
	shorttitle = {{ISPY}},
	url = {http://arxiv.org/abs/2109.07055},
	abstract = {Collaborative live chats are gaining popularity as a development communication tool. In community live chatting, developers are likely to post issues they encountered (e.g., setup issues and compile issues), and other developers respond with possible solutions. Therefore, community live chats contain rich sets of information for reported issues and their corresponding solutions, which can be quite useful for knowledge sharing and future reuse if extracted and restored in time. However, it remains challenging to accurately mine such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we ﬁrst formulate the problem of issue-solution pair extraction from developer live chat data, and propose an automated approach, named ISPY, based on natural language processing and deep learning techniques withDicaulostgo1mized enhancements, to address the problem. Speciﬁcally, ISPY automates three tasks: 1) Disentangle live chat logs, employing a feedforward neural network to disentangle a conversation history into separate dialogs automatically; 2) Detect dialogs discussing issues, using a novel convolutional neural network (CNN), which consists of a BERT-based utterance embedding layer, a context-aware dialog embedding layer, and an output layer; 3) Extract appropriate utterances and combine them as corresponding solutions, based on the same CNN structure but wDiitahlogdi2fferent feeding inputs. To evaluate ISPY, we compare it with six baselines, utilizing a dataset with 750 dialogs including 171 issue-solution pairs and evaluate ISPY from eight open source communities. The results show that, for issue-detection, our approach achieves the F1 of 76\%, and outperforms all baselines by 30\%. Our approach achieves the F1 of 63\% for solution-extraction and outperforms the baselines by 20\%. Furthermore, we apply ISPY on three new communities to extensively evaluate ISPY’s practical usage. Moreover, we publish over 30K issue-solution pairs extracted from 11 communities. We believe that ISPY can facilitate community-based software development by promoting knowledge sharing and shortening the issue-resolving process.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Shi, Lin and Jiang, Ziyou and Yang, Ye and Chen, Xiao and Zhang, Yumin and Mu, Fangwen and Jiang, Hanzhi and Wang, Qing},
	month = sep,
	year = {2021},
	note = {arXiv:2109.07055 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Shi et al. - 2021 - ISPY Automatic Issue-Solution Pair Extraction fro.pdf:/Users/akazad/Zotero/storage/FBCZ4INZ/Shi et al. - 2021 - ISPY Automatic Issue-Solution Pair Extraction fro.pdf:application/pdf},
}

@inproceedings{uddin_mining_2021,
	address = {Melbourne, Australia},
	title = {Mining {Cross}-{Domain} {Apps} for {Software} {Evolution}: {A} {Feature}-based {Approach}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-337-5},
	shorttitle = {Mining {Cross}-{Domain} {Apps} for {Software} {Evolution}},
	url = {https://ieeexplore.ieee.org/document/9678514/},
	doi = {10.1109/ASE51524.2021.9678514},
	abstract = {The skyrocketing growth of mobile apps and mobile devices has signiﬁcantly fueled the competition among app developers. They have leveraged the app store capabilities to analyse app data and identify app improvement opportunities. Existing research has shown that app developers mostly rely on in-domain (i.e., same domain or same app) data to improve their apps. However, relying on in-domain data results in low diversity and lacks novelty in recommended features. In this work, we present an approach that automatically identiﬁes, classiﬁes and ranks relevant popular features from cross-domain apps for recommendation to any given target app. It includes the following three steps: 1) identify cross-domain apps that are relevant to the target app in terms of their features; 2) ﬁlter and group semantically the features of the relevant crossdomain apps that are complementary to the target app; 3) rank and prioritize the complementary cross-domain features (in terms of their domain, app, feature and popularity characteristics) for adoption by the target app’s developers. We have run extensive experiments on 100 target apps from 10 categories over 15,200 cross-domain apps from 31 categories. The experimental results have shown that our approach to identifying, grouping and ranking complementary cross-domain features for recommendation has achieved an accuracy level of over 89\%. Our semantic feature grouping technique has also signiﬁcantly outperformed two existing baseline techniques. The empirical evaluation validates the efﬁcacy of our approach in providing personalised feature recommendation and enhancing app’s user serendipity.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Uddin, Md Kafil and He, Qiang and Han, Jun and Chua, Caslon},
	month = nov,
	year = {2021},
	pages = {743--755},
	file = {Uddin et al. - 2021 - Mining Cross-Domain Apps for Software Evolution A.pdf:/Users/akazad/Zotero/storage/G7QN2EVA/Uddin et al. - 2021 - Mining Cross-Domain Apps for Software Evolution A.pdf:application/pdf},
}

@inproceedings{zhou_finding_2021,
	address = {Melbourne, Australia},
	title = {Finding {A} {Needle} in a {Haystack}: {Automated} {Mining} of {Silent} {Vulnerability} {Fixes}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66540-337-5},
	shorttitle = {Finding {A} {Needle} in a {Haystack}},
	url = {https://ieeexplore.ieee.org/document/9678720/},
	doi = {10.1109/ASE51524.2021.9678720},
	abstract = {Following the coordinated vulnerability disclosure model, a vulnerability in open source software (OSS) is suggested to be ﬁxed “silently”, without disclosing the ﬁx until the vulnerability is disclosed. Yet, it is crucial for OSS users to be aware of vulnerability ﬁxes as early as possible, as once a vulnerability ﬁx is pushed to the source code repository, a malicious party could probe for the corresponding vulnerability to exploit it. In practice, OSS users often rely on the vulnerability disclosure information from security advisories (e.g., National Vulnerability Database) to sense vulnerability ﬁxes. However, the time between the availability of a vulnerability ﬁx and its disclosure can vary from days to months, and in some cases, even years. Due to manpower constraints and the lack of expert knowledge, it is infeasible for OSS users to manually analyze all code changes for vulnerability ﬁx detection. Therefore, it is essential to identify vulnerability ﬁxes automatically and promptly. In a ﬁrst-of-its-kind study, we propose VulFixMiner, a Transformer-based approach, capable of automatically extracting semantic meaning from commit-level code changes to identify silent vulnerability ﬁxes. We construct our model using sampled commits from 204 projects, and evaluate using the full set of commits from 52 additional projects. The evaluation results show that VulFixMiner outperforms various state-of-the-art baselines in terms of AUC (i.e., 0.81 and 0.73 on Java and Python dataset, respectively) and two effort-aware performance metrics (i.e., EffortCost, Popt). Especially, with an effort of inspecting 5\% of total LOC, VulFixMiner can identify 49\% of total vulnerability ﬁxes. Additionally, with manual veriﬁcation of sampled commits that were identiﬁed as vulnerability ﬁxes, but not marked as such in our dataset, we observe that 35\% (29 out of 82) of the commits are for ﬁxing vulnerabilities, indicating VulFixMiner is also capable of identifying unreported vulnerability ﬁxes.},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Zhou, Jiayuan and Pacheco, Michael and Wan, Zhiyuan and Xia, Xin and Lo, David and Wang, Yuan and Hassan, Ahmed E.},
	month = nov,
	year = {2021},
	pages = {705--716},
	file = {Zhou et al. - 2021 - Finding A Needle in a Haystack Automated Mining o.pdf:/Users/akazad/Zotero/storage/WSQJGGYY/Zhou et al. - 2021 - Finding A Needle in a Haystack Automated Mining o.pdf:application/pdf},
}

@misc{jiang_lilac_2024,
	title = {{LILAC}: {Log} {Parsing} using {LLMs} with {Adaptive} {Parsing} {Cache}},
	shorttitle = {{LILAC}},
	url = {http://arxiv.org/abs/2310.01796},
	abstract = {Log parsing transforms log messages into structured formats, serving as the prerequisite step for various log analysis tasks. Although a variety of log parsing approaches have been proposed, their performance on complicated log data remains compromised due to the use of human-crafted rules or learning-based models with limited training data. The recent emergence of powerful large language models (LLMs) demonstrates their vast pre-trained knowledge related to code and logging, making it promising to apply LLMs for log parsing. However, their lack of specialized log parsing capabilities currently hinders their parsing accuracy. Moreover, the inherent inconsistent answers, as well as the substantial overhead, prevent the practical adoption of LLM-based log parsing. To address these challenges, we propose LILAC, the first practical Log parsIng framework using LLMs with Adaptive parsing Cache. To facilitate accurate and robust log parsing, LILAC leverages the in-context learning (ICL) capability of the LLM by performing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations. Furthermore, LILAC incorporates a novel component, an adaptive parsing cache, to store and refine the templates generated by the LLM. It helps mitigate LLM’s inefficiency issue by enabling rapid retrieval of previously processed log templates. In this process, LILAC adaptively updates the templates within the parsing cache to ensure the consistency of parsed results. The extensive evaluation on public large-scale datasets shows that LILAC outperforms state-of-the-art methods by 69.5\% in terms of the average F1 score of template accuracy. In addition, LILAC reduces the query times to LLMs by several orders of magnitude, achieving a comparable efficiency to the fastest baseline. CCS Concepts: • Software and its engineering → Software creation and management.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Jiang, Zhihan and Liu, Jinyang and Chen, Zhuangbin and Li, Yichen and Huang, Junjie and Huo, Yintong and He, Pinjia and Gu, Jiazhen and Lyu, Michael R.},
	month = mar,
	year = {2024},
	note = {arXiv:2310.01796 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Jiang et al. - 2024 - LILAC Log Parsing using LLMs with Adaptive Parsin.pdf:/Users/akazad/Zotero/storage/WRBINBFM/Jiang et al. - 2024 - LILAC Log Parsing using LLMs with Adaptive Parsin.pdf:application/pdf},
}

@inproceedings{lin_cct5_2023-1,
	address = {San Francisco CA USA},
	title = {{CCT5}: {A} {Code}-{Change}-{Oriented} {Pre}-trained {Model}},
	isbn = {9798400703270},
	shorttitle = {{CCT5}},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616339},
	doi = {10.1145/3611643.3616339},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Lin, Bo and Wang, Shangwen and Liu, Zhongxin and Liu, Yepang and Xia, Xin and Mao, Xiaoguang},
	month = nov,
	year = {2023},
	pages = {1509--1521},
	file = {Lin et al. - 2023 - CCT5 A Code-Change-Oriented Pre-trained Model.pdf:/Users/akazad/Zotero/storage/QTPD8BM9/Lin et al. - 2023 - CCT5 A Code-Change-Oriented Pre-trained Model.pdf:application/pdf},
}

@inproceedings{yang_understanding_2023,
	address = {San Francisco CA USA},
	title = {Understanding the {Topics} and {Challenges} of {GPU} {Programming} by {Classifying} and {Analyzing} {Stack} {Overflow} {Posts}},
	isbn = {9798400703270},
	url = {https://dl.acm.org/doi/10.1145/3611643.3616365},
	doi = {10.1145/3611643.3616365},
	language = {en},
	urldate = {2024-04-27},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Yang, Wenhua and Zhang, Chong and Pan, Minxue},
	month = nov,
	year = {2023},
	pages = {1444--1456},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/NL89Q8AH/Yang et al. - 2023 - Understanding the Topics and Challenges of GPU Pro.pdf:application/pdf},
}

@misc{macedo_exploring_2024,
	title = {Exploring the {Impact} of the {Output} {Format} on the {Evaluation} of {Large} {Language} {Models} for {Code} {Translation}},
	url = {http://arxiv.org/abs/2403.17214},
	doi = {10.1145/3650105.3652301},
	abstract = {Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4\% and 73.7\% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with executionbased metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73\%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.},
	language = {en},
	urldate = {2024-04-27},
	author = {Macedo, Marcos and Tian, Yuan and Cogo, Filipe R. and Adams, Bram},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17214 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Artificial Intelligence},
	file = {Macedo et al. - 2024 - Exploring the Impact of the Output Format on the E.pdf:/Users/akazad/Zotero/storage/JSAM2452/Macedo et al. - 2024 - Exploring the Impact of the Output Format on the E.pdf:application/pdf},
}

@inproceedings{ma_llmparser_2024-1,
	address = {Lisbon Portugal},
	title = {{LLMParser}: {An} {Exploratory} {Study} on {Using} {Large} {Language} {Models} for {Log} {Parsing}},
	isbn = {9798400702174},
	shorttitle = {{LLMParser}},
	url = {https://dl.acm.org/doi/10.1145/3597503.3639150},
	doi = {10.1145/3597503.3639150},
	language = {en},
	urldate = {2024-04-30},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
	month = apr,
	year = {2024},
	pages = {1--13},
	file = {Ma et al. - 2024 - LLMParser An Exploratory Study on Using Large Lan.pdf:/Users/akazad/Zotero/storage/M3ZRA85L/Ma et al. - 2024 - LLMParser An Exploratory Study on Using Large Lan.pdf:application/pdf},
}

@misc{wang_delving_2023,
	title = {Delving into {Commit}-{Issue} {Correlation} to {Enhance} {Commit} {Message} {Generation} {Models}},
	url = {http://arxiv.org/abs/2308.00147},
	abstract = {Commit message generation (CMG) is a challenging task in automated software engineering that aims to generate natural language descriptions of code changes for commits. Previous methods all start from the modified code snippets, outputting commit messages through template-based, retrieval-based, or learning-based models. While these methods can summarize what is modified from the perspective of code, they struggle to provide reasons for the commit. The correlation between commits and issues that could be a critical factor for generating rational commit messages is still unexplored.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Wang, Liran and Tang, Xunzhu and He, Yichen and Ren, Changyu and Shi, Shuhua and Yan, Chaoran and Li, Zhoujun},
	month = sep,
	year = {2023},
	note = {arXiv:2308.00147 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Wang et al. - 2023 - Delving into Commit-Issue Correlation to Enhance C.pdf:/Users/akazad/Zotero/storage/5YKLH7DL/Wang et al. - 2023 - Delving into Commit-Issue Correlation to Enhance C.pdf:application/pdf},
}

@inproceedings{gao_what_2023,
	title = {What {Makes} {Good} {In}-context {Demonstrations} for {Code} {Intelligence} {Tasks} with {LLMs}?},
	url = {http://arxiv.org/abs/2304.07575},
	doi = {10.1109/ASE56229.2023.00109},
	abstract = {Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90\%, 175.96\%, and 50.81\% on code summarization, bug fixing, and program synthesis, respectively.},
	language = {en},
	urldate = {2024-04-30},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Zhang, Hongyu and Lyu, Michael R.},
	month = sep,
	year = {2023},
	note = {arXiv:2304.07575 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {761--773},
	file = {Gao et al. - 2023 - What Makes Good In-context Demonstrations for Code.pdf:/Users/akazad/Zotero/storage/RPKI3C42/Gao et al. - 2023 - What Makes Good In-context Demonstrations for Code.pdf:application/pdf},
}

@misc{li_zc3_2023,
	title = {{ZC3}: {Zero}-{Shot} {Cross}-{Language} {Code} {Clone} {Detection}},
	shorttitle = {{ZC3}},
	url = {http://arxiv.org/abs/2308.13754},
	abstract = {Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate representations that are aligned among different languages meanwhile are diacritical for different types of clones. To evaluate our approach, we conduct extensive experiments on four representative cross-language clone detection datasets. Experimental results show that ZC3 outperforms the stateof-the-art baselines by 67.12\%, 51.39\%, 14.85\%, and 53.01\% on the MAP score, respectively. We further investigate the representational distribution of different languages and discuss the effectiveness of our method.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Li, Jia and Tao, Chongyang and Jin, Zhi and Liu, Fang and Li, Jia and Li, Ge},
	month = sep,
	year = {2023},
	note = {arXiv:2308.13754 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Li et al. - 2023 - ZC3 Zero-Shot Cross-Language Code Clone Detection.pdf:/Users/akazad/Zotero/storage/WHBP4K5Q/Li et al. - 2023 - ZC3 Zero-Shot Cross-Language Code Clone Detection.pdf:application/pdf},
}

@misc{chen_diversevul_2023,
	title = {{DiverseVul}: {A} {New} {Vulnerable} {Source} {Code} {Dataset} for {Deep} {Learning} {Based} {Vulnerability} {Detection}},
	shorttitle = {{DiverseVul}},
	url = {http://arxiv.org/abs/2304.00409},
	abstract = {We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Chen, Yizheng and Ding, Zhoujie and Alowain, Lamya and Chen, Xinyun and Wagner, David},
	month = aug,
	year = {2023},
	note = {arXiv:2304.00409 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Chen et al. - 2023 - DiverseVul A New Vulnerable Source Code Dataset f.pdf:/Users/akazad/Zotero/storage/EC6NMRVB/Chen et al. - 2023 - DiverseVul A New Vulnerable Source Code Dataset f.pdf:application/pdf},
}

@misc{zhou_multi-llm_2024,
	title = {Multi-{LLM} {Collaboration} + {Data}-{Centric} {Innovation} = 2x {Better} {Vulnerability} {Repair}},
	url = {http://arxiv.org/abs/2401.15459},
	abstract = {The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Zhou, Xin and Kim, Kisub and Xu, Bowen and Han, DongGyun and Lo, David},
	month = mar,
	year = {2024},
	note = {arXiv:2401.15459 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Zhou et al. - 2024 - Multi-LLM Collaboration + Data-Centric Innovation .pdf:/Users/akazad/Zotero/storage/W7XNHA5C/2401.pdf:application/pdf},
}

@misc{he_instruction_2024,
	title = {Instruction {Tuning} for {Secure} {Code} {Generation}},
	url = {http://arxiv.org/abs/2402.09497},
	abstract = {Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs’ practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instructiontuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30\%), while preserving utility.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {He, Jingxuan and Vero, Mark and Krasnopolska, Gabriela and Vechev, Martin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09497 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {He et al. - 2024 - Instruction Tuning for Secure Code Generation.pdf:/Users/akazad/Zotero/storage/WCUBRZBU/2402.pdf:application/pdf},
}

@misc{gandhi_better_2024,
	title = {Better {Synthetic} {Data} by {Retrieving} and {Transforming} {Existing} {Datasets}},
	url = {http://arxiv.org/abs/2404.14361},
	abstract = {Despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, we introduce a method, DataTune, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, we find that finetuning language models via DataTune improves over a few-shot prompting baseline by 49\% and improves over existing methods that use synthetic or retrieved training data by 34\%. We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. We integrate DataTune into an open-source repository to make this method accessible to the community: https://github.com/neulab/prompt2model.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Gandhi, Saumya and Gala, Ritu and Viswanathan, Vijay and Wu, Tongshuang and Neubig, Graham},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14361 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Gandhi et al. - 2024 - Better Synthetic Data by Retrieving and Transformi.pdf:/Users/akazad/Zotero/storage/LGJKDHX7/Gandhi et al. - 2024 - Better Synthetic Data by Retrieving and Transformi.pdf:application/pdf},
}

@misc{nazzal_dataset_2024,
	title = {A {Dataset} for {Large} {Language} {Model}-{Driven} {AI} {Accelerator} {Generation}},
	url = {http://arxiv.org/abs/2404.10875},
	abstract = {In the ever-evolving landscape of Deep Neural Networks (DNN) hardware acceleration, unlocking the true potential of systolic array accelerators has long been hindered by the daunting challenges of expertise and time investment. Large Language Models (LLMs) offer a promising solution for automating code generation which is key to unlocking unprecedented efficiency and performance in various domains, including hardware descriptive code. However, the successful application of LLMs to hardware accelerator design is contingent upon the availability of specialized datasets tailored for this purpose. To bridge this gap, we introduce the Systolic Array-based Accelerator DataSet (SA-DS). SA-DS comprises of a diverse collection of spatial arrays following the standardized Berkeley's Gemmini accelerator generator template, enabling design reuse, adaptation, and customization. SA-DS is intended to spark LLM-centred research on DNN hardware accelerator architecture. We envision that SA-DS provides a framework which will shape the course of DNN hardware acceleration research for generations to come. SA-DS is open-sourced under the permissive MIT license at this https://github.com/ACADLab/SA-DS.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Nazzal, Mahmoud and Vungarala, Deepak and Morsali, Mehrdad and Zhang, Chao and Ghosh, Arnob and Khreishah, Abdallah and Angizi, Shaahin},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10875 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	file = {Nazzal et al. - 2024 - A Dataset for Large Language Model-Driven AI Accel.pdf:/Users/akazad/Zotero/storage/ZDRWM37C/Nazzal et al. - 2024 - A Dataset for Large Language Model-Driven AI Accel.pdf:application/pdf},
}

@inproceedings{stradowski_bridging_2023,
	address = {Luxembourg, Luxembourg},
	title = {Bridging the {Gap} {Between} {Academia} and {Industry} in {Machine} {Learning} {Software} {Defect} {Prediction}: {Thirteen} {Considerations}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350329964},
	shorttitle = {Bridging the {Gap} {Between} {Academia} and {Industry} in {Machine} {Learning} {Software} {Defect} {Prediction}},
	url = {https://ieeexplore.ieee.org/document/10298304/},
	doi = {10.1109/ASE56229.2023.00026},
	abstract = {This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world — Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Stradowski, Szymon and Madeyski, Lech},
	month = sep,
	year = {2023},
	pages = {1098--1110},
	file = {Stradowski and Madeyski - 2023 - Bridging the Gap Between Academia and Industry in .pdf:/Users/akazad/Zotero/storage/I7DZLJ6W/Stradowski and Madeyski - 2023 - Bridging the Gap Between Academia and Industry in .pdf:application/pdf},
}

@inproceedings{liu_searching_2018,
	address = {Beijing China},
	title = {Searching {StackOverflow} {Questions} with {Multi}-{Faceted} {Categorization}},
	copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
	isbn = {978-1-4503-6590-1},
	url = {https://dl.acm.org/doi/10.1145/3275219.3275227},
	doi = {10.1145/3275219.3275227},
	abstract = {StackOverflow provides answers for a huge number of software development questions that are frequently encountered by developers. However, searching relevant questions in StackOverflow is not always easy using the keyword based search engine provided by StackOverflow. A software development question can be characterized by multiple attributes, such as, its concern (e.g.,configuration problem, error handling, sample code, etc.), programming language, operating system, and involved middleware, framework, library and software technology. We propose a multi-faceted and interactive approach for searching StackOverflow questions (called MFISSO), which leverages these attributes of the questions. Our approach starts with an initial keyword-based query and extracts a multifaceted categorization from all the candidate questions using natural language processing and data mining. It then allows developers to iteratively refine the search results through an interactive process. We evaluated an implementation of MFISSO in a controlled experiments with 20 computing students, solving ten software development tasks using StackOverflow. The experiment shows that MFISSO can help developers find relevant questions faster and with higher accuracy.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the {Tenth} {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {ACM},
	author = {Liu, Mingwei and Peng, Xin and Jiang, Qingtao and Marcus, Andrian and Yang, Junwen and Zhao, Wenyun},
	month = sep,
	year = {2018},
	pages = {1--10},
	file = {Liu et al. - 2018 - Searching StackOverflow Questions with Multi-Facet.pdf:/Users/akazad/Zotero/storage/65QPWHGX/Internetware18SO.pdf:application/pdf},
}

@misc{fagbohun_empirical_2024,
	title = {An {Empirical} {Categorization} of {Prompting} {Techniques} for {Large} {Language} {Models}: {A} {Practitioner}'s {Guide}},
	shorttitle = {An {Empirical} {Categorization} of {Prompting} {Techniques} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.14837},
	abstract = {Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniques tailored to their specific domains. We believe that this approach will help simplify the complex landscape of prompt engineering and enable more effective utilization of LLMs in various applications. By providing practitioners with a systematic approach to prompt categorization, we aim to assist in navigating the intricacies of effective prompt design for conversational pre-trained LLMs and inspire new possibilities in their respective fields.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Fagbohun, Oluwole and Harrison, Rachel M. and Dereventsov, Anton},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14837 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Fagbohun et al. - 2024 - An Empirical Categorization of Prompting Technique.pdf:/Users/akazad/Zotero/storage/FLZKCI8M/Fagbohun et al. - 2024 - An Empirical Categorization of Prompting Technique.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:/Users/akazad/Zotero/storage/EPNRVFXK/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{zhang_teleclass_2024,
	title = {{TELEClass}: {Taxonomy} {Enrichment} and {LLM}-{Enhanced} {Hierarchical} {Text} {Classification} with {Minimal} {Supervision}},
	shorttitle = {{TELEClass}},
	url = {http://arxiv.org/abs/2403.00165},
	abstract = {Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which (1) automatically enriches the label taxonomy with class-indicative topical terms mined from the corpus to facilitate classifier training and (2) utilizes LLMs for both data annotation and creation tailored for the hierarchical label space. Experiments show that TELEClass can outperform previous weakly-supervised hierarchical text classification methods and LLM-based zero-shot prompting methods on two public datasets.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Zhang, Yunyi and Yang, Ruozhen and Xu, Xueqiang and Xiao, Jinfeng and Shen, Jiaming and Han, Jiawei},
	month = feb,
	year = {2024},
	note = {arXiv:2403.00165 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Zhang et al. - 2024 - TELEClass Taxonomy Enrichment and LLM-Enhanced Hi.pdf:/Users/akazad/Zotero/storage/IP8IC3YZ/Zhang et al. - 2024 - TELEClass Taxonomy Enrichment and LLM-Enhanced Hi.pdf:application/pdf},
}

@inproceedings{fan_chain--thought_2023,
	address = {Singapore},
	title = {Chain-of-{Thought} {Tuning}: {Masked} {Language} {Models} can also {Think} {Step} {By} {Step} in {Natural} {Language} {Understanding}},
	shorttitle = {Chain-of-{Thought} {Tuning}},
	url = {https://aclanthology.org/2023.emnlp-main.913},
	doi = {10.18653/v1/2023.emnlp-main.913},
	abstract = {Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Fan, Caoyun and Tian, Jidong and Li, Yitian and Chen, Wenqing and He, Hao and Jin, Yaohui},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {classifiaction},
	pages = {14774--14785},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/IVVTA2PH/Fan et al. - 2023 - Chain-of-Thought Tuning Masked Language Models ca.pdf:application/pdf},
}

@misc{dai_auggpt_2023,
	title = {{AugGPT}: {Leveraging} {ChatGPT} for {Text} {Data} {Augmentation}},
	shorttitle = {{AugGPT}},
	url = {http://arxiv.org/abs/2302.13007},
	abstract = {Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can’t ensure the correct labeling of the generated data (lacking faithfulness) or can’t ensure sufﬁcient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classiﬁcation tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and Xu, Shaochen and Liu, Wei and Liu, Ninghao and Li, Sheng and Zhu, Dajiang and Cai, Hongmin and Sun, Lichao and Li, Quanzheng and Shen, Dinggang and Liu, Tianming and Li, Xiang},
	month = mar,
	year = {2023},
	note = {arXiv:2302.13007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Dai et al. - 2023 - AugGPT Leveraging ChatGPT for Text Data Augmentat.pdf:/Users/akazad/Zotero/storage/NZ5LHPTA/Dai et al. - 2023 - AugGPT Leveraging ChatGPT for Text Data Augmentat.pdf:application/pdf},
}

@misc{hu_llmvssmall_2024,
	title = {{LLMvsSmall} {Model}? {Large} {Language} {Model} {Based} {Text} {Augmentation} {Enhanced} {Personality} {Detection} {Model}},
	shorttitle = {{LLMvsSmall} {Model}?},
	url = {http://arxiv.org/abs/2403.07581},
	abstract = {Personality detection aims to detect one’s personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM’s knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Hu, Linmei and He, Hongyu and Wang, Duokang and Zhao, Ziwang and Shao, Yingxia and Nie, Liqiang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.07581 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Hu et al. - 2024 - LLMvsSmall Model Large Language Model Based Text .pdf:/Users/akazad/Zotero/storage/4WN2QH8E/Hu et al. - 2024 - LLMvsSmall Model Large Language Model Based Text .pdf:application/pdf},
}

@misc{xu_survey_2024,
	title = {A {Survey} on {Knowledge} {Distillation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.13116},
	abstract = {In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their selfimprovement by employing themselves as teachers. This paper presents a comprehensive survey of KD’s role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and selfimprovement. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization – providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs’ performance. By leveraging DA to generate contextrich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
	month = mar,
	year = {2024},
	note = {arXiv:2402.13116 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Xu et al. - 2024 - A Survey on Knowledge Distillation of Large Langua.pdf:/Users/akazad/Zotero/storage/276PC75R/2402.pdf:application/pdf},
}

@misc{tian_tinyllm_2024,
	title = {{TinyLLM}: {Learning} a {Small} {Student} from {Multiple} {Large} {Language} {Models}},
	shorttitle = {{TinyLLM}},
	url = {http://arxiv.org/abs/2402.04616},
	abstract = {Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TINYLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TINYLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Tian, Yijun and Han, Yikun and Chen, Xiusi and Wang, Wei and Chawla, Nitesh V.},
	month = mar,
	year = {2024},
	note = {arXiv:2402.04616 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Tian et al. - 2024 - TinyLLM Learning a Small Student from Multiple La.pdf:/Users/akazad/Zotero/storage/DTSHHGMF/Tian et al. - 2024 - TinyLLM Learning a Small Student from Multiple La.pdf:application/pdf},
}

@misc{ding_is_2023,
	title = {Is {GPT}-3 a {Good} {Data} {Annotator}?},
	url = {http://arxiv.org/abs/2212.10450},
	abstract = {Data annotation is the process of labeling data that could be used to train machine learning models. Having high-quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP 1.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Joty, Shafiq and Li, Boyang and Bing, Lidong},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10450 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Ding et al. - 2023 - Is GPT-3 a Good Data Annotator.pdf:/Users/akazad/Zotero/storage/L44R4BVQ/Ding et al. - 2023 - Is GPT-3 a Good Data Annotator.pdf:application/pdf},
}

@misc{hsieh_distilling_2023,
	title = {Distilling {Step}-by-{Step}! {Outperforming} {Larger} {Language} {Models} with {Less} {Training} {Data} and {Smaller} {Model} {Sizes}},
	url = {http://arxiv.org/abs/2305.02301},
	abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
	month = jul,
	year = {2023},
	note = {arXiv:2305.02301 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Lang.pdf:/Users/akazad/Zotero/storage/2A9LAEDT/Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Lang.pdf:application/pdf},
}

@misc{he_annollm_2024,
	title = {{AnnoLLM}: {Making} {Large} {Language} {Models} to {Be} {Better} {Crowdsourced} {Annotators}},
	shorttitle = {{AnnoLLM}},
	url = {http://arxiv.org/abs/2303.16854},
	abstract = {Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chainof-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversationbased information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset’s high quality.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	month = apr,
	year = {2024},
	note = {arXiv:2303.16854 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {He et al. - 2024 - AnnoLLM Making Large Language Models to Be Better.pdf:/Users/akazad/Zotero/storage/CFIRLK6Y/He et al. - 2024 - AnnoLLM Making Large Language Models to Be Better.pdf:application/pdf},
}

@inproceedings{wang_want_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Want {To} {Reduce} {Labeling} {Cost}? {GPT}-3 {Can} {Help}},
	shorttitle = {Want {To} {Reduce} {Labeling} {Cost}?},
	url = {https://aclanthology.org/2021.findings-emnlp.354},
	doi = {10.18653/v1/2021.findings-emnlp.354},
	abstract = {Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50\% to 96\% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.},
	urldate = {2024-05-03},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4195--4205},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/BBB9CFCD/Wang et al. - 2021 - Want To Reduce Labeling Cost GPT-3 Can Help.pdf:application/pdf},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} {Outperforms} {Crowd}-{Workers} for {Text}-{Annotation} {Tasks}},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2303.15056},
	doi = {10.1073/pnas.2305016120},
	abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.},
	language = {en},
	number = {30},
	urldate = {2024-05-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	month = jul,
	year = {2023},
	note = {arXiv:2303.15056 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	pages = {e2305016120},
	file = {Gilardi et al. - 2023 - ChatGPT Outperforms Crowd-Workers for Text-Annotat.pdf:/Users/akazad/Zotero/storage/CBJNFVGL/Gilardi et al. - 2023 - ChatGPT Outperforms Crowd-Workers for Text-Annotat.pdf:application/pdf},
}

@misc{wei_chain--thought_2023-1,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:/Users/akazad/Zotero/storage/Y4M7H8PL/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{zhou_large_2024,
	title = {Large {Language} {Model} for {Vulnerability} {Detection}: {Emerging} {Results} and {Future} {Directions}},
	shorttitle = {Large {Language} {Model} for {Vulnerability} {Detection}},
	url = {http://arxiv.org/abs/2401.15468},
	abstract = {Previous learning-based vulnerability detection methods relied on either medium-sized pre-trained models or smaller neural networks from scratch. Recent advancements in Large Pre-Trained Language Models (LLMs) have showcased remarkable few-shot learning capabilities in various tasks. However, the eﬀectiveness of LLMs in detecting software vulnerabilities is largely unexplored. This paper aims to bridge this gap by exploring how LLMs perform with various prompts, particularly focusing on two state-of-the-art LLMs: GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves competitive performance with the prior state-of-the-art vulnerability detection approach and GPT-4 consistently outperformed the state-of-the-art.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Zhou, Xin and Zhang, Ting and Lo, David},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15468 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Zhou et al. - 2024 - Large Language Model for Vulnerability Detection .pdf:/Users/akazad/Zotero/storage/4Y247IPV/Zhou et al. - 2024 - Large Language Model for Vulnerability Detection .pdf:application/pdf},
}

@misc{chen_compcodevet_2023,
	title = {{CompCodeVet}: {A} {Compiler}-guided {Validation} and {Enhancement} {Approach} for {Code} {Dataset}},
	shorttitle = {{CompCodeVet}},
	url = {http://arxiv.org/abs/2311.06505},
	abstract = {Large language models (LLMs) have become increasingly prominent in academia and industry due to their remarkable performance in diverse applications. As these models evolve with increasing parameters, they excel in tasks like sentiment analysis and machine translation. However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. While LLMs trained on code datasets demonstrate competence in many tasks, they struggle with rectifying non-compilable C and C++ code. Our investigation attributes this subpar performance to two primary factors: the quality of the training dataset and the inherent complexity of the problem, which demands intricate reasoning. Existing ”Chain of Thought” (CoT) prompting techniques aim to enhance multi-step reasoning. This approach, however, retains the limitations associated with the latent drawbacks of LLMs. In this work, we propose CompCodeVet, a compiler-guided CoT approach to produce compilable code from non-compilable ones. Diverging from the conventional approach of utilizing larger LLMs, we employ compilers as a teacher to establish a more robust zero-shot thought process. The evaluation of CompCodeVet on two open-source code datasets shows that CompCodeVet can improve the training dataset quality for LLMs.},
	language = {en},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Chen, Le and Bhattacharjee, Arijit and Ahmed, Nesreen K. and Hasabnis, Niranjan and Oren, Gal and Lei, Bin and Jannesari, Ali},
	month = nov,
	year = {2023},
	note = {arXiv:2311.06505 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Chen et al. - 2023 - CompCodeVet A Compiler-guided Validation and Enha.pdf:/Users/akazad/Zotero/storage/VQK6JYKK/Chen et al. - 2023 - CompCodeVet A Compiler-guided Validation and Enha.pdf:application/pdf},
}

@misc{zhao_lora_2024,
	title = {{LoRA} {Land}: 310 {Fine}-tuned {LLMs} that {Rival} {GPT}-4, {A} {Technical} {Report}},
	shorttitle = {{LoRA} {Land}},
	url = {http://arxiv.org/abs/2405.00732},
	abstract = {Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.},
	language = {en},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
	month = apr,
	year = {2024},
	note = {arXiv:2405.00732 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A.pdf:/Users/akazad/Zotero/storage/HD3DNJW9/Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A.pdf:application/pdf},
}

@misc{bertsch_-context_2024,
	title = {In-{Context} {Learning} with {Long}-{Context} {Models}: {An} {In}-{Depth} {Exploration}},
	shorttitle = {In-{Context} {Learning} with {Long}-{Context} {Models}},
	url = {http://arxiv.org/abs/2405.00200},
	abstract = {As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. We use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts we see do not arise from cumulative gain from encoding many examples together. We conclude that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning.},
	language = {en},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R. and Neubig, Graham},
	month = apr,
	year = {2024},
	note = {arXiv:2405.00200 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Bertsch et al. - 2024 - In-Context Learning with Long-Context Models An I.pdf:/Users/akazad/Zotero/storage/BP4TCUTI/Bertsch et al. - 2024 - In-Context Learning with Long-Context Models An I.pdf:application/pdf},
}

@inproceedings{zaman_security_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Security versus performance bugs: a case study on {Firefox}},
	isbn = {978-1-4503-0574-7},
	shorttitle = {Security versus performance bugs},
	url = {https://dl.acm.org/doi/10.1145/1985441.1985457},
	doi = {10.1145/1985441.1985457},
	language = {en},
	urldate = {2024-05-05},
	booktitle = {Proceedings of the 8th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Zaman, Shahed and Adams, Bram and Hassan, Ahmed E.},
	month = may,
	year = {2011},
	pages = {93--102},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/GKA9MR4R/Zaman et al. - 2011 - Security versus performance bugs a case study on .pdf:application/pdf},
}

@inproceedings{kalam_azad_empirical_2023,
	address = {Melbourne, Australia},
	title = {An {Empirical} {Study} of {High} {Performance} {Computing} ({HPC}) {Performance} {Bugs}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350311846},
	url = {https://ieeexplore.ieee.org/document/10174003/},
	doi = {10.1109/MSR59073.2023.00037},
	abstract = {Performance efficiency and scalability are the major design goals for high performance computing (HPC) applications. However, it is challenging to achieve high efficiency and scalability for such applications due to complex underlying hardware architecture, inefficient algorithm implementation, suboptimal code generation by the compilers, inefficient parallelization, and so on. As a result, the HPC community spends a significant effort detecting and fixing the performance bugs frequently appearing in scientific applications. However, it is important to accumulate the experience to guide the scientific software engineering community to write performance-efficient code.},
	language = {en},
	urldate = {2024-05-05},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Kalam Azad, Md Abul and Iqbal, Nafees and Hassan, Foyzul and Roy, Probir},
	month = may,
	year = {2023},
	pages = {194--206},
	file = {Kalam Azad et al. - 2023 - An Empirical Study of High Performance Computing (.pdf:/Users/akazad/Zotero/storage/5W2QKMRV/Kalam Azad et al. - 2023 - An Empirical Study of High Performance Computing (.pdf:application/pdf},
}

@misc{bai_hallucination_2024,
	title = {Hallucination of {Multimodal} {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Hallucination of {Multimodal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.18930},
	abstract = {This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination. CCS Concepts: • Computing methodologies → Computer vision; Natural language processing; Machine learning.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18930 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Bai et al. - 2024 - Hallucination of Multimodal Large Language Models.pdf:/Users/akazad/Zotero/storage/TAIXCGEW/Bai et al. - 2024 - Hallucination of Multimodal Large Language Models.pdf:application/pdf},
}

@misc{noauthor_granite-code-modelspaperpdf_nodate,
	title = {granite-code-models/paper.pdf at main · ibm-granite/granite-code-models},
	url = {https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf},
	abstract = {Granite Code Models: A Family of Open Foundation Models for Code Intelligence - ibm-granite/granite-code-models},
	language = {en},
	urldate = {2024-05-07},
	journal = {GitHub},
	keywords = {bar figure},
	file = {Snapshot:/Users/akazad/Zotero/storage/3UQMA46J/paper.html:text/html},
}

@misc{silva_repairllama_2024,
	title = {{RepairLLaMA}: {Efficient} {Representations} and {Fine}-{Tuned} {Adapters} for {Program} {Repair}},
	shorttitle = {{RepairLLaMA}},
	url = {http://arxiv.org/abs/2312.15698},
	abstract = {Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-theart parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective ‘program repair adapter’ for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameterefficient fine-tuning helps fine-tuning to converge and contributes to the effectiveness of the repair adapter to fix data-points outside the fine-tuning data distribution. Overall, RepairLLaMA correctly fixes 125 Defects4J v2 and 82 HumanEval-Java bugs, outperforming all baselines.},
	language = {en},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Silva, André and Fang, Sen and Monperrus, Martin},
	month = mar,
	year = {2024},
	note = {arXiv:2312.15698 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
	file = {Silva et al. - 2024 - RepairLLaMA Efficient Representations and Fine-Tu.pdf:/Users/akazad/Zotero/storage/4VSLD5B4/Silva et al. - 2024 - RepairLLaMA Efficient Representations and Fine-Tu.pdf:application/pdf},
}

@misc{monperrus_monperrusmegadiff_2021,
	title = {monperrus/megadiff:},
	copyright = {Open Access},
	shorttitle = {monperrus/megadiff},
	url = {https://zenodo.org/record/5013515},
	abstract = {This paper presents Megadiﬀ, a dataset of source code diﬀs. It focuses on Java, with strict inclusion criteria based on commit message and diﬀ size. Megadiﬀ contains 663 029 Java diﬀs that can be used for research on commit comprehension, fault localization, automated program repair, and machine learning on code changes.},
	urldate = {2024-05-08},
	publisher = {[object Object]},
	author = {Monperrus, Martin},
	month = jun,
	year = {2021},
	doi = {10.5281/ZENODO.5013515},
}

@article{monperrus_megadi_nodate,
	title = {Megadiﬀ: {A} {Dataset} of 600k {Java} {Source} {Code} {Changes} {Categorized} by {Diﬀ} {Size}},
	abstract = {This paper presents Megadiﬀ, a dataset of source code diﬀs. It focuses on Java, with strict inclusion criteria based on commit message and diﬀ size. Megadiﬀ contains 663 029 Java diﬀs that can be used for research on commit comprehension, fault localization, automated program repair, and machine learning on code changes.},
	language = {en},
	author = {Monperrus, Martin and Martinez, Matias and Ye, He and Madeiral, Fernanda and Durieux, Thomas and Yu, Zhongxing},
	file = {Monperrus et al. - Megadiﬀ A Dataset of 600k Java Source Code Change.pdf:/Users/akazad/Zotero/storage/S2CJR8R8/Monperrus et al. - Megadiﬀ A Dataset of 600k Java Source Code Change.pdf:application/pdf},
}

@misc{perozzi_let_2024,
	title = {Let {Your} {Graph} {Do} the {Talking}: {Encoding} {Structured} {Data} for {LLMs}},
	shorttitle = {Let {Your} {Graph} {Do} the {Talking}},
	url = {http://arxiv.org/abs/2402.05862},
	abstract = {How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g., knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73\% points - on node, edge and, graph-level tasks from the GraphQA benchmark.},
	language = {en},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Perozzi, Bryan and Fatemi, Bahare and Zelle, Dustin and Tsitsulin, Anton and Kazemi, Mehran and Al-Rfou, Rami and Halcrow, Jonathan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05862 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.7, Statistics - Machine Learning, Computer Science - Social and Information Networks, I.2.6, I.5.1},
	file = {Perozzi et al. - 2024 - Let Your Graph Do the Talking Encoding Structured.pdf:/Users/akazad/Zotero/storage/4MVPU7GX/Perozzi et al. - 2024 - Let Your Graph Do the Talking Encoding Structured.pdf:application/pdf},
}

@inproceedings{selakovic_performance_2016,
	address = {Austin Texas},
	title = {Performance issues and optimizations in {JavaScript}: an empirical study},
	isbn = {978-1-4503-3900-1},
	shorttitle = {Performance issues and optimizations in {JavaScript}},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884829},
	doi = {10.1145/2884781.2884829},
	abstract = {As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-eﬃciency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 ﬁxed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that ineﬃcient usage of APIs is the most prevalent root cause. Furthermore, we ﬁnd that most issues are addressed by optimizations that modify only a few lines of code, without signiﬁcantly aﬀecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we ﬁnd that only 42.68\% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we ﬁnd based on the patterns identiﬁed during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Selakovic, Marija and Pradel, Michael},
	month = may,
	year = {2016},
	pages = {61--72},
	file = {Selakovic and Pradel - 2016 - Performance issues and optimizations in JavaScript.pdf:/Users/akazad/Zotero/storage/9LA8R892/Selakovic and Pradel - 2016 - Performance issues and optimizations in JavaScript.pdf:application/pdf},
}

@misc{van_der_aalst_how_2022,
	title = {How to {Write} {Beautiful} {Process}-and-{Data}-{Science} {Papers}?},
	url = {http://arxiv.org/abs/2203.09286},
	abstract = {After 25 years of PhD supervision, the author noted typical recurring problems that make papers look sloppy, diﬃcult to read, and incoherent. The goal is not to write a paper for the sake of writing a paper, but to convey a valuable message that is clear and precise. The goal is to write papers that have an impact and are still understandable a couple of decades later. Our mission should be to create papers of high quality that people want to read and that can stand the test of time. We use Dijkstra’s adagium “Beauty Is Our Business” to stress the importance of simplicity, correctness, and cleanness.},
	language = {en},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {van der Aalst, Wil M. P.},
	month = aug,
	year = {2022},
	note = {arXiv:2203.09286 [cs]},
	keywords = {A.0, Computer Science - Digital Libraries, K.3.0},
	file = {van der Aalst - 2022 - How to Write Beautiful Process-and-Data-Science Pa.pdf:/Users/akazad/Zotero/storage/C4BYWXR3/van der Aalst - 2022 - How to Write Beautiful Process-and-Data-Science Pa.pdf:application/pdf},
}

@misc{trumper_performance_2023,
	title = {Performance {Embeddings}: {A} {Similarity}-based {Approach} to {Automatic} {Performance} {Optimization}},
	shorttitle = {Performance {Embeddings}},
	url = {http://arxiv.org/abs/2303.08142},
	abstract = {Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.},
	language = {en},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Trümper, Lukas and Ben-Nun, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08142 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Trümper et al. - 2023 - Performance Embeddings A Similarity-based Approac.pdf:/Users/akazad/Zotero/storage/FBUNTYY8/Trümper et al. - 2023 - Performance Embeddings A Similarity-based Approac.pdf:application/pdf},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://link.springer.com/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are brieﬂy reviewed and comments on future research are discussed and forwarded.},
	language = {en},
	number = {6},
	urldate = {2024-05-13},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = jun,
	year = {2021},
	pages = {1789--1819},
	file = {Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:/Users/akazad/Zotero/storage/9UIQU87Y/Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},
}

@article{sullivan_structure_nodate,
	title = {The {Structure} and {Value} of {Modularity} in {Software} {Design}},
	abstract = {The concept of information hiding modularity is a cornerstone of modern software design thought, but its formulation remains casual and its emphasis on changeability is imperfectly related to the goal of creating added value in a given context. We need better explanatory and prescriptive models of the nature and value of information hiding. We evaluate the potential of a new theory—developed to account for the influence of modularity on the evolution of the computer industry—to inform software design. The theory uses design structure matrices to model designs and real options techniques to value them. To test the potential utility of the theory for software we apply it to Parnas’s KWIC designs. We contribute an extension to design structure matrices, and we show that the options results are consistent with Parnas’s conclusions. Our results suggest that such a theory does have potential to help inform software design.},
	language = {en},
	author = {Sullivan, Kevin J and Griswold, William G and Cai, Yuanfang and Hallen, Ben},
	file = {Sullivan et al. - The Structure and Value of Modularity in Software .pdf:/Users/akazad/Zotero/storage/NU6DTK7A/Sullivan et al. - The Structure and Value of Modularity in Software .pdf:application/pdf},
}

@inproceedings{trumper_performance_2023-1,
	address = {Orlando FL USA},
	title = {Performance {Embeddings}: {A} {Similarity}-{Based} {Transfer} {Tuning} {Approach} to {Performance} {Optimization}},
	isbn = {9798400700569},
	shorttitle = {Performance {Embeddings}},
	url = {https://dl.acm.org/doi/10.1145/3577193.3593714},
	doi = {10.1145/3577193.3593714},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Trümper, Lukas and Ben-Nun, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
	month = jun,
	year = {2023},
	pages = {50--62},
	file = {Full Text PDF:/Users/akazad/Zotero/storage/W9I9KSHX/Trümper et al. - 2023 - Performance Embeddings A Similarity-Based Transfe.pdf:application/pdf},
}

@misc{zhang_revisiting_2023,
	title = {Revisiting {Sentiment} {Analysis} for {Software} {Engineering} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.11113},
	abstract = {Software development is an inherently collaborative process, where various stakeholders frequently express their opinions and emotions across diverse platforms. Recognizing the sentiments conveyed in these interactions is crucial for the effective development and ongoing maintenance of software systems. Over the years, many tools have been proposed to aid in sentiment analysis, but accurately identifying the sentiments expressed in software engineering datasets remains challenging. Although fine-tuned smaller large language models (sLLMs) have shown potential in handling software engineering tasks, they struggle with the shortage of labeled data. With the emergence of bigger large language models (bLLMs), it is pertinent to investigate whether they can handle this challenge in the context of sentiment analysis for software engineering. In this work, we undertake a comprehensive empirical study using five established datasets. We assess the performance of three open-source bLLMs in both zero-shot and few-shot scenarios. Additionally, we compare them with fine-tuned sLLMs. Our experimental findings demonstrate that bLLMs exhibit state-of-the-art performance on datasets marked by limited training data and imbalanced distributions. bLLMs can also achieve excellent performance under a zero-shot setting. However, when ample training data is available or the dataset exhibits a more balanced distribution, fine-tuned sLLMs can still achieve superior results. CCS Concepts: • Software and its engineering → Maintaining software.},
	language = {en},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Zhang, Ting and Irsan, Ivana Clairine and Thung, Ferdian and Lo, David},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11113 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Zhang et al. - 2023 - Revisiting Sentiment Analysis for Software Enginee.pdf:/Users/akazad/Zotero/storage/M52YBTAR/Zhang et al. - 2023 - Revisiting Sentiment Analysis for Software Enginee.pdf:application/pdf},
}

@inproceedings{wei_demystifying_2024,
	address = {Lisbon Portugal},
	title = {Demystifying and {Detecting} {Misuses} of {Deep} {Learning} {APIs}},
	isbn = {9798400702174},
	url = {https://dl.acm.org/doi/10.1145/3597503.3639177},
	doi = {10.1145/3597503.3639177},
	abstract = {Deep Learning (DL) libraries have significantly impacted various domains in computer science over the last decade. However, developers often face challenges when using the DL APIs, as the development paradigm of DL applications differs greatly from traditional software development. Existing studies on API misuse mainly focus on traditional software, leaving a gap in understanding API misuse within DL APIs. To address this gap, we present the first comprehensive study of DL API misuse in TensorFlow and PyTorch. Specifically, we first collect a dataset of 4,224 commits from the top 200 most-starred projects using these two libraries and manually identified 891 API misuses. We then investigate the characteristics of these misuses from three perspectives, i.e., types, root causes, and symptoms. We have also conducted an evaluation to assess the effectiveness of the current state-of-the-art API misuse detector on our 891 confirmed API misuses. Our results confirmed that the stateof-the-art (SOTA) API misuse detector is ineffective in detecting DL API misuses. To address the limitations of existing API misuse detection for DL APIs, we propose LLMAPIDet, which leverages Large Language Models (LLMs) for DL API misuse detection and repair. We build LLMAPIDet by prompt-tuning a chain of ChatGPT prompts on 600 out of 891 confirmed API misuses and reserve the rest 291 API misuses as the testing dataset. Our evaluation shows that LLMAPIDet can detect 48 out of the 291 DL API misuses while none of them can be detected by the existing API misuse detector. We further evaluate LLMAPIDet on the latest versions of 10 GitHub projects. The evaluation shows that LLMAPIDet can identify 119 previously unknown API misuses and successfully fix 46 of them.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Wei, Moshi and Harzevili, Nima Shiri and Huang, Yuekai and Yang, Jinqiu and Wang, Junjie and Wang, Song},
	month = apr,
	year = {2024},
	pages = {1--12},
	file = {Wei et al. - 2024 - Demystifying and Detecting Misuses of Deep Learnin.pdf:/Users/akazad/Zotero/storage/9Q5AST3X/Wei et al. - 2024 - Demystifying and Detecting Misuses of Deep Learnin.pdf:application/pdf},
}

@misc{gekhman_does_2024,
	title = {Does {Fine}-{Tuning} {LLMs} on {New} {Knowledge} {Encourage} {Hallucinations}?},
	url = {http://arxiv.org/abs/2405.05904},
	abstract = {When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model’s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas finetuning teaches them to use it more efficiently.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
	month = may,
	year = {2024},
	note = {arXiv:2405.05904 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Gekhman et al. - 2024 - Does Fine-Tuning LLMs on New Knowledge Encourage H.pdf:/Users/akazad/Zotero/storage/9MPJCI95/Gekhman et al. - 2024 - Does Fine-Tuning LLMs on New Knowledge Encourage H.pdf:application/pdf},
}

@article{newaz_evaluating_nodate,
	title = {Evaluating {Adaptive} {Routing} {Performance} on {Large} {Scale} {Megafly} {Topology}},
	language = {en},
	author = {Newaz, Nahid and Mollah, Atiqul and Faizian, Peyman and Tong, Zhou},
	keywords = {poster},
	file = {Newaz et al. - Evaluating Adaptive Routing Performance on Large S.pdf:/Users/akazad/Zotero/storage/5DIVY6CF/Newaz et al. - Evaluating Adaptive Routing Performance on Large S.pdf:application/pdf},
}

@misc{rawal_cinepile_2024,
	title = {{CinePile}: {A} {Long} {Video} {Question} {Answering} {Dataset} and {Benchmark}},
	shorttitle = {{CinePile}},
	url = {http://arxiv.org/abs/2405.08813},
	abstract = {Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset. The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Rawal, Ruchit and Saifullah, Khalid and Basri, Ronen and Jacobs, David and Somepalli, Gowthami and Goldstein, Tom},
	month = may,
	year = {2024},
	note = {arXiv:2405.08813 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {Rawal et al. - 2024 - CinePile A Long Video Question Answering Dataset .pdf:/Users/akazad/Zotero/storage/6EMFNVPQ/Rawal et al. - 2024 - CinePile A Long Video Question Answering Dataset .pdf:application/pdf},
}

@misc{edwards_language_2024,
	title = {Language {Models} for {Text} {Classification}: {Is} {In}-{Context} {Learning} {Enough}?},
	shorttitle = {Language {Models} for {Text} {Classification}},
	url = {http://arxiv.org/abs/2403.17661},
	abstract = {Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.},
	language = {en},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Edwards, Aleksandra and Camacho-Collados, Jose},
	month = apr,
	year = {2024},
	note = {arXiv:2403.17661 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Edwards and Camacho-Collados - 2024 - Language Models for Text Classification Is In-Con.pdf:/Users/akazad/Zotero/storage/3KBBZMTV/Edwards and Camacho-Collados - 2024 - Language Models for Text Classification Is In-Con.pdf:application/pdf},
}
