{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 00:39:13.386742: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-19 00:39:13.415677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-19 00:39:13.916405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and map Mistral's raw output to 'Yes' or 'No'\n",
    "def clean_prediction(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    match = re.search(r'\\b(Yes|No)\\b', text)\n",
    "    return match.group(0) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_evaluate(test_data_path, output_csv_path):\n",
    "    # Load the true labels and data from the .jsonl file\n",
    "    data = []\n",
    "    y_true = []\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            data.append(record)  # Store each record for later\n",
    "            y_true.append(record['target'])  # Assuming 'target' is the true label field\n",
    "\n",
    "    # Initialize the Mistral LLM\n",
    "    llm = LLM(model=\"/home/akazad/Mistral-7B-Instruct-v0.2-AWQ/\", tensor_parallel_size=2, gpu_memory_utilization=0.8, enforce_eager=True, quantization=\"awq\", dtype=\"auto\")\n",
    "    #llm = LLM(model=\"/home/akazad/Mistral-7B-Instruct-v0.2/\", tensor_parallel_size=2, gpu_memory_utilization=0.8, enforce_eager=True, quantization=\"awq\", dtype=\"auto\")\n",
    "\n",
    "    # Prepare the prompts\n",
    "    prompt_template = ''' <s> [INST] You are an analytical tool specialized in processing and classifying GitHub commit messages along with their associated code diffs. Your task is to assess the developer's intent in a given commit message and code diff, and categorize it into one of the following predefined categories based on its content:\n",
    "                      \n",
    "'Yes': A commit message and code diff that explicitly mention or demonstrate performance improvement or optimization, specifically in terms of execution time or resource utilization. The message or diff should clearly indicate actions that made the code run faster or more efficiently, use less memory, or more efficiently utilize system resources. This includes optimizations like replacing inefficient code patterns that are known to kill performance even if the message does not use the words 'improvement' or 'performance' explicitly. The diff may show changes such as algorithmic improvements, memory management enhancements, reduced computational complexity, or better resource utilization.\n",
    "\n",
    "'No': A commit message and code diff that do not pertain to performance enhancements. This includes changes related to testing, documentation, performance profiling/monitoring/debugging/analysis, and bug/error/crash fixes that don't explicitly mention performance improvement of the application itself. Also, code refactoring or feature addition without explicit performance optimization, and mentions of necessary, speculative, or potential performance enhancements without concrete evidence or results. Any message or diff that is irrelevant, unclear, ambiguous, or does not provide enough context to determine its intent should also be classified as 'No'. Additionally, pay close attention to the context in which terms like 'performance', 'improve', or 'improvements' are used. Not all improvements are related to performance—only classify a message as 'Yes' if it specifically mentions enhancements related to execution time, memory usage, or resource efficiency. Avoid making assumptions based on ambiguous terms. You should have high confidence in classifying a message as 'Yes' based on careful examination of the information provided in the commit message and code diff.\n",
    "\n",
    "If the commit message and code diff contain multiple intentions, where at least one of those intentions includes a performance improvement, classify the entire commit as 'Yes'.\n",
    "You will only respond with the predefined category. Do not include the word 'Category'. Do not provide explanations or notes.\n",
    "\n",
    "Commit message: ```{commit_message}```\n",
    "Code diff: ```{code_diff}```\n",
    "[/INST] Model_answer:  </s>'''\n",
    "\n",
    "    prompts = [\n",
    "        prompt_template.format(\n",
    "            commit_message=record['commit_message'], code_diff=record['code_diff']) for record in data]\n",
    "\n",
    "    # Generate Mistral predictions\n",
    "    sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=5)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Clean and map the Mistral predictions\n",
    "    mistral_classifications = [output.outputs[0].text.strip() for output in outputs]\n",
    "    cleaned_predictions = [clean_prediction(pred) for pred in mistral_classifications]\n",
    "    \n",
    "        # Save the results to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Mistral_classification'] = mistral_classifications\n",
    "    df['Mistral_Target'] = cleaned_predictions\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.clean_prediction(text)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Save the results to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Mistral_classification'] = mistral_classifications\n",
    "    df['Mistral_Target'] = cleaned_predictions\n",
    "\n",
    "    # Only save necessary fields\n",
    "    results_df = df[['commit_url', 'commit_message', 'code_diff', 'Mistral_Target']].copy()\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Convert y_true to a numpy array for consistency\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    # Convert cleaned predictions ('Yes' -> 1, 'No' -> 0)\n",
    "    mistral_numeric_predictions = np.array([1 if pred == 'Yes' else 0 for pred in cleaned_predictions if pred is not None])\n",
    "\n",
    "    # Handle case when there are no predictions or only a single class\n",
    "    if len(np.unique(mistral_numeric_predictions)) == 0:\n",
    "        print(\"No predictions were made.\")\n",
    "        return\n",
    "\n",
    "    # Calculate F1 Score and other metrics\n",
    "    report = classification_report(\n",
    "        y_true[:len(mistral_numeric_predictions)],\n",
    "        mistral_numeric_predictions,\n",
    "        target_names=['No', 'Yes'],\n",
    "        labels=[0, 1],  # Specify the labels explicitly\n",
    "        zero_division=0  # Avoid zero division issues\n",
    "    )\n",
    "\n",
    "    # Write the classification report to the same file\n",
    "    with open(output_csv_path, 'a') as file:\n",
    "        file.write(\"\\nClassification Report:\\n\")\n",
    "        file.write(report)\n",
    "\n",
    "    print(\"All data and results have been saved to\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-19 00:39:28 awq_marlin.py:101] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 10-19 00:39:28 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 10-19 00:39:28 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 10-19 00:39:28 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-19 00:39:28 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/akazad/Mistral-7B-Instruct-v0.2-AWQ/', speculative_config=None, tokenizer='/home/akazad/Mistral-7B-Instruct-v0.2-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/akazad/Mistral-7B-Instruct-v0.2-AWQ/, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 10-19 00:39:28 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 10-19 00:39:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:39:28 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 10-19 00:39:29 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 10-19 00:39:29 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:39:29 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:39:29 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 10-19 00:39:29 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/akazad/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m WARNING 10-19 00:39:29 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 10-19 00:39:29 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/akazad/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m WARNING 10-19 00:39:29 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 10-19 00:39:29 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7853caa97bb0>, local_subscribe_port=43671, remote_subscribe_port=None)\n",
      "INFO 10-19 00:39:29 model_runner.py:1056] Starting to load model /home/akazad/Mistral-7B-Instruct-v0.2-AWQ/...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:39:29 model_runner.py:1056] Starting to load model /home/akazad/Mistral-7B-Instruct-v0.2-AWQ/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe6a0ef9e194856978b9a53c3ecb072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-19 00:39:30 model_runner.py:1067] Loading model weights took 1.9429 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:39:30 model_runner.py:1067] Loading model weights took 1.9429 GB\n",
      "INFO 10-19 00:39:36 distributed_gpu_executor.py:57] # GPU blocks: 14387, # CPU blocks: 4096\n",
      "INFO 10-19 00:39:36 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 7.02x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 276/276 [00:34<00:00,  8.07it/s, est. speed input: 6191.35 toks/s, output: 40.35 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-19 00:40:13 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1730038)\u001b[0;0m INFO 10-19 00:40:13 multiproc_worker_utils.py:240] Worker exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_data_path = '/home/akazad/gt_final.jsonl'  # Path to your test data\n",
    "output_csv_path = 'Mistral_classification_results.csv'  # Path to save the results\n",
    "result = process_and_evaluate(test_data_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['commit_url', 'commit_message', 'code_diff',\n",
       "       'commit_message_token_length', 'code_token_length',\n",
       "       'combined_token_length', 'target', 'Mistral_classification',\n",
       "       'Mistral_Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_url</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>code_diff</th>\n",
       "      <th>commit_message_token_length</th>\n",
       "      <th>code_token_length</th>\n",
       "      <th>combined_token_length</th>\n",
       "      <th>target</th>\n",
       "      <th>Mistral_classification</th>\n",
       "      <th>Mistral_Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/qbittorrent/qBittorrent/com...</td>\n",
       "      <td>Change MixedModeAlgorithm default to TCP. Clos...</td>\n",
       "      <td>@@ -304,7 +304,7 @@ Session::Session(QObject *...</td>\n",
       "      <td>55</td>\n",
       "      <td>287</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/fzi-forschungszentrum-infor...</td>\n",
       "      <td>Further increase wait times for shove tree tes...</td>\n",
       "      <td>@@ -192,13 +192,13 @@ class TestShoveTree(unit...</td>\n",
       "      <td>9</td>\n",
       "      <td>318</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/tonlabs/TVM-Compiler/commit...</td>\n",
       "      <td>ValueTracking: Small cleanup in ComputeNumSign...</td>\n",
       "      <td>@@ -1768,7 +1768,7 @@ unsigned ComputeNumSignB...</td>\n",
       "      <td>43</td>\n",
       "      <td>414</td>\n",
       "      <td>457</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/git/git/commit/a6eec1263883...</td>\n",
       "      <td>upload-pack: drop lookup-before-parse optimiza...</td>\n",
       "      <td>@@ -327,9 +327,7 @@ static int got_sha1(char *...</td>\n",
       "      <td>189</td>\n",
       "      <td>148</td>\n",
       "      <td>337</td>\n",
       "      <td>1</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/systemd/systemd/commit/66a6...</td>\n",
       "      <td>sd-dhcp-client: --omg-optimized\\n\\nPassing the...</td>\n",
       "      <td>@@ -37,8 +37,7 @@ int dhcp_network_bind_raw_so...</td>\n",
       "      <td>183</td>\n",
       "      <td>218</td>\n",
       "      <td>401</td>\n",
       "      <td>1</td>\n",
       "      <td>'Yes'</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>https://github.com/hfinkel/llvm-project-cxxjit...</td>\n",
       "      <td>Output a very high-precision number  llvm-svn:...</td>\n",
       "      <td>@@ -80,7 +80,7 @@ static inline std::string it...</td>\n",
       "      <td>17</td>\n",
       "      <td>91</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>https://github.com/intel/cm-compiler/commit/d1...</td>\n",
       "      <td>Handle functions with multiple exit blocks pro...</td>\n",
       "      <td>@@ -526,6 +526,9 @@ bool GVNPRE::runOnFunction...</td>\n",
       "      <td>17</td>\n",
       "      <td>383</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>https://github.com/ilstam/FF-Multi-Converter/c...</td>\n",
       "      <td>use textwrap.fill() from stdlib to wrap long l...</td>\n",
       "      <td>@@ -30,7 +30,7 @@ import sys\\n import re\\n imp...</td>\n",
       "      <td>13</td>\n",
       "      <td>452</td>\n",
       "      <td>465</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>https://github.com/NYRDS/remixed-dungeon/commi...</td>\n",
       "      <td>33% chance for lich to jump on hit</td>\n",
       "      <td>@@ -199,7 +199,11 @@ public class Lich extends...</td>\n",
       "      <td>9</td>\n",
       "      <td>141</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>https://github.com/morganstanley/binlog/commit...</td>\n",
       "      <td>Note why logging a large variant takes long co...</td>\n",
       "      <td>@@ -54,8 +54,9 @@ int main()\\n   BINLOG_INFO(\"...</td>\n",
       "      <td>14</td>\n",
       "      <td>178</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>No.\\n\\nEx</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            commit_url  \\\n",
       "0    https://github.com/qbittorrent/qBittorrent/com...   \n",
       "1    https://github.com/fzi-forschungszentrum-infor...   \n",
       "2    https://github.com/tonlabs/TVM-Compiler/commit...   \n",
       "3    https://github.com/git/git/commit/a6eec1263883...   \n",
       "4    https://github.com/systemd/systemd/commit/66a6...   \n",
       "..                                                 ...   \n",
       "271  https://github.com/hfinkel/llvm-project-cxxjit...   \n",
       "272  https://github.com/intel/cm-compiler/commit/d1...   \n",
       "273  https://github.com/ilstam/FF-Multi-Converter/c...   \n",
       "274  https://github.com/NYRDS/remixed-dungeon/commi...   \n",
       "275  https://github.com/morganstanley/binlog/commit...   \n",
       "\n",
       "                                        commit_message  \\\n",
       "0    Change MixedModeAlgorithm default to TCP. Clos...   \n",
       "1    Further increase wait times for shove tree tes...   \n",
       "2    ValueTracking: Small cleanup in ComputeNumSign...   \n",
       "3    upload-pack: drop lookup-before-parse optimiza...   \n",
       "4    sd-dhcp-client: --omg-optimized\\n\\nPassing the...   \n",
       "..                                                 ...   \n",
       "271  Output a very high-precision number  llvm-svn:...   \n",
       "272  Handle functions with multiple exit blocks pro...   \n",
       "273  use textwrap.fill() from stdlib to wrap long l...   \n",
       "274                 33% chance for lich to jump on hit   \n",
       "275  Note why logging a large variant takes long co...   \n",
       "\n",
       "                                             code_diff  \\\n",
       "0    @@ -304,7 +304,7 @@ Session::Session(QObject *...   \n",
       "1    @@ -192,13 +192,13 @@ class TestShoveTree(unit...   \n",
       "2    @@ -1768,7 +1768,7 @@ unsigned ComputeNumSignB...   \n",
       "3    @@ -327,9 +327,7 @@ static int got_sha1(char *...   \n",
       "4    @@ -37,8 +37,7 @@ int dhcp_network_bind_raw_so...   \n",
       "..                                                 ...   \n",
       "271  @@ -80,7 +80,7 @@ static inline std::string it...   \n",
       "272  @@ -526,6 +526,9 @@ bool GVNPRE::runOnFunction...   \n",
       "273  @@ -30,7 +30,7 @@ import sys\\n import re\\n imp...   \n",
       "274  @@ -199,7 +199,11 @@ public class Lich extends...   \n",
       "275  @@ -54,8 +54,9 @@ int main()\\n   BINLOG_INFO(\"...   \n",
       "\n",
       "     commit_message_token_length  code_token_length  combined_token_length  \\\n",
       "0                             55                287                    342   \n",
       "1                              9                318                    327   \n",
       "2                             43                414                    457   \n",
       "3                            189                148                    337   \n",
       "4                            183                218                    401   \n",
       "..                           ...                ...                    ...   \n",
       "271                           17                 91                    108   \n",
       "272                           17                383                    400   \n",
       "273                           13                452                    465   \n",
       "274                            9                141                    150   \n",
       "275                           14                178                    192   \n",
       "\n",
       "     target Mistral_classification Mistral_Target  \n",
       "0         1              No.\\n\\nEx             No  \n",
       "1         0              No.\\n\\nEx             No  \n",
       "2         0              No.\\n\\nEx             No  \n",
       "3         1              No.\\n\\nEx             No  \n",
       "4         1                  'Yes'            Yes  \n",
       "..      ...                    ...            ...  \n",
       "271       0              No.\\n\\nEx             No  \n",
       "272       0              No.\\n\\nEx             No  \n",
       "273       0              No.\\n\\nEx             No  \n",
       "274       0              No.\\n\\nEx             No  \n",
       "275       0              No.\\n\\nEx             No  \n",
       "\n",
       "[276 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.99      0.79       138\n",
      "           1       0.97      0.48      0.64       138\n",
      "\n",
      "    accuracy                           0.73       276\n",
      "   macro avg       0.81      0.73      0.71       276\n",
      "weighted avg       0.81      0.73      0.71       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df = result\n",
    "# Assuming df is your DataFrame\n",
    "# Create a new column 'target_prediction' with 1 for 'Yes' and 0 for 'No'\n",
    "df['target_prediction'] = df['Mistral_Target'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Extract 'target' and 'target_prediction' columns as lists\n",
    "y_true = df['target'].tolist()\n",
    "y_pred = df['target_prediction'].tolist()\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_url</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>code_diff</th>\n",
       "      <th>commit_message_token_length</th>\n",
       "      <th>code_token_length</th>\n",
       "      <th>combined_token_length</th>\n",
       "      <th>target</th>\n",
       "      <th>Mistral_classification</th>\n",
       "      <th>Mistral_Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [commit_url, commit_message, code_diff, commit_message_token_length, code_token_length, combined_token_length, target, Mistral_classification, Mistral_Target]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "\n",
    "def process_batches(df, batch_size, output_csv_path):\n",
    "    # Determine the starting batch index in case of a restart\n",
    "    if os.path.exists(output_csv_path):\n",
    "        existing_df = pd.read_csv(output_csv_path,engine='python').dropna()\n",
    "        processed_entries = len(existing_df)\n",
    "        start_batch_idx = processed_entries // batch_size\n",
    "        print(f\"Resuming from batch {start_batch_idx}.\")\n",
    "    else:\n",
    "        start_batch_idx = 0\n",
    "\n",
    "    llm = LLM(model=\"/home/akazad/Mistral-7B-Instruct-v0.2-AWQ/\",tensor_parallel_size=2,gpu_memory_utilization=0.8,enforce_eager=True,\n",
    "              quantization=\"awq\", dtype=\"auto\")\n",
    "    \n",
    "    num_batches = (len(df) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "    for batch_idx in range(start_batch_idx, num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        prompt_template = ''' <s> [INST] You are an analytical tool specialized in processing and classifying GitHub commit messages along with their associated code diffs. Your task is to assess the developer's intent in a given commit message and code diff, and categorize it into one of the following predefined categories based on its content:\n",
    "                      \n",
    "'Yes': A commit message and code diff that explicitly mention or demonstrate performance improvement or optimization, specifically in terms of execution time or resource utilization. The message or diff should clearly indicate actions that made the code run faster or more efficiently, use less memory, or more efficiently utilize system resources. This includes optimizations like replacing inefficient code patterns that are known to kill performance even if the message does not use the words 'improvement' or 'performance' explicitly. The diff may show changes such as algorithmic improvements, memory management enhancements, reduced computational complexity, or better resource utilization.\n",
    "\n",
    "'No': A commit message and code diff that do not pertain to performance enhancements. This includes changes related to testing, documentation, performance profiling/monitoring/debugging/analysis, and bug/error/crash fixes that don't explicitly mention performance improvement of the application itself. Also, code refactoring or feature addition without explicit performance optimization, and mentions of necessary, speculative, or potential performance enhancements without concrete evidence or results. Any message or diff that is irrelevant, unclear, ambiguous, or does not provide enough context to determine its intent should also be classified as 'No'. Additionally, pay close attention to the context in which terms like 'performance', 'improve', or 'improvements' are used. Not all improvements are related to performance—only classify a message as 'Yes' if it specifically mentions enhancements related to execution time, memory usage, or resource efficiency. Avoid making assumptions based on ambiguous terms. You should have high confidence in classifying a message as 'Yes' based on careful examination of the information provided in the commit message and code diff.\n",
    "\n",
    "If the commit message and code diff contain multiple intentions, where at least one of those intentions includes a performance improvement, classify the entire commit as 'Yes'.\n",
    "You will only respond with the predefined category. Do not include the word 'Category'. Do not provide explanations or notes.\n",
    "\n",
    "Commit message: ```{commit_message}```\n",
    "Code diff: ```{code_diff}```\n",
    "[/INST] Model_answer:  </s>'''\n",
    "\n",
    "\n",
    "        prompts = [\n",
    "            prompt_template.format(\n",
    "                commit_message=row['commit_message'],code_diff=row['diff']) for _, row in batch_df.iterrows()]\n",
    "\n",
    "        sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=5)\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        batch_df['Mistral_classification']=[output.outputs[0].text for output in outputs]\n",
    "        results_df=batch_df[['commit_url','commit_message', 'diff','nloc', 'n_added_lines', 'n_deleted_lines','Mistral_classification']].copy()\n",
    "        if batch_idx == 0 and not os.path.exists(output_csv_path):\n",
    "            results_df.to_csv(output_csv_path, mode='w', index=False)\n",
    "        else:\n",
    "            results_df.to_csv(output_csv_path, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Batch {batch_idx} processed and saved.\")\n",
    "\n",
    "    print(f\"All data has been saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage of the function\n",
    "output_csv_path = 'Data/Mistral_classification_with_patch_java_1L.csv'\n",
    "process_batches(cpp_filtered, 10000, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta-KD report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('ManojAlexender/second_Base_version_of_codebert_with_commit_and_diff')\n",
    "model = RobertaForSequenceClassification.from_pretrained('ManojAlexender/second_Base_version_of_codebert_with_commit_and_diff')\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('ManojAlexender/second_Base_version_of_codebert_with_commit_and_diff')\n",
    "model = RobertaForSequenceClassification.from_pretrained('ManojAlexender/second_Base_version_of_codebert_with_commit_and_diff')\n",
    "\n",
    "# Load the data from the .jsonl file\n",
    "data = []\n",
    "y_true = []\n",
    "commit_messages = []  # To store commit messages\n",
    "code_diffs = []  # To store code diffs\n",
    "\n",
    "with open('/home/akazad/gt_final.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        # Concatenate 'commit_message' and 'code_diff' fields for the model\n",
    "        commit_message = record['commit_message']\n",
    "        code_diff = record['code_diff']\n",
    "        data.append(commit_message + ' [SEP] ' + code_diff)\n",
    "        # Append the target field to y_true\n",
    "        y_true.append(record['target'])\n",
    "        # Store the commit_message and code_diff for later writing\n",
    "        commit_messages.append(commit_message)\n",
    "        code_diffs.append(code_diff)\n",
    "\n",
    "# Convert y_true to a numpy array for later use\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Function to process data instance by instance and calculate predictions\n",
    "def predict_instance_by_instance(model, tokenizer, data):\n",
    "    model.to('cuda')\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "\n",
    "    for instance in data:\n",
    "        inputs = tokenizer(instance, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "        all_predictions.append(prediction)\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Run prediction once and generate the classification report\n",
    "predictions = predict_instance_by_instance(model, tokenizer, data)\n",
    "\n",
    "# Convert predictions to numpy array and adjust labels if necessary\n",
    "predictions = np.array(predictions)\n",
    "predictions[predictions == 2] = 0  # Adjust according to your label scheme if needed\n",
    "\n",
    "# Write commit_message, code_diff, target, and predicted values to a file\n",
    "with open('results.txt', 'w') as file:\n",
    "    for i in range(len(data)):\n",
    "        file.write(f\"Commit Message: {commit_messages[i]}\\n\")\n",
    "        file.write(f\"Code Diff: {code_diffs[i]}\\n\")\n",
    "        file.write(f\"Target: {y_true[i]}\\n\")\n",
    "        file.write(f\"Predicted: {predictions[i]}\\n\")\n",
    "        file.write(\"-\" * 50 + \"\\n\")  # Separator between instances\n",
    "\n",
    "# Generate classification report and write it to the same file\n",
    "report = classification_report(y_true, predictions)\n",
    "with open('results.txt', 'a') as file:  # Append the classification report\n",
    "    file.write(\"\\nClassification Report:\\n\")\n",
    "    file.write(report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
