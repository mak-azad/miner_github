{"commit_url": "https://github.com/kokkos/kokkos-kernels/commit/272807196b3c14f602bdc08f3acd076b1af422f0", "commit_message": "Async matrix release for MKL >= 2023.2", "code_diff": "@@ -733,9 +733,16 @@ struct spmv_onemkl_wrapper<false> {\n     auto ev_gemv =\n         oneapi::mkl::sparse::gemv(exec.sycl_queue(), mkl_mode, alpha, handle,\n                                   x.data(), beta, y.data(), {ev_opt});\n+    // MKL 2023.2 and up make this release okay async even though it takes a\n+    // pointer to a stack variable\n+#if INTEL_MKL_VERSION >= 20230200\n+    oneapi::mkl::sparse::release_matrix_handle(exec.sycl_queue(), &handle,\n+                                               {ev_gemv});\n+#else\n     auto ev_release = oneapi::mkl::sparse::release_matrix_handle(\n         exec.sycl_queue(), &handle, {ev_gemv});\n     ev_release.wait();\n+#endif\n   }\n };\n \n@@ -768,9 +775,16 @@ struct spmv_onemkl_wrapper<true> {\n         reinterpret_cast<std::complex<mag_type>*>(\n             const_cast<scalar_type*>(x.data())),\n         beta, reinterpret_cast<std::complex<mag_type>*>(y.data()), {ev_opt});\n+    // MKL 2023.2 and up make this release okay async even though it takes a\n+    // pointer to a stack variable\n+#if INTEL_MKL_VERSION >= 20230200\n+    oneapi::mkl::sparse::release_matrix_handle(exec.sycl_queue(), &handle,\n+                                               {ev_gemv});\n+#else\n     auto ev_release = oneapi::mkl::sparse::release_matrix_handle(\n         exec.sycl_queue(), &handle, {ev_gemv});\n     ev_release.wait();\n+#endif\n   }\n };\n \n", "changed_method_name": "KokkosSparse::Impl::spmv_onemkl_wrapper::spmv"}
{"commit_url": "https://github.com/kokkos/kokkos-kernels/commit/cb24a0d477a35e20eabc321985e14d720788c120", "commit_message": "Don't call optimize_gemv for one-shot spmv", "code_diff": "@@ -736,11 +736,9 @@ struct spmv_onemkl_wrapper<false> {\n         const_cast<ordinal_type*>(A.graph.row_map.data()),\n         const_cast<ordinal_type*>(A.graph.entries.data()),\n         const_cast<scalar_type*>(A.values.data()));\n-    auto ev_opt = oneapi::mkl::sparse::optimize_gemv(\n-        exec.sycl_queue(), mkl_mode, handle, {ev_set});\n     auto ev_gemv =\n         oneapi::mkl::sparse::gemv(exec.sycl_queue(), mkl_mode, alpha, handle,\n-                                  x.data(), beta, y.data(), {ev_opt});\n+                                  x.data(), beta, y.data(), {ev_set});\n     // MKL 2023.2 and up make this release okay async even though it takes a\n     // pointer to a stack variable\n #if INTEL_MKL_VERSION >= 20230200\n@@ -776,13 +774,11 @@ struct spmv_onemkl_wrapper<true> {\n         const_cast<ordinal_type*>(A.graph.entries.data()),\n         reinterpret_cast<std::complex<mag_type>*>(\n             const_cast<scalar_type*>(A.values.data())));\n-    auto ev_opt = oneapi::mkl::sparse::optimize_gemv(\n-        exec.sycl_queue(), mkl_mode, handle, {ev_set});\n     auto ev_gemv = oneapi::mkl::sparse::gemv(\n         exec.sycl_queue(), mkl_mode, alpha, handle,\n         reinterpret_cast<std::complex<mag_type>*>(\n             const_cast<scalar_type*>(x.data())),\n-        beta, reinterpret_cast<std::complex<mag_type>*>(y.data()), {ev_opt});\n+        beta, reinterpret_cast<std::complex<mag_type>*>(y.data()), {ev_set});\n     // MKL 2023.2 and up make this release okay async even though it takes a\n     // pointer to a stack variable\n #if INTEL_MKL_VERSION >= 20230200\n", "changed_method_name": "KokkosSparse::Impl::spmv_onemkl_wrapper::spmv"}
{"commit_url": "https://github.com/PrincetonUniversity/athena/commit/36a24ba98c79313fdc8eb99a2b6328e13432b187", "commit_message": "fix a bug in disk.cpp related to #430", "code_diff": "@@ -175,7 +175,7 @@ void GetCylCoord(Coordinates *pco,Real &rad,Real &phi,Real &z,int i,int j,int k)\n     z=pco->x3v(k);\n   } else if (std::strcmp(COORDINATE_SYSTEM, \"spherical_polar\") == 0) {\n     rad=std::abs(pco->x1v(i)*std::sin(pco->x2v(j)));\n-    phi=pco->x3v(i);\n+    phi=pco->x3v(k);\n     z=pco->x1v(i)*std::cos(pco->x2v(j));\n   }\n   return;\n", "changed_method_name": "GetCylCoord"}
{"commit_url": "https://github.com/mapbox/mapbox-gl-native/commit/bea457697a550392971c143071296e0d26a09fb4", "commit_message": "Ignore tile URL sharding\n\nThe source returns sharded tile URLs, but sharding doesn\u2019t buy us anything on native platforms, so always grab the first.\n\nref mapbox/mapbox-gl-native#2007", "code_diff": "@@ -98,7 +98,7 @@ void SourceInfo::parseTileJSONProperties(const rapidjson::Value& value) {\n }\n \n std::string SourceInfo::tileURL(const TileID& id, float pixelRatio) const {\n-    std::string result = tiles.at((id.x + id.y) % tiles.size());\n+    std::string result = tiles.at(0);\n     result = util::mapbox::normalizeTileURL(result, url, type);\n     result = util::replaceTokens(result, [&](const std::string &token) -> std::string {\n         if (token == \"z\") return util::toString(std::min(id.z, static_cast<int8_t>(max_zoom)));\n", "changed_method_name": "mbgl::SourceInfo::tileURL"}
{"commit_url": "https://github.com/mapbox/mapbox-gl-native/commit/2f30dfe97d851f2a2bb4141dc234013b01f8afd3", "commit_message": "Disable heading updates for .FollowWithCourse\n\nFixes #2180.", "code_diff": "@@ -2310,6 +2310,7 @@ CLLocationCoordinate2D MGLLocationCoordinate2DFromLatLng(mbgl::LatLng latLng)\n             break;\n         }\n         case MGLUserTrackingModeFollow:\n+        case MGLUserTrackingModeFollowWithCourse:\n         {\n             self.showsUserLocation = YES;\n \n@@ -2323,7 +2324,6 @@ CLLocationCoordinate2D MGLLocationCoordinate2DFromLatLng(mbgl::LatLng latLng)\n             break;\n         }\n         case MGLUserTrackingModeFollowWithHeading:\n-        case MGLUserTrackingModeFollowWithCourse:\n         {\n             self.showsUserLocation = YES;\n \n", "changed_method_name": "switch"}
{"commit_url": "https://github.com/mapbox/mapbox-gl-native/commit/253a007d99c2079b95d5c6d11715e16815067e16", "commit_message": "[ios][bench] Add total and avg FPS to final summary", "code_diff": "@@ -81,13 +81,17 @@ static const int benchmarkDuration = 200; // frames\n         // Do nothing. The benchmark is completed.\n         NSLog(@\"Benchmark completed.\");\n         NSLog(@\"Result:\");\n+        double totalFPS = 0;\n         size_t colWidth = 0;\n         for (const auto& row : result) {\n             colWidth = std::max(row.first.size(), colWidth);\n         }\n         for (const auto& row : result) {\n             NSLog(@\"| %-*s | %4.1f fps |\", int(colWidth), row.first.c_str(), row.second);\n+            totalFPS += row.second;\n         }\n+        NSLog(@\"Total FPS: %4.1f\", totalFPS);\n+        NSLog(@\"Average FPS: %4.1f\", totalFPS / result.size());\n         exit(0);\n     }\n }\n", "changed_method_name": "for"}
{"commit_url": "https://github.com/rwengine/openrw/commit/f7bd8701db2a98678e206ae78515b0a0e16b092b", "commit_message": "setForcedUpdateAllAabbs to false; Reduces stepSimulation time by 35%.\n\nThis causes a drop from 6.5ms to 4.2ms on my machine.", "code_diff": "@@ -92,6 +92,7 @@ GameWorld::GameWorld(Logger* log, GameData* dat)\n         _overlappingPairCallback.get());\n     gContactProcessedCallback = ContactProcessedCallback;\n     dynamicsWorld->setInternalTickCallback(PhysicsTickCallback, this);\n+    dynamicsWorld->setForceUpdateAllAabbs(false);\n }\n \n GameWorld::~GameWorld() {\n", "changed_method_name": "GameWorld::GameWorld"}
{"commit_url": "https://github.com/kokkos/kokkos/commit/910d43e45b04fb0e7d155233f7069cab0aeefc79", "commit_message": "OpenMP: Adding an ifdef around chunksize for static schedule for GCC compiler.", "code_diff": "@@ -101,8 +101,15 @@ class ParallelFor<FunctorType, Kokkos::RangePolicy<Traits...>, Kokkos::OpenMP> {\n   std::enable_if_t<!std::is_same<typename Policy::schedule_type::type,\n                                  Kokkos::Dynamic>::value>\n   execute_parallel() const {\n+// Specifying an chunksize with GCC compiler leads to performance regression\n+// with static schedule.\n+#ifdef KOKKOS_COMPILER_GNU\n+#pragma omp parallel for schedule(static) \\\n+    num_threads(m_instance->thread_pool_size())\n+#else\n #pragma omp parallel for schedule(static KOKKOS_OPENMP_OPTIONAL_CHUNK_SIZE) \\\n     num_threads(m_instance->thread_pool_size())\n+#endif\n     KOKKOS_PRAGMA_IVDEP_IF_ENABLED\n     for (auto iwork = m_policy.begin(); iwork < m_policy.end(); ++iwork) {\n       exec_work(m_functor, iwork);\n", "changed_method_name": "Kokkos::Impl::ParallelFor::execute_parallel"}
{"commit_url": "https://github.com/FEX-Emu/FEX/commit/a24680b0a1d210099bb72741561174fbd8c07228", "commit_message": "ConstProp: Adds constpool distance heuristic\n\nFEX has a problem with large blocks that uses a ton of constants spread\nthroughout the block. Once a block gets large enough with enough\nconstants that have large live ranges, FEX slows down to unusable speeds\ndue to the register allocator spending more time calculating node\ninterferences than anything else in the program.\n\nThis adds a little heuristic to ensure that constants aren't reused if\nthe previous value is past a certain distance threshold. This threshold\nworks well enough that XeSS's pedantic initialization code doesn't have\nissues now. See https://github.com/FEX-Emu/FEX/issues/2688 for more\ninformation about that.\n\nFEX itself should work to remove bad constant usages to make this pass\nless necessary anyway. In most cases we are materializing duplicated 0,\n1, and masks which could be done without a constant entirely.\nMaybe once we've improve that enough we could remove this constant\npooling entirely.\n\nTo note, this doesn't fix the issue that XeSS causes our register\nallocator, this is purely a heuristic workaround.", "code_diff": "@@ -208,7 +208,11 @@ private:\n       OrderedNode* CodeNode, IROp_Header* IROp);\n   bool ConstantInlining(IREmitter *IREmit, const IRListView& CurrentIR);\n \n-  fextl::unordered_map<uint64_t, OrderedNode*> ConstPool;\n+  struct ConstPoolData {\n+    OrderedNode *Node;\n+    IR::NodeID NodeID;\n+  };\n+  fextl::unordered_map<uint64_t, ConstPoolData> ConstPool;\n   fextl::map<OrderedNode*, uint64_t> AddressgenConsts;\n \n   // Pool inline constant generation. These are typically very small and pool efficiently.\n@@ -222,6 +226,11 @@ private:\n     return Result.first->second;\n   }\n   bool SupportsTSOImm9{};\n+  // This is a heuristic to limit constant pool live ranges to reduce RA interference pressure.\n+  // If the range is unbounded then RA interference pressure seems to increase to the point\n+  // that long blocks of constant usage can slow to a crawl.\n+  // See https://github.com/FEX-Emu/FEX/issues/2688 for more information.\n+  constexpr static uint32_t CONSTANT_POOL_RANGE_LIMIT = 200;\n };\n \n bool ConstProp::HandleConstantPools(IREmitter *IREmit, const IRListView& CurrentIR) {\n@@ -232,11 +241,28 @@ bool ConstProp::HandleConstantPools(IREmitter *IREmit, const IRListView& Current\n     for (auto [CodeNode, IROp] : CurrentIR.GetCode(BlockNode)) {\n       if (IROp->Op == OP_CONSTANT) {\n         auto Op = IROp->C<IR::IROp_Constant>();\n-        if (ConstPool.count(Op->Constant)) {\n-          IREmit->ReplaceAllUsesWith(CodeNode, ConstPool[Op->Constant]);\n+        const auto NewNodeID = CurrentIR.GetID(CodeNode);\n+\n+        auto it = ConstPool.find(Op->Constant);\n+        if (it != ConstPool.end()) {\n+          const auto OldNodeID = it->second.NodeID;\n+\n+          if ((NewNodeID.Value - OldNodeID.Value) > CONSTANT_POOL_RANGE_LIMIT) {\n+            // Don't reuse if the live range is beyond the heurstic range.\n+            // Update the tracked value to this new constant.\n+            it->second.Node = CodeNode;\n+            it->second.NodeID = NewNodeID;\n+            continue;\n+          }\n+\n+          auto CodeIter = CurrentIR.at(CodeNode);\n+          IREmit->ReplaceUsesWithAfter(CodeNode, it->second.Node, CodeIter);\n           Changed = true;\n         } else {\n-          ConstPool[Op->Constant] = CodeNode;\n+          ConstPool[Op->Constant] = ConstPoolData {\n+            .Node = CodeNode,\n+            .NodeID = NewNodeID,\n+          };\n         }\n       }\n     }\n", "changed_method_name": "FEXCore::IR::ConstProp::HandleConstantPools"}
{"commit_url": "https://github.com/bitshares/bitshares-core/commit/3c931a97ec32b15eb4d09886936186dee7a45e35", "commit_message": "remove needless find()", "code_diff": "@@ -750,13 +750,11 @@ namespace graphene { namespace net { namespace detail {\n             idump((inventory_to_advertise));\n             for (const item_id& item_to_advertise : inventory_to_advertise)\n             {\n-              if (peer->inventory_advertised_to_peer.find(item_to_advertise) != peer->inventory_advertised_to_peer.end() )\n-                 idump((*peer->inventory_advertised_to_peer.find(item_to_advertise)));\n-              if (peer->inventory_peer_advertised_to_us.find(item_to_advertise) != peer->inventory_peer_advertised_to_us.end() )\n-                 idump((*peer->inventory_peer_advertised_to_us.find(item_to_advertise)));\n+               auto adv_to_peer = peer->inventory_advertised_to_peer.find(item_to_advertise);\n+               auto adv_to_us   = peer->inventory_peer_advertised_to_us.find(item_to_advertise);\n \n-              if (peer->inventory_advertised_to_peer.find(item_to_advertise) == peer->inventory_advertised_to_peer.end() &&\n-                  peer->inventory_peer_advertised_to_us.find(item_to_advertise) == peer->inventory_peer_advertised_to_us.end())\n+              if (adv_to_peer == peer->inventory_advertised_to_peer.end() &&\n+                  adv_to_us == peer->inventory_peer_advertised_to_us.end())\n               {\n                 items_to_advertise_by_type[item_to_advertise.item_type].push_back(item_to_advertise.item_hash);\n                 peer->inventory_advertised_to_peer.insert(peer_connection::timestamped_item_id(item_to_advertise, fc::time_point::now()));\n@@ -765,6 +763,13 @@ namespace graphene { namespace net { namespace detail {\n                   testnetlog(\"advertising transaction ${id} to peer ${endpoint}\", (\"id\", item_to_advertise.item_hash)(\"endpoint\", peer->get_remote_endpoint()));\n                 dlog(\"advertising item ${id} to peer ${endpoint}\", (\"id\", item_to_advertise.item_hash)(\"endpoint\", peer->get_remote_endpoint()));\n               }\n+              else\n+              {\n+                 if (adv_to_peer != peer->inventory_advertised_to_peer.end() )\n+                    idump( (*adv_to_peer) );\n+                 if (adv_to_us != peer->inventory_peer_advertised_to_us.end() )\n+                    idump( (*adv_to_us) );\n+              }\n             }\n               dlog(\"advertising ${count} new item(s) of ${types} type(s) to peer ${endpoint}\",\n                    (\"count\", total_items_to_send_to_this_peer)\n", "changed_method_name": "graphene::net::detail::node_impl::advertise_inventory_loop"}
{"commit_url": "https://github.com/aspnet/dnx/commit/a561ddc5f7696da195bdfc86ba76dc7ff095dd99", "commit_message": "Use async xcopy to do package installation during publish", "code_diff": "@@ -25,21 +25,16 @@ public async Task Emit(PublishRoot root)\n         {\n             root.Reports.Quiet.WriteLine(\"Using {0} dependency {1}\", _package.Type, Library);\n \n-            var packagePathResolver = new DefaultPackagePathResolver(root.SourcePackagesPath);\n-            var srcNupkgPath = packagePathResolver.GetPackageFilePath(_package.Identity.Name, _package.Identity.Version);\n-\n-            var options = new Packages.AddOptions\n-            {\n-                NuGetPackage = srcNupkgPath,\n-                PackageHash = _package.Library.Sha512,\n-                SourcePackages = root.TargetPackagesPath,\n-            };\n-\n-            // Mute \"packages add\" subcommand\n-            options.Reports = Reports.Constants.NullReports;\n-\n-            var packagesAddCommand = new Packages.AddCommand(options);\n-            await packagesAddCommand.Execute();\n+            var srcPackagePathResolver = new DefaultPackagePathResolver(root.SourcePackagesPath);\n+            var targetPackagePathResolver = new DefaultPackagePathResolver(root.TargetPackagesPath);\n+            var srcPackageDir = srcPackagePathResolver.GetInstallPath(\n+                _package.Identity.Name,\n+                _package.Identity.Version);\n+            var targetPackageDir = targetPackagePathResolver.GetInstallPath(\n+                _package.Identity.Name,\n+                _package.Identity.Version);\n+\n+            await Task.Run(() => root.Operations.Copy(srcPackageDir, targetPackageDir));\n         }\n     }\n }\n", "changed_method_name": "Microsoft.Dnx.Tooling.Publish::PublishPackage::Emit"}
{"commit_url": "https://github.com/NREL/EnergyPlus/commit/ff568f7733204d88c810b10b3c792fe6b9c2a188", "commit_message": "Don't copy structs into kiva convection lambdas.", "code_diff": "@@ -3441,16 +3441,11 @@ void CalcCeilingDiffuserIntConvCoeff(EnergyPlusData &state,\n \n     for (auto SurfNum = Zone(ZoneNum).HTSurfaceFirst; SurfNum <= Zone(ZoneNum).HTSurfaceLast; ++SurfNum) {\n         if (Surface(SurfNum).ExtBoundCond == DataSurfaces::KivaFoundation) {\n+            Real64 height = state.dataSurface->Surface(SurfNum).Height;\n+            bool isWindow = state.dataConstruction->Construct(state.dataSurface->Surface(SurfNum).Construction).TypeIsWindow;\n             state.dataSurfaceGeometry->kivaManager.surfaceConvMap[SurfNum].in =\n                 [=, &state](double Tsurf, double Tamb, double, double, double cosTilt) -> double {\n-                return CalcCeilingDiffuserIntConvCoeff(state,\n-                                                       ACH,\n-                                                       Tsurf,\n-                                                       Tamb,\n-                                                       cosTilt,\n-                                                       AirHumRat,\n-                                                       Surface(SurfNum).Height,\n-                                                       state.dataConstruction->Construct(Surface(SurfNum).Construction).TypeIsWindow);\n+                return CalcCeilingDiffuserIntConvCoeff(state, ACH, Tsurf, Tamb, cosTilt, AirHumRat, height, isWindow);\n             };\n         } else {\n             state.dataHeatBalSurf->SurfHConvInt(SurfNum) =\n", "changed_method_name": "EnergyPlus::ConvectionCoefficients::CalcCeilingDiffuserIntConvCoeff"}
{"commit_url": "https://github.com/EasyRPG/Player/commit/a086820de8eadbd830f3aa5ebf822d548a465879", "commit_message": "When the cache is exhausted do not delete unreferenced assets that were used during the last 50ms (approx. 3 frames).\n\nThe current behaviour (flush everything unreferenced when cache is full) was added to workaround out-of-memory issues on systems with limited RAM but this direct unload makes some use cases like face rendering in the Battle scene with Gauge style really slow (when the cache is full the face is unloaded and reloaded from disk once every frame).\n\nI hope that 50ms is a good compromise here.\n\nFix #2509", "code_diff": "@@ -101,8 +101,15 @@ namespace {\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\tif (cache_size <= cache_limit && cur_ticks - it->second.last_access < 3s) {\n-\t\t\t\t// Below memory limit and last access < 3s\n+\t\t\tauto last_access = cur_ticks - it->second.last_access;\n+\t\t\tbool cache_exhausted = cache_size > cache_limit;\n+\t\t\tif (cache_exhausted) {\n+\t\t\t\tif (last_access <= 50ms) {\n+\t\t\t\t\t// Used during the last 3 frames, must be important, keep it.\n+\t\t\t\t\t++it;\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t} else if (last_access <= 3s) {\n \t\t\t\t++it;\n \t\t\t\tcontinue;\n \t\t\t}\n", "changed_method_name": "FreeBitmapMemory"}
{"commit_url": "https://github.com/EasyRPG/Player/commit/72749b7a94ea09ec59870e099aac9bbf5de89a45", "commit_message": "Effects cache: Fix memory leak\n\nThe BitmapRef reference counter never reached 0 because it was part of the key, preventing deletion.\nChanged it to a pointer. The value is never read so doesn't matter if it becomes stale.\n\nFix #3163", "code_diff": "@@ -81,7 +81,7 @@ namespace {\n \tstd::unordered_map<tile_key_type, std::weak_ptr<Bitmap>> cache_tiles;\n \n \t// rect, flip_x, flip_y, tone, blend\n-\tusing effect_key_type = std::tuple<BitmapRef, Rect, bool, bool, Tone, Color>;\n+\tusing effect_key_type = std::tuple<Bitmap*, Rect, bool, bool, Tone, Color>;\n \tstd::map<effect_key_type, std::weak_ptr<Bitmap>> cache_effects;\n \n \tstd::string system_name;\n@@ -444,7 +444,7 @@ BitmapRef Cache::Tile(StringView filename, int tile_id) {\n \n BitmapRef Cache::SpriteEffect(const BitmapRef& src_bitmap, const Rect& rect, bool flip_x, bool flip_y, const Tone& tone, const Color& blend) {\n \tconst effect_key_type key {\n-\t\tsrc_bitmap,\n+\t\tsrc_bitmap.get(),\n \t\trect,\n \t\tflip_x,\n \t\tflip_y,\n", "changed_method_name": "Cache::SpriteEffect"}
{"commit_url": "https://github.com/CasparCG/server/commit/a7fb293dd8ba4ed5c3cafa3da35dbdf872371d84", "commit_message": "fix(decklink): guard against null video or audio frame from decklink", "code_diff": "@@ -388,15 +388,18 @@ class decklink_producer : public IDeckLinkInputCallback\n         {\n             state_[\"file/name\"]              = model_name_;\n             state_[\"file/path\"]              = device_index_;\n-            state_[\"file/video/width\"]       = video->GetWidth();\n-            state_[\"file/video/height\"]      = video->GetHeight();\n             state_[\"file/audio/sample-rate\"] = format_desc_.audio_sample_rate;\n             state_[\"file/audio/channels\"]    = format_desc_.audio_channels;\n             state_[\"file/fps\"]               = format_desc_.fps;\n             state_[\"profiler/time\"]          = {frame_timer.elapsed(), format_desc_.fps};\n             state_[\"buffer\"]                 = {frame_buffer_.size(), frame_buffer_.capacity()};\n \n-            graph_->set_value(\"frame-time\", frame_timer.elapsed() * format_desc_.fps / format_desc_.field_count * 0.5);\n+            if (video) {\n+                state_[\"file/video/width\"]  = video->GetWidth();\n+                state_[\"file/video/height\"] = video->GetHeight();\n+            }\n+\n+            graph_->set_value(\"frame-time\", frame_timer.elapsed() * format_desc_.fps * 0.5);\n             graph_->set_value(\"output-buffer\",\n                               static_cast<float>(frame_buffer_.size()) / static_cast<float>(frame_buffer_.capacity()));\n         };\n@@ -408,7 +411,7 @@ class decklink_producer : public IDeckLinkInputCallback\n             BMDTimeValue in_video_pts = 0LL;\n             BMDTimeValue in_audio_pts = 0LL;\n \n-            {\n+            if (video) {\n                 auto src    = std::shared_ptr<AVFrame>(av_frame_alloc(), [](AVFrame* ptr) { av_frame_free(&ptr); });\n                 src->format = AV_PIX_FMT_UYVY422;\n                 src->width  = video->GetWidth();\n@@ -418,7 +421,7 @@ class decklink_producer : public IDeckLinkInputCallback\n                 src->key_frame        = 1;\n \n                 void* video_bytes = nullptr;\n-                if (video && SUCCEEDED(video->GetBytes(&video_bytes)) && video_bytes) {\n+                if (SUCCEEDED(video->GetBytes(&video_bytes)) && video_bytes) {\n                     video->AddRef();\n                     src = std::shared_ptr<AVFrame>(src.get(), [src, video](AVFrame* ptr) { video->Release(); });\n \n@@ -439,14 +442,14 @@ class decklink_producer : public IDeckLinkInputCallback\n                 }\n             }\n \n-            {\n+            if (audio) {\n                 auto src             = std::shared_ptr<AVFrame>(av_frame_alloc(), [](AVFrame* ptr) { av_frame_free(&ptr); });\n                 src->format          = AV_SAMPLE_FMT_S32;\n                 src->channels        = format_desc_.audio_channels;\n                 src->sample_rate     = format_desc_.audio_sample_rate;\n \n                 void* audio_bytes = nullptr;\n-                if (audio && SUCCEEDED(audio->GetBytes(&audio_bytes)) && audio_bytes) {\n+                if (SUCCEEDED(audio->GetBytes(&audio_bytes)) && audio_bytes) {\n                     audio->AddRef();\n                     src = std::shared_ptr<AVFrame>(src.get(), [src, audio](AVFrame* ptr) { audio->Release(); });\n                     src->nb_samples  = audio->GetSampleFrameCount();\n@@ -472,6 +475,8 @@ class decklink_producer : public IDeckLinkInputCallback\n                     auto av_video = alloc_frame();\n                     auto av_audio = alloc_frame();\n \n+                    // TODO (fix) this may get stuck if the decklink sends a frame of video or audio\n+\n                     if (av_buffersink_get_frame_flags(video_filter_.sink, av_video.get(), AV_BUFFERSINK_FLAG_PEEK) <\n                         0) {\n                         return S_OK;\n", "changed_method_name": "caspar::decklink::decklink_producer::VideoInputFrameArrived"}
{"commit_url": "https://github.com/mockingbirdnest/Principia/commit/2c7f44c717fc333bd98e8cd2d0748aaba4db7448", "commit_message": "A more useful trace.", "code_diff": "@@ -20,6 +20,8 @@ using base::HexadecimalEncoder;\n using base::UniqueArray;\n using interface::principia__ActivatePlayer;\n \n+using namespace std::chrono_literals;\n+\n namespace journal {\n \n Player::Player(std::filesystem::path const& path)\n@@ -67,8 +69,9 @@ bool Player::Play(int const index) {\n #include \"journal/player.generated.cc\"\n \n   auto const after = std::chrono::system_clock::now();\n-  if (after - before > std::chrono::milliseconds(100)) {\n-    LOG(ERROR) << \"Long method:\\n\" << method_in->DebugString();\n+  if (after - before > 100ms) {\n+    LOG(ERROR) << \"Long method (\" << (after - before) / 1ms << \" ms):\\n\"\n+               << method_in->DebugString();\n   }\n \n   last_method_in_.swap(method_in);\n", "changed_method_name": "principia::journal::Player::Play"}
{"commit_url": "https://github.com/stan-dev/math/commit/b295b04f0d3500e50740e8aff32ff7c22d04af76", "commit_message": "Improve performance of poisson_log_lpmf in some cases", "code_diff": "@@ -41,9 +41,7 @@ return_type_t<T_log_rate> poisson_log_lpmf(const T_n& n,\n   scalar_seq_view<T_log_rate> alpha_vec(alpha);\n   size_t max_size_seq_view = max_size(n, alpha);\n \n-  // FIXME: first loop size of alpha_vec, second loop if-ed for\n-  // max_size_seq_view==1\n-  for (size_t i = 0; i < max_size_seq_view; i++) {\n+  for (size_t i = 0, size_alpha = size(alpha); i < size_alpha; i++) {\n     if (INFTY == alpha_vec[i]) {\n       return LOG_ZERO;\n     }\n@@ -56,20 +54,28 @@ return_type_t<T_log_rate> poisson_log_lpmf(const T_n& n,\n \n   operands_and_partials<T_log_rate> ops_partials(alpha);\n \n-  // FIXME: cache value_of for alpha_vec?  faster if only one?\n+  VectorBuilder<include_summand<propto>::value, T_partials_return, T_n>\n+      lgamma_n_plus_one(size(n));\n+  if (include_summand<propto>::value) {\n+    for (size_t i = 0, size_n = size(n); i < size_n; i++) {\n+      lgamma_n_plus_one[i] = lgamma(n_vec[i] + 1.0);\n+    }\n+  }\n+\n   VectorBuilder<include_summand<propto, T_log_rate>::value, T_partials_return,\n                 T_log_rate>\n       exp_alpha(size(alpha));\n-  for (size_t i = 0; i < size(alpha); i++) {\n+  for (size_t i = 0, size_alpha = size(alpha); i < size_alpha; i++) {\n     exp_alpha[i] = exp(value_of(alpha_vec[i]));\n   }\n \n   for (size_t i = 0; i < max_size_seq_view; i++) {\n-    if (!(alpha_vec[i] == NEGATIVE_INFTY && n_vec[i] == 0)) {\n+    const auto& alpha_val = value_of(alpha_vec[i]);\n+    if (!(alpha_val == NEGATIVE_INFTY && n_vec[i] == 0)) {\n       if (include_summand<propto>::value) {\n-        logp -= lgamma(n_vec[i] + 1.0);\n+        logp -= lgamma_n_plus_one[i];\n       }\n-      logp += n_vec[i] * value_of(alpha_vec[i]) - exp_alpha[i];\n+      logp += n_vec[i] * alpha_val - exp_alpha[i];\n     }\n \n     if (!is_constant_all<T_log_rate>::value) {\n", "changed_method_name": "stan::math::poisson_log_lpmf"}
{"commit_url": "https://github.com/stan-dev/math/commit/75bcadb96cc061cf3dea20c68c5847bbdc09a3ce", "commit_message": "avoid putting un-needed vars on chain stack and place them on nochain stack to reduce chain calls", "code_diff": "@@ -43,7 +43,9 @@ void gradient(const F& f, const Eigen::Matrix<double, Eigen::Dynamic, 1>& x,\n               double& fx, Eigen::Matrix<double, Eigen::Dynamic, 1>& grad_fx) {\n   start_nested();\n   try {\n-    Eigen::Matrix<var, Eigen::Dynamic, 1> x_var(x);\n+    Eigen::Matrix<var, Eigen::Dynamic, 1> x_var(x.size());\n+    for (int i = 0; i < x.size(); ++i)\n+      x_var(i) = var(new vari(x(i), false));\n     var fx_var = f(x_var);\n     fx = fx_var.val();\n     grad_fx.resize(x.size());\n", "changed_method_name": "stan::math::gradient"}
{"commit_url": "https://github.com/godotengine/godot/commit/75dd294732a5670eb1978515a02cac109d169b3b", "commit_message": "Fix emitting duplicate edges for convex hulls\nIdentical to https://github.com/godotengine/godot/pull/52059", "code_diff": "@@ -2260,10 +2260,21 @@ Error ConvexHullComputer::convex_hull(const Vector<Vector3> &p_points, Geometry3\n \n \tr_mesh.vertices = ch.vertices;\n \n-\tr_mesh.edges.resize(ch.edges.size());\n+\t// Copy the edges over. There's two \"half-edges\" for every edge, so we pick only one of them.\n+\tr_mesh.edges.resize(ch.edges.size() / 2);\n+\tuint32_t edges_copied = 0;\n \tfor (uint32_t i = 0; i < ch.edges.size(); i++) {\n-\t\tr_mesh.edges.write[i].a = (&ch.edges[i])->get_source_vertex();\n-\t\tr_mesh.edges.write[i].b = (&ch.edges[i])->get_target_vertex();\n+\t\tuint32_t a = (&ch.edges[i])->get_source_vertex();\n+\t\tuint32_t b = (&ch.edges[i])->get_target_vertex();\n+\t\tif (a < b) { // Copy only the \"canonical\" edge. For the reverse edge, this will be false.\n+\t\t\tERR_BREAK(edges_copied >= (uint32_t)r_mesh.edges.size());\n+\t\t\tr_mesh.edges.write[edges_copied].a = a;\n+\t\t\tr_mesh.edges.write[edges_copied].b = b;\n+\t\t\tedges_copied++;\n+\t\t}\n+\t}\n+\tif (edges_copied != (uint32_t)r_mesh.edges.size()) {\n+\t\tERR_PRINT(\"Invalid edge count.\");\n \t}\n \n \tr_mesh.faces.resize(ch.faces.size());\n", "changed_method_name": "ConvexHullComputer::convex_hull"}
{"commit_url": "https://github.com/godotengine/godot/commit/082f9245668384d5bad79998dee40370c2d38dce", "commit_message": "Updated Tabs to not update excessively", "code_diff": "@@ -98,29 +98,45 @@ void Tabs::gui_input(const Ref<InputEvent> &p_event) {\n \tif (mm.is_valid()) {\n \t\tPoint2 pos = mm->get_position();\n \n-\t\thighlight_arrow = -1;\n \t\tif (buttons_visible) {\n \t\t\tRef<Texture2D> incr = get_theme_icon(SNAME(\"increment\"));\n \t\t\tRef<Texture2D> decr = get_theme_icon(SNAME(\"decrement\"));\n \n \t\t\tif (is_layout_rtl()) {\n \t\t\t\tif (pos.x < decr->get_width()) {\n-\t\t\t\t\thighlight_arrow = 1;\n+\t\t\t\t\tif (highlight_arrow != 1) {\n+\t\t\t\t\t\thighlight_arrow = 1;\n+\t\t\t\t\t\tupdate();\n+\t\t\t\t\t}\n \t\t\t\t} else if (pos.x < incr->get_width() + decr->get_width()) {\n-\t\t\t\t\thighlight_arrow = 0;\n+\t\t\t\t\tif (highlight_arrow != 0) {\n+\t\t\t\t\t\thighlight_arrow = 0;\n+\t\t\t\t\t\tupdate();\n+\t\t\t\t\t}\n+\t\t\t\t} else if (highlight_arrow != -1) {\n+\t\t\t\t\thighlight_arrow = -1;\n+\t\t\t\t\tupdate();\n \t\t\t\t}\n \t\t\t} else {\n \t\t\t\tint limit_minus_buttons = get_size().width - incr->get_width() - decr->get_width();\n \t\t\t\tif (pos.x > limit_minus_buttons + decr->get_width()) {\n-\t\t\t\t\thighlight_arrow = 1;\n+\t\t\t\t\tif (highlight_arrow != 1) {\n+\t\t\t\t\t\thighlight_arrow = 1;\n+\t\t\t\t\t\tupdate();\n+\t\t\t\t\t}\n \t\t\t\t} else if (pos.x > limit_minus_buttons) {\n-\t\t\t\t\thighlight_arrow = 0;\n+\t\t\t\t\tif (highlight_arrow != 0) {\n+\t\t\t\t\t\thighlight_arrow = 0;\n+\t\t\t\t\t\tupdate();\n+\t\t\t\t\t}\n+\t\t\t\t} else if (highlight_arrow != -1) {\n+\t\t\t\t\thighlight_arrow = -1;\n+\t\t\t\t\tupdate();\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n \t\t_update_hover();\n-\t\tupdate();\n \t\treturn;\n \t}\n \n", "changed_method_name": "Tabs::gui_input"}
{"commit_url": "https://github.com/godotengine/godot/commit/27a6ab457b15e448147ab97ab6b23e835ac9e741", "commit_message": "Turn off physics and 3d navigation servers in the Project Manager.\n\nCopying the editor behaviour of deactivating these servers we're not\nusing, to reduce CPU load.", "code_diff": "@@ -52,6 +52,7 @@\n #include \"scene/gui/texture_rect.h\"\n #include \"scene/main/window.h\"\n #include \"servers/display_server.h\"\n+#include \"servers/navigation_server_3d.h\"\n \n static inline String get_project_key_from_path(const String &dir) {\n \treturn dir.replace(\"/\", \"::\");\n@@ -2387,6 +2388,11 @@ ProjectManager::ProjectManager() {\n \t\tEditorSettings::create();\n \t}\n \n+\t// Turn off some servers we aren't going to be using in the Project Manager.\n+\tNavigationServer3D::get_singleton()->set_active(false);\n+\tPhysicsServer3D::get_singleton()->set_active(false);\n+\tPhysicsServer2D::get_singleton()->set_active(false);\n+\n \tEditorSettings::get_singleton()->set_optimize_save(false); //just write settings as they came\n \n \t{\n", "changed_method_name": "ProjectManager::ProjectManager"}
{"commit_url": "https://github.com/godotengine/godot/commit/ba431a9306f6f018488b6d19854ea40fa528f205", "commit_message": "Fix volumetric fog memory leak on resize", "code_diff": "@@ -4011,6 +4011,9 @@ void RendererSceneRenderRD::_volumetric_fog_erase(RenderBuffers *rb) {\n \tRD::get_singleton()->free(rb->volumetric_fog->prev_light_density_map);\n \tRD::get_singleton()->free(rb->volumetric_fog->light_density_map);\n \tRD::get_singleton()->free(rb->volumetric_fog->fog_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->density_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->light_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->emissive_map);\n \n \tif (rb->volumetric_fog->fog_uniform_set.is_valid() && RD::get_singleton()->uniform_set_is_valid(rb->volumetric_fog->fog_uniform_set)) {\n \t\tRD::get_singleton()->free(rb->volumetric_fog->fog_uniform_set);\n", "changed_method_name": "RendererSceneRenderRD::_volumetric_fog_erase"}
{"commit_url": "https://github.com/godotengine/godot/commit/c37bd41c794819e0b6bc3ad4b162548057098e1c", "commit_message": "Increase RemoteDebuggerPeerTCP poll to 6.9ms\n\nFix high CPU usage on MacOS by reverting the polling for Network\ndebugging to match 144hz refresh rate.", "code_diff": "@@ -190,7 +190,8 @@ Error RemoteDebuggerPeerTCP::connect_to_host(const String &p_host, uint16_t p_po\n }\n \n void RemoteDebuggerPeerTCP::_thread_func(void *p_ud) {\n-\tconst uint64_t min_tick = 100;\n+\t// Update in time for 144hz monitors\n+\tconst uint64_t min_tick = 6900;\n \tRemoteDebuggerPeerTCP *peer = (RemoteDebuggerPeerTCP *)p_ud;\n \twhile (peer->running && peer->is_peer_connected()) {\n \t\tuint64_t ticks_usec = OS::get_singleton()->get_ticks_usec();\n", "changed_method_name": "RemoteDebuggerPeerTCP::_thread_func"}
{"commit_url": "https://github.com/godotengine/godot/commit/66d27df12f5b710ac5f569144944c1db0c86c96b", "commit_message": "Fix 3D sky update performance regression", "code_diff": "@@ -1172,7 +1172,7 @@ void RendererSceneSkyRD::setup(RendererSceneEnvironmentRD *p_env, RID p_render_b\n \t\t\t\t}\n \t\t\t}\n \t\t\t// Check whether the directional_light_buffer changes\n-\t\t\tbool light_data_dirty = true;\n+\t\t\tbool light_data_dirty = false;\n \n \t\t\t// Light buffer is dirty if we have fewer or more lights\n \t\t\t// If we have fewer lights, make sure that old lights are disabled\n", "changed_method_name": "RendererSceneSkyRD::setup"}
{"commit_url": "https://github.com/godotengine/godot/commit/ddc8ec6b448edd69642d61c4827882a035498274", "commit_message": "Limit inspector updates when dragging anchored controls", "code_diff": "@@ -1585,14 +1585,25 @@ void Control::set_anchor_and_offset(Side p_side, real_t p_anchor, real_t p_pos,\n }\n \n void Control::_set_anchors_layout_preset(int p_preset) {\n-\tset_meta(\"_edit_layout_mode\", (int)LayoutMode::LAYOUT_MODE_ANCHORS);\n+\tbool list_changed = false;\n+\n+\tif (has_meta(\"_edit_layout_mode\") && (int)get_meta(\"_edit_layout_mode\") != (int)LayoutMode::LAYOUT_MODE_ANCHORS) {\n+\t\tlist_changed = true;\n+\t\tset_meta(\"_edit_layout_mode\", (int)LayoutMode::LAYOUT_MODE_ANCHORS);\n+\t}\n \n \tif (p_preset == -1) {\n-\t\tset_meta(\"_edit_use_custom_anchors\", true);\n-\t\tnotify_property_list_changed();\n+\t\tif (!has_meta(\"_edit_use_custom_anchors\") || !(bool)get_meta(\"_edit_use_custom_anchors\")) {\n+\t\t\tset_meta(\"_edit_use_custom_anchors\", true);\n+\t\t\tnotify_property_list_changed();\n+\t\t}\n \t\treturn; // Keep settings as is.\n \t}\n-\tset_meta(\"_edit_use_custom_anchors\", false);\n+\n+\tif (!has_meta(\"_edit_use_custom_anchors\") || (bool)get_meta(\"_edit_use_custom_anchors\")) {\n+\t\tlist_changed = true;\n+\t\tset_meta(\"_edit_use_custom_anchors\", false);\n+\t}\n \n \tLayoutPreset preset = (LayoutPreset)p_preset;\n \t// Set correct anchors.\n@@ -1625,7 +1636,9 @@ void Control::_set_anchors_layout_preset(int p_preset) {\n \t// Select correct grow directions.\n \tset_grow_direction_preset(preset);\n \n-\tnotify_property_list_changed();\n+\tif (list_changed) {\n+\t\tnotify_property_list_changed();\n+\t}\n }\n \n int Control::_get_anchors_layout_preset() const {\n", "changed_method_name": "Control::_set_anchors_layout_preset"}
{"commit_url": "https://github.com/godotengine/godot/commit/ab5eaf0ad97cfac2d3bc924ae99c5fc83cd3e838", "commit_message": "Fix new performance regressions (short delay_usec)\n\nMy Mac was using 20% cpu again, which was related to the Javascript\nExport plugin.\n\nI had however no export templates setup in the project so this is more\nof a stopgap fix.", "code_diff": "@@ -645,7 +645,7 @@ Ref<Texture2D> EditorExportPlatformJavaScript::get_run_icon() const {\n void EditorExportPlatformJavaScript::_server_thread_poll(void *data) {\n \tEditorExportPlatformJavaScript *ej = (EditorExportPlatformJavaScript *)data;\n \twhile (!ej->server_quit) {\n-\t\tOS::get_singleton()->delay_usec(1000);\n+\t\tOS::get_singleton()->delay_usec(6900);\n \t\t{\n \t\t\tMutexLock lock(ej->server_lock);\n \t\t\tej->server->poll();\n", "changed_method_name": "EditorExportPlatformJavaScript::_server_thread_poll"}
{"commit_url": "https://github.com/godotengine/godot/commit/32b16c876b92c3dae35046d37740fc6e5cc65b24", "commit_message": "[Net] Fix SceneReplicationConfig setter.\n\nUsed by resource loader, it would always add properties as both sync and\nspawn, disregarding the actual option value.", "code_diff": "@@ -52,11 +52,19 @@ bool SceneReplicationConfig::_set(const StringName &p_name, const Variant &p_val\n \t\tReplicationProperty &prop = properties[idx];\n \t\tif (what == \"sync\") {\n \t\t\tprop.sync = p_value;\n-\t\t\tsync_props.push_back(prop.name);\n+\t\t\tif (prop.sync) {\n+\t\t\t\tsync_props.push_back(prop.name);\n+\t\t\t} else {\n+\t\t\t\tsync_props.erase(prop.name);\n+\t\t\t}\n \t\t\treturn true;\n \t\t} else if (what == \"spawn\") {\n \t\t\tprop.spawn = p_value;\n-\t\t\tspawn_props.push_back(prop.name);\n+\t\t\tif (prop.spawn) {\n+\t\t\t\tspawn_props.push_back(prop.name);\n+\t\t\t} else {\n+\t\t\t\tspawn_props.erase(prop.name);\n+\t\t\t}\n \t\t\treturn true;\n \t\t}\n \t}\n", "changed_method_name": "SceneReplicationConfig::_set"}
{"commit_url": "https://github.com/godotengine/godot/commit/31712cc9e705e49e217fca98adae40e9bbda9846", "commit_message": "Address slow copy performance when using the `FileAccessFilesystemJAndroid` implementation.\n\nRead/write ops for this implementation are done through the java layer via jni, and so for good performance, it's key to avoid numerous repeated small read/write ops due the jni overhead.\n\nThe alternative is to allocate a (conversatively-sized) large buffer to reduce the number of read/write ops over the jni boundary.", "code_diff": "@@ -34,6 +34,7 @@\n #include \"core/io/file_access.h\"\n #include \"core/os/memory.h\"\n #include \"core/os/os.h\"\n+#include \"core/templates/local_vector.h\"\n \n String DirAccess::_get_root_path() const {\n \tswitch (_access_type) {\n@@ -286,11 +287,16 @@ Error DirAccess::copy(String p_from, String p_to, int p_chmod_flags) {\n \t\tRef<FileAccess> fdst = FileAccess::open(p_to, FileAccess::WRITE, &err);\n \t\tERR_FAIL_COND_V_MSG(err != OK, err, \"Failed to open \" + p_to);\n \n+\t\tconst size_t copy_buffer_limit = 65536; // 64 KB\n+\n \t\tfsrc->seek_end(0);\n \t\tint size = fsrc->get_position();\n \t\tfsrc->seek(0);\n \t\terr = OK;\n-\t\twhile (size--) {\n+\t\tsize_t buffer_size = MIN(size * sizeof(uint8_t), copy_buffer_limit);\n+\t\tLocalVector<uint8_t> buffer;\n+\t\tbuffer.resize(buffer_size);\n+\t\twhile (size > 0) {\n \t\t\tif (fsrc->get_error() != OK) {\n \t\t\t\terr = fsrc->get_error();\n \t\t\t\tbreak;\n@@ -300,7 +306,14 @@ Error DirAccess::copy(String p_from, String p_to, int p_chmod_flags) {\n \t\t\t\tbreak;\n \t\t\t}\n \n-\t\t\tfdst->store_8(fsrc->get_8());\n+\t\t\tint bytes_read = fsrc->get_buffer(buffer.ptr(), buffer_size);\n+\t\t\tif (bytes_read <= 0) {\n+\t\t\t\terr = FAILED;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tfdst->store_buffer(buffer.ptr(), bytes_read);\n+\n+\t\t\tsize -= bytes_read;\n \t\t}\n \t}\n \n", "changed_method_name": "DirAccess::copy"}
{"commit_url": "https://github.com/godotengine/godot/commit/0cbd1c85a96384ef93978661e408ce7c1b721847", "commit_message": "Fix burning CPU with udev disabled on Flatpak\n\nFixes #67355.", "code_diff": "@@ -218,8 +218,8 @@ void JoypadLinux::monitor_joypads() {\n \t\t\t}\n \t\t}\n \t\tclosedir(input_directory);\n+\t\tusleep(1000000); // 1s\n \t}\n-\tusleep(1000000); // 1s\n }\n \n void JoypadLinux::close_joypads() {\n", "changed_method_name": "JoypadLinux::monitor_joypads"}
{"commit_url": "https://github.com/godotengine/godot/commit/d44a91c2f78809365020a67647a2c94ccab418ad", "commit_message": "use depth prepass to increase performance", "code_diff": "@@ -256,6 +256,7 @@ void SceneShaderForwardClustered::ShaderData::set_code(const String &p_code) {\n \t\tdepth_stencil_state.depth_compare_operator = RD::COMPARE_OP_LESS_OR_EQUAL;\n \t\tdepth_stencil_state.enable_depth_write = depth_draw != DEPTH_DRAW_DISABLED ? true : false;\n \t}\n+\tbool depth_pre_pass_enabled = bool(GLOBAL_GET(\"rendering/driver/depth_prepass/enable\"));\n \n \tfor (int i = 0; i < CULL_VARIANT_MAX; i++) {\n \t\tRD::PolygonCullMode cull_mode_rd_table[CULL_VARIANT_MAX][3] = {\n@@ -307,8 +308,16 @@ void SceneShaderForwardClustered::ShaderData::set_code(const String &p_code) {\n \t\t\t\t\t\t\tcontinue;\n \t\t\t\t\t\t}\n \n-\t\t\t\t\t\tRD::PipelineColorBlendState blend_state;\n \t\t\t\t\t\tRD::PipelineDepthStencilState depth_stencil = depth_stencil_state;\n+\t\t\t\t\t\tif (depth_pre_pass_enabled && casts_shadows()) {\n+\t\t\t\t\t\t\t// We already have a depth from the depth pre-pass, there is no need to write it again.\n+\t\t\t\t\t\t\t// In addition we can use COMPARE_OP_EQUAL instead of COMPARE_OP_LESS_OR_EQUAL.\n+\t\t\t\t\t\t\t// This way we can use the early depth test to discard transparent fragments before the fragment shader even starts.\n+\t\t\t\t\t\t\tdepth_stencil.depth_compare_operator = RD::COMPARE_OP_EQUAL;\n+\t\t\t\t\t\t\tdepth_stencil.enable_depth_write = false;\n+\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\tRD::PipelineColorBlendState blend_state;\n \t\t\t\t\t\tRD::PipelineMultisampleState multisample_state;\n \n \t\t\t\t\t\tint shader_flags = 0;\n", "changed_method_name": "SceneShaderForwardClustered::ShaderData::set_code"}
{"commit_url": "https://github.com/godotengine/godot/commit/51777a2914dc46f2831752b74db6276369df0905", "commit_message": "Fix constant editor redraw after shortcut\n\nIntroduced by me by mistake on #71328. Fixes #71652.", "code_diff": "@@ -385,6 +385,7 @@ void BaseButton::shortcut_input(const Ref<InputEvent> &p_event) {\n \t\tif (shortcut_feedback) {\n \t\t\tif (shortcut_feedback_timer == nullptr) {\n \t\t\t\tshortcut_feedback_timer = memnew(Timer);\n+\t\t\t\tshortcut_feedback_timer->set_one_shot(true);\n \t\t\t\tadd_child(shortcut_feedback_timer);\n \t\t\t\tshortcut_feedback_timer->set_wait_time(GLOBAL_GET(\"gui/timers/button_shortcut_feedback_highlight_time\"));\n \t\t\t\tshortcut_feedback_timer->connect(\"timeout\", callable_mp(this, &BaseButton::_shortcut_feedback_timeout));\n", "changed_method_name": "BaseButton::shortcut_input"}
{"commit_url": "https://github.com/godotengine/godot/commit/340c3b84fd9f06f4d4a794c0a0b8e95165aeee9f", "commit_message": "Set Default compression to VRAM uncompressed for LightmapGI\n\nThis increases the speed to be near instant and removes the perceived lightmap bake speed regression\n\nWe need to investigate the speed and quality issues with BPTC and re-enable compression when we can", "code_diff": "@@ -163,7 +163,8 @@ Array LightmapGIData::_get_light_textures_data() const {\n \t\tconfig->set_value(\"remap\", \"importer\", \"2d_array_texture\");\n \t\tconfig->set_value(\"remap\", \"type\", \"CompressedTexture2DArray\");\n \t\tif (!config->has_section_key(\"params\", \"compress/mode\")) {\n-\t\t\tconfig->set_value(\"params\", \"compress/mode\", 2); //user may want another compression, so leave it be\n+\t\t\t// User may want another compression, so leave it be, but default to VRAM uncompressed.\n+\t\t\tconfig->set_value(\"params\", \"compress/mode\", 3);\n \t\t}\n \t\tconfig->set_value(\"params\", \"compress/channel_pack\", 1);\n \t\tconfig->set_value(\"params\", \"mipmaps/generate\", false);\n", "changed_method_name": "LightmapGIData::_get_light_textures_data"}
{"commit_url": "https://github.com/godotengine/godot/commit/f84c6df8d1aec35fe53521f241b26fc5312d26e3", "commit_message": "Use DXT1 when compressing PNGs with RGB format\n\nThis results in much smaller file sizes with the same quality", "code_diff": "@@ -66,7 +66,7 @@ EtcpakType _determine_dxt_type(Image::UsedChannels p_channels) {\n \t\tcase Image::USED_CHANNELS_RG:\n \t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5_RA_AS_RG;\n \t\tcase Image::USED_CHANNELS_RGB:\n-\t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5;\n+\t\t\treturn EtcpakType::ETCPAK_TYPE_DXT1;\n \t\tcase Image::USED_CHANNELS_RGBA:\n \t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5;\n \t\tdefault:\n", "changed_method_name": "_determine_dxt_type"}
{"commit_url": "https://github.com/godotengine/godot/commit/e5bebbc9ffda8b4fa5a786842ea24f0a3ea0763a", "commit_message": "Fix unnecessary break when calculating the height of visible lines\n\nThis break causes the minsize to be smaller than expected, and then\nthe size keeps increasing by one line to cover all visible lines.\nThis can cause performance issues when there are many visible lines.", "code_diff": "@@ -290,9 +290,6 @@ void Label::_update_visible() {\n \tint last_line = MIN(lines_rid.size(), lines_visible + lines_skipped);\n \tfor (int64_t i = lines_skipped; i < last_line; i++) {\n \t\tminsize.height += TS->shaped_text_get_size(lines_rid[i]).y + line_spacing;\n-\t\tif (minsize.height > (get_size().height - style->get_minimum_size().height + line_spacing)) {\n-\t\t\tbreak;\n-\t\t}\n \t}\n }\n \n", "changed_method_name": "Label::_update_visible"}
{"commit_url": "https://github.com/godotengine/godot/commit/8cdab04d7fd57aaabd790349cd8a4e9ec21a7edd", "commit_message": "Fix that the focus-out notification got sent deferred\n\nCurrently the window receives a focus-out notification, directly after\nit popup, because currently the signal is sent deferred.\nThe original intention was that the previously focused window must\nreceive a focus-out notification.\nThis change makes the notification more precise by only sending the\nfocus-out to the previously focused window.", "code_diff": "@@ -1624,7 +1624,15 @@ void Window::popup(const Rect2i &p_screen_rect) {\n \t\t// Send a focus-out notification when opening a Window Manager Popup.\n \t\tSceneTree *scene_tree = get_tree();\n \t\tif (scene_tree) {\n-\t\t\tscene_tree->notify_group_flags(SceneTree::GROUP_CALL_DEFERRED, \"_viewports\", NOTIFICATION_WM_WINDOW_FOCUS_OUT);\n+\t\t\tList<Node *> list;\n+\t\t\tscene_tree->get_nodes_in_group(\"_viewports\", &list);\n+\t\t\tfor (Node *n : list) {\n+\t\t\t\tWindow *w = Object::cast_to<Window>(n);\n+\t\t\t\tif (w && !w->get_embedder() && w->has_focus()) {\n+\t\t\t\t\tw->_event_callback(DisplayServer::WINDOW_EVENT_FOCUS_OUT);\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \t}\n \n", "changed_method_name": "Window::popup"}
{"commit_url": "https://github.com/godotengine/godot/commit/bc1aef88eef7bb09d1c537bf150414c50abfa374", "commit_message": "SCons: Disable misbehaving MSVC incremental linking\n\nFixes #77968.", "code_diff": "@@ -355,6 +355,9 @@ def configure_msvc(env, vcvars_msvc_config):\n         else:\n             env.AppendUnique(CCFLAGS=[\"/MD\"])\n \n+    # MSVC incremental linking is broken and _increases_ link time (GH-77968).\n+    env.Append(LINKFLAGS=[\"/INCREMENTAL:NO\"])\n+\n     if env[\"arch\"] == \"x86_32\":\n         env[\"x86_libtheora_opt_vc\"] = True\n \n", "changed_method_name": "configure_msvc"}
{"commit_url": "https://github.com/godotengine/godot/commit/98db2b42f7352193a9e76573f910180bfe515212", "commit_message": "fix performance hit due to enabling collision\n\nSigned-off-by: Saif Kandil <74428638+k0T0z@users.noreply.github.com>", "code_diff": "@@ -984,12 +984,16 @@ void ParticleProcessMaterial::_update_shader() {\n \t\t}\n \t\tcode += \"\t\t\\n\";\n \t\tcode += \"\t\tvec3 noise_direction = get_noise_direction(TRANSFORM[3].xyz);\\n\";\n-\t\tcode += \"\t\tif (!COLLIDED) {\\n\";\n-\t\tcode += \"\t\t\t\\n\";\n-\t\tcode += \"\t\t\tfloat vel_mag = length(final_velocity);\\n\";\n-\t\tcode += \"\t\t\tfloat vel_infl = clamp(dynamic_params.turb_influence * turbulence_influence, 0.0,1.0);\\n\";\n-\t\tcode += \"\t\t\tfinal_velocity = mix(final_velocity, normalize(noise_direction) * vel_mag * (1.0 + (1.0 - vel_infl) * 0.2), vel_infl);\\n\";\n-\t\tcode += \"\t\t}\\n\";\n+\t\t// The following snippet causes massive performance hit. We don't need it as long as collision is disabled.\n+\t\t// Refer to GH-83744 for more info.\n+\t\tif (collision_mode != COLLISION_DISABLED) {\n+\t\t\tcode += \"\t\tif (!COLLIDED) {\\n\";\n+\t\t\tcode += \"\t\t\t\\n\";\n+\t\t\tcode += \"\t\t\tfloat vel_mag = length(final_velocity);\\n\";\n+\t\t\tcode += \"\t\t\tfloat vel_infl = clamp(dynamic_params.turb_influence * turbulence_influence, 0.0,1.0);\\n\";\n+\t\t\tcode += \"\t\t\tfinal_velocity = mix(final_velocity, normalize(noise_direction) * vel_mag * (1.0 + (1.0 - vel_infl) * 0.2), vel_infl);\\n\";\n+\t\t\tcode += \"\t\t}\\n\";\n+\t\t}\n \t}\n \tcode += \"\t\\n\";\n \tcode += \"\t// limit velocity\\n\";\n", "changed_method_name": "ParticleProcessMaterial::_update_shader"}
{"commit_url": "https://github.com/godotengine/godot/commit/085255bd0ed2c36b217769c6f75c8ea856725b1a", "commit_message": "Improved X11 screen_get_refresh_rate performance", "code_diff": "@@ -1571,7 +1571,7 @@ float DisplayServerX11::screen_get_refresh_rate(int p_screen) const {\n \n \t//Use xrandr to get screen refresh rate.\n \tif (xrandr_ext_ok) {\n-\t\tXRRScreenResources *screen_info = XRRGetScreenResources(x11_display, windows[MAIN_WINDOW_ID].x11_window);\n+\t\tXRRScreenResources *screen_info = XRRGetScreenResourcesCurrent(x11_display, windows[MAIN_WINDOW_ID].x11_window);\n \t\tif (screen_info) {\n \t\t\tRRMode current_mode = 0;\n \t\t\txrr_monitor_info *monitors = nullptr;\n", "changed_method_name": "DisplayServerX11::screen_get_refresh_rate"}
{"commit_url": "https://github.com/godotengine/godot/commit/3a67eb26754d779b308d478e92b5d12252c70a5a", "commit_message": "Disable a prohibitively slow code branch when reparenting nodes", "code_diff": "@@ -1762,6 +1762,8 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t} break;\n \n+// FIXME: This approach causes a significant performance regression, see GH-84910.\n+#if 0\n \t\tcase Variant::OBJECT: {\n \t\t\tResource *resource = Object::cast_to<Resource>(r_variant);\n \t\t\tif (!resource) {\n@@ -1792,6 +1794,7 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t\tbreak;\n \t\t};\n+#endif\n \n \t\tdefault: {\n \t\t}\n", "changed_method_name": "SceneTreeDock::_check_node_path_recursive"}
{"commit_url": "https://github.com/godotengine/godot/commit/5dd11e8eee739f2ac3bbecd93cd4862c151aebc8", "commit_message": "Limit window size updates on title change.", "code_diff": "@@ -284,7 +284,13 @@ void Window::set_title(const String &p_title) {\n \t\tembedder->_sub_window_update(this);\n \t} else if (window_id != DisplayServer::INVALID_WINDOW_ID) {\n \t\tDisplayServer::get_singleton()->window_set_title(tr_title, window_id);\n-\t\t_update_window_size();\n+\t\tif (keep_title_visible) {\n+\t\t\tSize2i title_size = DisplayServer::get_singleton()->window_get_title_size(tr_title, window_id);\n+\t\t\tSize2i size_limit = get_clamped_minimum_size();\n+\t\t\tif (title_size.x > size_limit.x || title_size.y > size_limit.y) {\n+\t\t\t\t_update_window_size();\n+\t\t\t}\n+\t\t}\n \t}\n }\n \n", "changed_method_name": "Window::set_title"}
{"commit_url": "https://github.com/godotengine/godot/commit/829349d2ca6e49eaf0703154467ee8020484b387", "commit_message": "Do not reload resources and send notification if locale is not changed.", "code_diff": "@@ -518,8 +518,12 @@ String TranslationServer::get_country_name(const String &p_country) const {\n }\n \n void TranslationServer::set_locale(const String &p_locale) {\n-\tlocale = standardize_locale(p_locale);\n+\tString new_locale = standardize_locale(p_locale);\n+\tif (locale == new_locale) {\n+\t\treturn;\n+\t}\n \n+\tlocale = new_locale;\n \tResourceLoader::reload_translation_remaps();\n \n \tif (OS::get_singleton()->get_main_loop()) {\n", "changed_method_name": "TranslationServer::set_locale"}
{"commit_url": "https://github.com/godotengine/godot/commit/f9c42d9fffa8244eb84f6f0ab1f3d5f546b59c11", "commit_message": "Limit window size updates on title translation change.", "code_diff": "@@ -1301,7 +1301,13 @@ void Window::_notification(int p_what) {\n \n \t\t\tif (!embedder && window_id != DisplayServer::INVALID_WINDOW_ID) {\n \t\t\t\tDisplayServer::get_singleton()->window_set_title(tr_title, window_id);\n-\t\t\t\t_update_window_size();\n+\t\t\t\tif (keep_title_visible) {\n+\t\t\t\t\tSize2i title_size = DisplayServer::get_singleton()->window_get_title_size(tr_title, window_id);\n+\t\t\t\t\tSize2i size_limit = get_clamped_minimum_size();\n+\t\t\t\t\tif (title_size.x > size_limit.x || title_size.y > size_limit.y) {\n+\t\t\t\t\t\t_update_window_size();\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \t\t} break;\n \n", "changed_method_name": "Window::_notification"}
{"commit_url": "https://github.com/godotengine/godot/commit/77879d4288443fcacb0668bd867438b53bd800ae", "commit_message": "Update NodePaths only in built-in resources", "code_diff": "@@ -1812,8 +1812,6 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t} break;\n \n-// FIXME: This approach causes a significant performance regression, see GH-84910.\n-#if 0\n \t\tcase Variant::OBJECT: {\n \t\t\tResource *resource = Object::cast_to<Resource>(r_variant);\n \t\t\tif (!resource) {\n@@ -1825,6 +1823,11 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t\tbreak;\n \t\t\t}\n \n+\t\t\tif (!resource->is_built_in()) {\n+\t\t\t\t// For performance reasons, assume that scene paths are no concern for external resources.\n+\t\t\t\tbreak;\n+\t\t\t}\n+\n \t\t\tList<PropertyInfo> properties;\n \t\t\tresource->get_property_list(&properties);\n \n@@ -1841,9 +1844,7 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t\t\tundo_redo->add_undo_property(resource, propertyname, old_variant);\n \t\t\t\t}\n \t\t\t}\n-\t\t\tbreak;\n-\t\t};\n-#endif\n+\t\t} break;\n \n \t\tdefault: {\n \t\t}\n", "changed_method_name": "SceneTreeDock::_check_node_path_recursive"}
{"commit_url": "https://github.com/godotengine/godot/commit/b9225f67c85c3eab47df96b9fa1bd5a5034eae53", "commit_message": "Warn that navigation mesh baking from Meshes is bad for runtime performance\n\nWarns that navigation mesh baking from Meshes is bad for runtime performance.", "code_diff": "@@ -158,6 +158,15 @@ void NavigationMeshSourceGeometryData3D::_add_faces(const PackedVector3Array &p_\n \n void NavigationMeshSourceGeometryData3D::add_mesh(const Ref<Mesh> &p_mesh, const Transform3D &p_xform) {\n \tERR_FAIL_COND(!p_mesh.is_valid());\n+\n+#ifdef DEBUG_ENABLED\n+\tif (!Engine::get_singleton()->is_editor_hint()) {\n+\t\tWARN_PRINT_ONCE(\"Source geometry parsing for navigation mesh baking had to parse RenderingServer meshes at runtime.\\n\\\n+\t\tThis poses a significant performance issues as visual meshes store geometry data on the GPU and transferring this data back to the CPU blocks the rendering.\\n\\\n+\t\tFor runtime (re)baking navigation meshes use and parse collision shapes as source geometry or create geometry data procedurally in scripts.\");\n+\t}\n+#endif\n+\n \t_add_mesh(p_mesh, root_node_transform * p_xform);\n }\n \n", "changed_method_name": "NavigationMeshSourceGeometryData3D::add_mesh"}
{"commit_url": "https://github.com/godotengine/godot/commit/e201e5bf30bdd7ea4f515ebf8d47a70172683f7e", "commit_message": "Fix Windows Activate Process", "code_diff": "@@ -4727,6 +4727,7 @@ LRESULT DisplayServerWindows::WndProc(HWND hWnd, UINT uMsg, WPARAM wParam, LPARA\n \t\t} break;\n \t\tcase WM_EXITSIZEMOVE: {\n \t\t\tKillTimer(windows[window_id].hWnd, windows[window_id].move_timer_id);\n+\t\t\twindows[window_id].move_timer_id = 0;\n \t\t} break;\n \t\tcase WM_TIMER: {\n \t\t\tif (wParam == windows[window_id].move_timer_id) {\n", "changed_method_name": "DisplayServerWindows::WndProc"}
{"commit_url": "https://github.com/godotengine/godot/commit/4b266871d7ff6be7c1e73ecad07aac0954c85236", "commit_message": "CVTT: Include float formats for hdr compression", "code_diff": "@@ -149,7 +149,7 @@ void image_compress_cvtt(Image *p_image, Image::UsedChannels p_channels) {\n \tint h = p_image->get_height();\n \n \tbool is_ldr = (p_image->get_format() <= Image::FORMAT_RGBA8);\n-\tbool is_hdr = (p_image->get_format() >= Image::FORMAT_RH) && (p_image->get_format() <= Image::FORMAT_RGBE9995);\n+\tbool is_hdr = (p_image->get_format() >= Image::FORMAT_RF) && (p_image->get_format() <= Image::FORMAT_RGBE9995);\n \n \tif (!is_ldr && !is_hdr) {\n \t\treturn; // Not a usable source format\n", "changed_method_name": "image_compress_cvtt"}
{"commit_url": "https://github.com/godotengine/godot/commit/97085358ce154f34d6ab571c35529a8b0e063513", "commit_message": "[Editor] Prevent unnecessary editor theme regeneration on unrelated system setting update.", "code_diff": "@@ -670,7 +670,10 @@ void EditorNode::_notification(int p_what) {\n \n \t\t\tcallable_mp(this, &EditorNode::_begin_first_scan).call_deferred();\n \n-\t\t\tDisplayServer::get_singleton()->set_system_theme_change_callback(callable_mp(this, &EditorNode::_update_theme).bind(false));\n+\t\t\tlast_dark_mode_state = DisplayServer::get_singleton()->is_dark_mode();\n+\t\t\tlast_system_accent_color = DisplayServer::get_singleton()->get_accent_color();\n+\t\t\tlast_system_base_color = DisplayServer::get_singleton()->get_base_color();\n+\t\t\tDisplayServer::get_singleton()->set_system_theme_change_callback(callable_mp(this, &EditorNode::_check_system_theme_changed));\n \n \t\t\t/* DO NOT LOAD SCENES HERE, WAIT FOR FILE SCANNING AND REIMPORT TO COMPLETE */\n \t\t} break;\n", "changed_method_name": "EditorNode::_notification"}
{"commit_url": "https://github.com/opencv/opencv/commit/333e53eee896ee15247eea8e0c7f71ea44831eb6", "commit_message": "Minor improvement to SSE code in HOGDescriptor::computeGradient, replace emulation of _mm_mullo_epi32 with constant multiplicand 3 with two _mm_add_epi32. OpenCV issue #11161", "code_diff": "@@ -323,14 +323,10 @@ void HOGDescriptor::computeGradient(const Mat& img, Mat& grad, Mat& qangle,\n         int end = gradsize.width + 2;\n         xmap -= 1, x = 0;\n #if CV_SSE2\n-        __m128i ithree = _mm_set1_epi32(3);\n         for ( ; x <= end - 4; x += 4)\n         {\n-            //emulation of _mm_mullo_epi32\n             __m128i mul_res = _mm_loadu_si128((const __m128i*)(xmap + x));\n-            __m128i tmp1 = _mm_mul_epu32(ithree, mul_res);\n-            __m128i tmp2 = _mm_mul_epu32( _mm_srli_si128(ithree,4), _mm_srli_si128(mul_res,4));\n-            mul_res = _mm_unpacklo_epi32(_mm_shuffle_epi32(tmp1, _MM_SHUFFLE (0,0,2,0)), _mm_shuffle_epi32(tmp2, _MM_SHUFFLE (0,0,2,0)));\n+            mul_res = _mm_add_epi32(_mm_add_epi32(mul_res, mul_res), mul_res); // multiply by 3\n             _mm_storeu_si128((__m128i*)(xmap + x), mul_res);\n         }\n #elif CV_NEON\n", "changed_method_name": "cv::HOGDescriptor::computeGradient"}
{"commit_url": "https://github.com/opencv/opencv/commit/1ce5a724c73b26c4ec40f11c53eec22ac27b5031", "commit_message": "Fixed StereoBM uniqueness check", "code_diff": "@@ -558,7 +558,10 @@ static void findStereoCorrespondenceBM_SIMD( const Mat& left, const Mat& right,\n                 {\n                     v_int32 sad4_l = vx_load_expand((short*)sad + d);\n                     if (v_check_any((thresh4 > sad4_l) & ((d1 > d4) | (d4 > d2))))\n+                    {\n+                        dptr[y*dstep] = FILTERED;\n                         continue;\n+                    }\n                     d += v_int16::nlanes;\n                 }\n                 for( ; d < ndisp; d++ )\n", "changed_method_name": "cv::findStereoCorrespondenceBM_SIMD"}
{"commit_url": "https://github.com/opencv/opencv/commit/ac177b849c2860d7469fcd0dda6edf6a18c997b6", "commit_message": "Improve initialization performance of Brisk\n\nreformatting\n\nImprove initialization performance of Brisk\n\nfix formatting\n\nImprove initialization performance of Brisk\n\nformatting\n\nImprove initialization performance of Brisk\n\nmake a lookup table for ring\n\nuse cosine/sine lookup table for theta in brisk and utilize trig identity\n\nfix ring lookup table\n\nuse cosine/sine lookup table for theta in brisk and utilize trig identity\n\nformatting\n\nuse cosine/sine lookup table for theta in brisk and utilize trig identity\n\nmove scale radius product to ring loop to ensure it's not recomputed for each rot\n\nrevert change\n\nmove scale radius product to ring loop to ensure it's not recomputed for each rot\n\nremove rings lookup table\n\nmove scale radius product to ring loop to ensure it's not recomputed for each rot\n\nfix formatting of for loop\n\nmove scale radius product to ring loop to ensure it's not recomputed for each rot\n\nuse sine/cosine approximations for brisk lookup table.\n\nadd documentation for sine/cosine lookup tables\n\nImprove initialization performance of BRISK", "code_diff": "@@ -353,13 +353,30 @@ BRISK_Impl::generateKernel(const std::vector<float> &radiusList,\n   const int rings = (int)radiusList.size();\n   CV_Assert(radiusList.size() != 0 && radiusList.size() == numberList.size());\n   points_ = 0; // remember the total number of points\n+  double sineThetaLookupTable[n_rot_];\n+  double cosThetaLookupTable[n_rot_];\n   for (int ring = 0; ring < rings; ring++)\n   {\n     points_ += numberList[ring];\n   }\n+\n+  // using a sine/cosine approximation for the lookup table\n+  // utilizes the trig identities:\n+  // sin(a + b) = sin(a)cos(b) + cos(a)sin(b)\n+  // cos(a + b) = cos(a)cos(b) - sin(a)sin(b)\n+  // and the fact that sin(0) = 0, cos(0) = 1\n+  double cosval = 1., sinval = 0.;\n+  double dcos = cos(2*CV_PI/double(n_rot_)), dsin = sin(2*CV_PI/double(n_rot_));\n+  for( size_t rot = 0; rot < n_rot_; ++rot)\n+  {\n+    sineThetaLookupTable[rot] = sinval;\n+    cosThetaLookupTable[rot] = cosval;\n+    double t = sinval*dcos + cosval*dsin;\n+    cosval = cosval*dcos - sinval*dsin;\n+    sinval = t;\n+  }\n   // set up the patterns\n   patternPoints_ = new BriskPatternPoint[points_ * scales_ * n_rot_];\n-  BriskPatternPoint* patternIterator = patternPoints_;\n \n   // define the scale discretization:\n   static const float lb_scale = (float)(std::log(scalerange_) / std::log(2.0));\n@@ -370,46 +387,51 @@ BRISK_Impl::generateKernel(const std::vector<float> &radiusList,\n \n   const float sigma_scale = 1.3f;\n \n-  for (unsigned int scale = 0; scale < scales_; ++scale)\n-  {\n-    scaleList_[scale] = (float)std::pow((double) 2.0, (double) (scale * lb_scale_step));\n-    sizeList_[scale] = 0;\n-\n-    // generate the pattern points look-up\n-    double alpha, theta;\n-    for (size_t rot = 0; rot < n_rot_; ++rot)\n-    {\n-      theta = double(rot) * 2 * CV_PI / double(n_rot_); // this is the rotation of the feature\n-      for (int ring = 0; ring < rings; ++ring)\n-      {\n-        for (int num = 0; num < numberList[ring]; ++num)\n-        {\n-          // the actual coordinates on the circle\n-          alpha = (double(num)) * 2 * CV_PI / double(numberList[ring]);\n-          patternIterator->x = (float)(scaleList_[scale] * radiusList[ring] * cos(alpha + theta)); // feature rotation plus angle of the point\n-          patternIterator->y = (float)(scaleList_[scale] * radiusList[ring] * sin(alpha + theta));\n-          // and the gaussian kernel sigma\n-          if (ring == 0)\n-          {\n-            patternIterator->sigma = sigma_scale * scaleList_[scale] * 0.5f;\n-          }\n-          else\n-          {\n-            patternIterator->sigma = (float)(sigma_scale * scaleList_[scale] * (double(radiusList[ring]))\n-                                     * sin(CV_PI / numberList[ring]));\n+  for (unsigned int scale = 0; scale < scales_; ++scale) {\n+      scaleList_[scale] = (float) std::pow((double) 2.0, (double) (scale * lb_scale_step));\n+      sizeList_[scale] = 0;\n+      BriskPatternPoint *patternIteratorOuter = patternPoints_ + (scale * n_rot_ * points_);\n+      // generate the pattern points look-up\n+      for (int ring = 0; ring < rings; ++ring) {\n+          double scaleRadiusProduct = scaleList_[scale] * radiusList[ring];\n+          float patternSigma = 0.0f;\n+          if (ring == 0) {\n+              patternSigma = sigma_scale * scaleList_[scale] * 0.5f;\n+          } else {\n+              patternSigma = (float) (sigma_scale * scaleList_[scale] * (double(radiusList[ring]))\n+                                      * sin(CV_PI / numberList[ring]));\n           }\n           // adapt the sizeList if necessary\n-          const unsigned int size = cvCeil(((scaleList_[scale] * radiusList[ring]) + patternIterator->sigma)) + 1;\n-          if (sizeList_[scale] < size)\n-          {\n-            sizeList_[scale] = size;\n+          const unsigned int size = cvCeil(((scaleList_[scale] * radiusList[ring]) + patternSigma)) + 1;\n+          if (sizeList_[scale] < size) {\n+              sizeList_[scale] = size;\n+          }\n+          for (int num = 0; num < numberList[ring]; ++num) {\n+              BriskPatternPoint *patternIterator = patternIteratorOuter;\n+              double alpha = (double(num)) * 2 * CV_PI / double(numberList[ring]);\n+              double sine_alpha = sin(alpha);\n+              double cosine_alpha = cos(alpha);\n+\n+              for (size_t rot = 0; rot < n_rot_; ++rot) {\n+                  double cosine_theta = cosThetaLookupTable[rot];\n+                  double sine_theta = sineThetaLookupTable[rot];\n+\n+                  // the actual coordinates on the circle\n+                  // sin(a + b) = sin(a) cos(b) + cos(a) sin(b)\n+                  // cos(a + b) = cos(a) cos(b) - sin(a) sin(b)\n+                  patternIterator->x = (float) (scaleRadiusProduct *\n+                                                (cosine_theta * cosine_alpha -\n+                                                 sine_theta * sine_alpha)); // feature rotation plus angle of the point\n+                  patternIterator->y = (float) (scaleRadiusProduct *\n+                                                (sine_theta * cosine_alpha + cosine_theta * sine_alpha));\n+                  patternIterator->sigma = patternSigma;\n+                  // and the gaussian kernel sigma\n+                  // increment the iterator\n+                  patternIterator += points_;\n+              }\n+              ++patternIteratorOuter;\n           }\n-\n-          // increment the iterator\n-          ++patternIterator;\n-        }\n       }\n-    }\n   }\n \n   // now also generate pairings\n", "changed_method_name": "cv::BRISK_Impl::generateKernel"}
{"commit_url": "https://github.com/opencv/opencv/commit/3bc1b5396258a85c6b3078b176cab3f7c8210c3a", "commit_message": "Added YUV conversion fix\n\nFixed OpenCV issue #18878", "code_diff": "@@ -104,6 +104,7 @@ inline int dstChannels(int code)\n             return 4;\n \n         case COLOR_BGRA2BGR: case COLOR_RGBA2BGR: case COLOR_RGB2BGR:\n+        case COLOR_YUV2RGB: case COLOR_YUV2BGR: case COLOR_RGB2YUV: case COLOR_BGR2YUV:\n         case COLOR_BGR5652BGR: case COLOR_BGR5552BGR: case COLOR_BGR5652RGB: case COLOR_BGR5552RGB:\n         case COLOR_GRAY2BGR:\n         case COLOR_YUV2BGR_NV21: case COLOR_YUV2RGB_NV21: case COLOR_YUV2BGR_NV12: case COLOR_YUV2RGB_NV12:\n", "changed_method_name": "cv::impl::dstChannels"}
{"commit_url": "https://github.com/opencv/opencv/commit/c526705f4f72fbd5720fb739ae7f01eb2d45d7b4", "commit_message": "[cv::transform] Enable CV_SIMD for the 16U case on AArch64.", "code_diff": "@@ -1537,7 +1537,7 @@ transform_8u( const uchar* src, uchar* dst, const float* m, int len, int scn, in\n static void\n transform_16u( const ushort* src, ushort* dst, const float* m, int len, int scn, int dcn )\n {\n-#if CV_SIMD && !defined(__aarch64__) && !defined(_M_ARM64)\n+#if CV_SIMD\n     if( scn == 3 && dcn == 3 )\n     {\n         int x = 0;\n", "changed_method_name": "cv::transform_16u"}
{"commit_url": "https://github.com/opencv/opencv/commit/028d4d83d3087db6ce789ec3fa900649cde761d9", "commit_message": "imgproc: sigma2=sigma1 in top-level function of GaussianBlur", "code_diff": "@@ -635,6 +635,9 @@ void GaussianBlur(InputArray _src, OutputArray _dst, Size ksize,\n         return;\n     }\n \n+    if (sigma2 <= 0)\n+        sigma2 = sigma1;\n+\n     bool useOpenCL = ocl::isOpenCLActivated() && _dst.isUMat() && _src.dims() <= 2 &&\n                _src.rows() >= ksize.height && _src.cols() >= ksize.width &&\n                ksize.width > 1 && ksize.height > 1;\n", "changed_method_name": "cv::GaussianBlur"}
{"commit_url": "https://github.com/opencv/opencv/commit/5dc5b2785884736f2889402502f35020b0481f45", "commit_message": "Enable build with OpenVINO in Debug", "code_diff": "@@ -252,7 +252,7 @@ void NetImplOpenVINO::addNgraphOutputs(LayerData& ld)\n             CV_Assert(!ieInpNode->net.empty());\n             if (layerNet != ieInpNode->net)\n             {\n-                CV_LOG_DEBUG(NULL, \"DNN/IE: pin output between subnets: \" << ieInpNode->node->get_friendly_name());\n+                CV_LOG_DEBUG(NULL, \"DNN/IE: pin output between subnets: \" << ieInpNode->node.get_node()->get_friendly_name());\n                 ieInpNode->net->addOutput(ieInpNode);\n             }\n         }\n", "changed_method_name": "cv::dnn::NetImplOpenVINO::addNgraphOutputs"}
{"commit_url": "https://github.com/opencv/opencv/commit/a289eba357dcdc21d088315cbf981afe9d7bb439", "commit_message": "Fixes #24677", "code_diff": "@@ -338,7 +338,7 @@ int runWinograd63(InputArray _input, InputArray _fusedAddMat, OutputArray _outpu\n                         }\n #if CV_TRY_AVX2\n                         if (conv->useAVX2)\n-                            opt_AVX::winofunc_AtXA_8x8_F32((float *)out_wbuf + ((k - k0)*CONV_WINO_IBLOCK + (block_id - block_id0))*CONV_WINO_AREA, CONV_WINO_SIZE,\n+                            opt_AVX2::winofunc_AtXA_8x8_F32((float *)out_wbuf + ((k - k0)*CONV_WINO_IBLOCK + (block_id - block_id0))*CONV_WINO_AREA, CONV_WINO_SIZE,\n                                                                 bpptr, outstep, outptr, outstep, biasv, minval, maxval, ifMinMaxAct);\n                         else\n #endif\n", "changed_method_name": "cv::dnn::runWinograd63"}
{"commit_url": "https://github.com/opencv/opencv/commit/db3654ef51b156feab4f59c13f2ee41ca2ab9a85", "commit_message": "python: prefer cv::Mat over cv::UMat in python binding", "code_diff": "@@ -854,7 +854,22 @@ class FuncInfo(object):\n \n         all_code_variants = []\n \n+        # See https://github.com/opencv/opencv/issues/25928\n+        # Conversion to UMat is expensive more than conversion to Mat.\n+        # To reduce this cost, conversion to Mat is prefer than to UMat.\n+        variants = []\n+        variants_umat = []\n         for v in self.variants:\n+            hasUMat = False\n+            for a in v.args:\n+                hasUMat = hasUMat or \"UMat\" in a.tp\n+            if hasUMat :\n+                variants_umat.append(v)\n+            else:\n+                variants.append(v)\n+        variants.extend(variants_umat)\n+\n+        for v in variants:\n             code_decl = \"\"\n             code_ret = \"\"\n             code_cvt_list = []\n", "changed_method_name": "gen_code"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/eeb78bf29172112ee832e2e46ed75271961717d9", "commit_message": "slightly optimize very short queries with LowCardinality", "code_diff": "@@ -43,11 +43,13 @@ MergeTreeReaderStream::MergeTreeReaderStream(\n         /// If the end of range is inside the block, we will need to read it too.\n         if (right_mark < marks_count && marks_loader.getMark(right_mark).offset_in_decompressed_block > 0)\n         {\n-            while (right_mark < marks_count\n-                && marks_loader.getMark(right_mark).offset_in_compressed_file == marks_loader.getMark(mark_range.end).offset_in_compressed_file)\n+            auto indices = ext::range(right_mark, marks_count);\n+            auto it = std::upper_bound(indices.begin(), indices.end(), right_mark, [this](size_t i, size_t j)\n             {\n-                ++right_mark;\n-            }\n+                return marks_loader.getMark(i).offset_in_compressed_file < marks_loader.getMark(j).offset_in_compressed_file;\n+            });\n+\n+            right_mark = (it == indices.end() ? marks_count : *it);\n         }\n \n         size_t mark_range_bytes;\n", "changed_method_name": "DB::MergeTreeReaderStream::MergeTreeReaderStream"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/fff110aa38c8af9d91682a34d79f21a47a47b632", "commit_message": "HashedDictionary clickhouse source preallocate regression fix", "code_diff": "@@ -384,42 +384,13 @@ void HashedDictionary::loadData()\n {\n     if (!source_ptr->hasUpdateField())\n     {\n-        /// atomic since progress callbac called in parallel\n-        std::atomic<uint64_t> new_size = 0;\n         auto stream = source_ptr->loadAll();\n \n-        /// preallocation can be used only when we know number of rows, for this we need:\n-        /// - source clickhouse\n-        /// - no filtering (i.e. lack of <where>), since filtering can filter\n-        ///   too much rows and eventually it may allocate memory that will\n-        ///   never be used.\n-        bool preallocate = false;\n-        if (const auto & clickhouse_source = dynamic_cast<ClickHouseDictionarySource *>(source_ptr.get()))\n-        {\n-            if (!clickhouse_source->hasWhere())\n-                preallocate = true;\n-        }\n-\n-        if (preallocate)\n-        {\n-            stream->setProgressCallback([&new_size](const Progress & progress)\n-            {\n-                new_size += progress.total_rows_to_read;\n-            });\n-        }\n-\n         stream->readPrefix();\n \n         while (const auto block = stream->read())\n         {\n-            if (new_size)\n-            {\n-                size_t current_new_size = new_size.exchange(0);\n-                if (current_new_size)\n-                    resize(current_new_size);\n-            }\n-            else\n-                resize(block.rows());\n+            resize(block.rows());\n             blockToAttributes(block);\n         }\n \n", "changed_method_name": "DB::HashedDictionary::loadData"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/3e700e854d6a1620bf80da5340206e1ec691b22a", "commit_message": "cancel merges before acquiring lock for truncate", "code_diff": "@@ -35,6 +35,10 @@ namespace ErrorCodes\n     extern const int TABLE_IS_READ_ONLY;\n }\n \n+namespace ActionLocks\n+{\n+    extern const StorageActionBlockType PartsMerge;\n+}\n \n static DatabasePtr tryGetDatabase(const String & database_name, bool if_exists)\n {\n@@ -202,7 +206,15 @@ BlockIO InterpreterDropQuery::executeToTableImpl(ContextPtr context_, ASTDropQue\n \n             table->checkTableCanBeDropped();\n \n-            auto table_lock = table->lockExclusively(context_->getCurrentQueryId(), context_->getSettingsRef().lock_acquire_timeout);\n+            TableExclusiveLockHolder table_lock;\n+            /// We don't need this lock for ReplicatedMergeTree\n+            if (!table->supportsReplication())\n+            {\n+                /// And for simple MergeTree we can stop merges before acquiring the lock\n+                auto merges_blocker = table->getActionLock(ActionLocks::PartsMerge);\n+                auto table_lock = table->lockExclusively(context_->getCurrentQueryId(), context_->getSettingsRef().lock_acquire_timeout);\n+            }\n+\n             auto metadata_snapshot = table->getInMemoryMetadataPtr();\n             /// Drop table data, don't touch metadata\n             table->truncate(query_ptr, metadata_snapshot, context_, table_lock);\n", "changed_method_name": "DB::InterpreterDropQuery::executeToTableImpl"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/a45e3d47adacc59699de26b59e5966307b97b8fb", "commit_message": "Remove useless codec from system.asynchronous_metric_log", "code_diff": "@@ -40,7 +40,7 @@ struct AsynchronousMetricLogElement\n         return \"event_date Date CODEC(Delta(2), ZSTD(1)), \"\n                \"event_time DateTime CODEC(Delta(4), ZSTD(1)), \"\n                \"metric LowCardinality(String) CODEC(ZSTD(1)), \"\n-               \"value Float64 CODEC(Gorilla, ZSTD(3))\";\n+               \"value Float64 CODEC(ZSTD(3))\";\n     }\n };\n \n", "changed_method_name": "DB::AsynchronousMetricLogElement::getCustomColumnList"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/6c8fc4cd11966cb96571fe3a2dfaac6b82cbd03c", "commit_message": "fix hashjoin debug code condition", "code_diff": "@@ -495,7 +495,7 @@ size_t HashJoin::getTotalByteCount() const\n     if (!data)\n         return 0;\n \n-#ifdef NDEBUG\n+#ifndef NDEBUG\n     size_t debug_blocks_allocated_size = 0;\n     for (const auto & block : data->blocks)\n         debug_blocks_allocated_size += block.allocatedBytes();\n", "changed_method_name": "DB::HashJoin::getTotalByteCount"}
{"commit_url": "https://github.com/ClickHouse/ClickHouse/commit/3acb6005f041051b7c00c48df5035843744a7e24", "commit_message": "Reduce the number of syscalls in FileCache::loadMetadata", "code_diff": "@@ -870,13 +870,12 @@ void FileCache::loadMetadata()\n     }\n \n     size_t total_size = 0;\n-    for (auto key_prefix_it = fs::directory_iterator{metadata.getBaseDirectory()};\n-         key_prefix_it != fs::directory_iterator();)\n+    for (auto key_prefix_it = fs::directory_iterator{metadata.getBaseDirectory()}; key_prefix_it != fs::directory_iterator();\n+         key_prefix_it++)\n     {\n         const fs::path key_prefix_directory = key_prefix_it->path();\n-        key_prefix_it++;\n \n-        if (!fs::is_directory(key_prefix_directory))\n+        if (!key_prefix_it->is_directory())\n         {\n             if (key_prefix_directory.filename() != \"status\")\n             {\n@@ -887,19 +886,19 @@ void FileCache::loadMetadata()\n             continue;\n         }\n \n-        if (fs::is_empty(key_prefix_directory))\n+        fs::directory_iterator key_it{key_prefix_directory};\n+        if (key_it == fs::directory_iterator{})\n         {\n             LOG_DEBUG(log, \"Removing empty key prefix directory: {}\", key_prefix_directory.string());\n             fs::remove(key_prefix_directory);\n             continue;\n         }\n \n-        for (fs::directory_iterator key_it{key_prefix_directory}; key_it != fs::directory_iterator();)\n+        for (/* key_it already initialized to verify emptiness */; key_it != fs::directory_iterator(); key_it++)\n         {\n             const fs::path key_directory = key_it->path();\n-            ++key_it;\n \n-            if (!fs::is_directory(key_directory))\n+            if (!key_it->is_directory())\n             {\n                 LOG_DEBUG(\n                     log,\n@@ -908,7 +907,7 @@ void FileCache::loadMetadata()\n                 continue;\n             }\n \n-            if (fs::is_empty(key_directory))\n+            if (fs::directory_iterator{key_directory} == fs::directory_iterator{})\n             {\n                 LOG_DEBUG(log, \"Removing empty key directory: {}\", key_directory.string());\n                 fs::remove(key_directory);\n", "changed_method_name": "DB::FileCache::loadMetadata"}
{"commit_url": "https://github.com/qbittorrent/qBittorrent/commit/534ed91d043abfe8ad7ccd307e4c8b060bdaf214", "commit_message": "Change MixedModeAlgorithm default to TCP. Closes #7779.\n\nMixedModeAlgorithm::Proportional will throttle TCP connections when utp\nis in use and users is expecting maximum speed no matter what, so now\ndisable the throttling.", "code_diff": "@@ -304,7 +304,7 @@ Session::Session(QObject *parent)\n     , m_btProtocol(BITTORRENT_SESSION_KEY(\"BTProtocol\"), BTProtocol::Both\n         , clampValue(BTProtocol::Both, BTProtocol::UTP))\n     , m_isUTPRateLimited(BITTORRENT_SESSION_KEY(\"uTPRateLimited\"), true)\n-    , m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::Proportional\n+    , m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::TCP\n         , clampValue(MixedModeAlgorithm::TCP, MixedModeAlgorithm::Proportional))\n     , m_multiConnectionsPerIpEnabled(BITTORRENT_SESSION_KEY(\"MultiConnectionsPerIp\"), false)\n     , m_isAddTrackersEnabled(BITTORRENT_SESSION_KEY(\"AddTrackersEnabled\"), false)\n", "changed_method_name": "Session::Session"}
{"commit_url": "https://github.com/qbittorrent/qBittorrent/commit/2f1a0ffe5c3e01a1903809106df7156e86a22201", "commit_message": "Use a more detailed alert mask where possible\n\nCloses #9547", "code_diff": "@@ -392,7 +392,11 @@ Session::Session(QObject *parent)\n                     | libt::alert::tracker_notification\n                     | libt::alert::status_notification\n                     | libt::alert::ip_block_notification\n+#if LIBTORRENT_VERSION_NUM < 10110\n                     | libt::alert::progress_notification\n+#else\n+                    | libt::alert::file_progress_notification\n+#endif\n                     | libt::alert::stats_notification;\n \n #if LIBTORRENT_VERSION_NUM < 10100\n", "changed_method_name": "Session::Session"}
{"commit_url": "https://github.com/hrydgard/ppsspp/commit/354d263ccf4cf8f3913433b1b6f7f534e497de82", "commit_message": "sceKernelFindModuleByName:Add delay for Fake module\n\nFix #13601", "code_diff": "@@ -2444,12 +2444,18 @@ u32 sceKernelFindModuleByName(const char *name)\n \t\tPSPModule *module = kernelObjects.Get<PSPModule>(moduleId, error);\n \t\tif (!module)\n \t\t\tcontinue;\n-\t\tif (!module->isFake && strcmp(name, module->nm.name) == 0) {\n-\t\t\tINFO_LOG(SCEMODULE, \"%d = sceKernelFindModuleByName(%s)\", module->modulePtr, name);\n-\t\t\treturn module->modulePtr;\n+\t\tif (strcmp(name, module->nm.name) == 0) {\n+\t\t\tif (!module->isFake) {\n+\t\t\t\tINFO_LOG(SCEMODULE, \"%d = sceKernelFindModuleByName(%s)\", module->modulePtr, name);\n+\t\t\t\treturn module->modulePtr;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Fake\", name);\n+\t\t\t\treturn hleDelayResult(0, \"Module Fake\", 1000 * 1000);\n+\t\t\t}\n \t\t}\n \t}\n-\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Not Found or Fake\", name);\n+\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Not Found\", name);\n \treturn 0;\n }\n \n", "changed_method_name": "sceKernelFindModuleByName"}
{"commit_url": "https://github.com/hrydgard/ppsspp/commit/adda49d05d1091decb8832259f0ca4e9e085f143", "commit_message": "Add a heuristic avoiding joining framebuffers horizontally\n\n...when texturing from the other one.\n\nGreatly improves GPU performance in Rainbow Six: Vegas.\n\nFixes #9324.", "code_diff": "@@ -426,6 +426,14 @@ VirtualFramebuffer *FramebufferManagerCommon::DoSetRenderFrameBuffer(Framebuffer\n \t\t\tu32 v_fb_end_ptr = v->fb_address + v->fb_stride * v->height * bpp;\n \n \t\t\tif (params.fb_address > v->fb_address && params.fb_address < v_fb_first_line_end_ptr) {\n+\t\t\t\t// If the framebuffer we can join to is currently bound as a texture, we likely have\n+\t\t\t\t// a situation like in #9324 and don't want to do this.\n+\t\t\t\tu32 curTextureAddress = gstate.getTextureAddress(0);\n+\t\t\t\tif (v->fb_address == curTextureAddress) {\n+\t\t\t\t\t// Don't try these joining shenanigans.\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\n \t\t\t\tconst int x_offset = (params.fb_address - v->fb_address) / bpp;\n \t\t\t\tif (x_offset < params.fb_stride && v->height >= drawing_height) {\n \t\t\t\t\t// Pretty certainly a pure render-to-X-offset.\n", "changed_method_name": "FramebufferManagerCommon::DoSetRenderFrameBuffer"}
{"commit_url": "https://github.com/NixOS/nix/commit/0f977bf91e29192d7f0c0f9cad16351bad7cd137", "commit_message": "Remove a useless debug message in filetransfer.cc\n\nRemove the `verify TLS: Nix CA file = 'blah'` message that Nix used to print when fetching anything as it's both useless (`libcurl` prints the same info in its logs) and misleading (gives the impression that a new TLS connection is being established which might not be the case because of multiplexing. See #7011 )", "code_diff": "@@ -322,7 +322,6 @@ struct curlFileTransfer : public FileTransfer\n             }\n \n             if (request.verifyTLS) {\n-                debug(\"verify TLS: Nix CA file = '%s'\", settings.caFile);\n                 if (settings.caFile != \"\")\n                     curl_easy_setopt(req, CURLOPT_CAINFO, settings.caFile.c_str());\n             } else {\n", "changed_method_name": "nix::curlFileTransfer::TransferItem::init"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/666085351618ea56365661c47af7c97fd959bfd0", "commit_message": "k/topic_utils: do not return an error when waiting for leaders failed\n\nRecently introduced change (waiting for leaders before returning from\ncreate topics) change the behavior of `CreateTopicsRequest` handler.\nPreviously handler was waiting for topic partitions to be created but\neven if that timed out it returned success to the client. Waiting for\nleaders threw an exception when timeout was trigger which caused client\nconnection to be dropped.\n\nFixed an error by going back to previous behavior i.e. ignoring waiting\ntimeouts. The timeout does not indicate that topic creation failed\nactually when waiting for creation topic already exists.\n\nFixes: #7942\n\nSigned-off-by: Michal Maslanka <michal@redpanda.com>", "code_diff": "@@ -79,7 +79,11 @@ ss::future<> wait_for_topics(\n                    })\n             .then([&md_cache, &results, timeout]() {\n                 return wait_for_leaders(md_cache, results, timeout)\n-                  .discard_result();\n+                  .discard_result()\n+                  .handle_exception_type([](const ss::timed_out_error&) {\n+                      // discard timed out exception, even tho waiting failed\n+                      // the topic is created\n+                  });\n             });\n       });\n }\n", "changed_method_name": "kafka::wait_for_topics"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/074e634a856dbcb45132d41a470d33f9922d6a2d", "commit_message": "rptest/scale_tests: limit producer rate in test", "code_diff": "@@ -40,6 +40,12 @@ class ManyClientsTest(RedpandaTest):\n             # Enable segment size jitter as this is a stress test and does not\n             # rely on exact segment counts.\n             'log_segment_size_jitter_percent': 5,\n+            # This limit caps the produce throughput to a sustainable rate for a RP\n+            # cluster that has 384MB of memory per shard. It is set here to\n+            # since our current backpressure mechanisms will allow producers to\n+            # produce at a much higher rate and cause RP to run out of memory.\n+            'target_quota_byte_rate':\n+            31460000,  # 30MiB/s of throughput per shard\n         }\n         super().__init__(*args, **kwargs)\n \n", "changed_method_name": "__init__"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/04926fa313b682f9efd62502b7b73d69d083a1f7", "commit_message": "tests: permit OMB log error\n\nAs far as we know, this isn't causing any actual\nproblems.  Add a generic allow list to the OMB\nservice and add \"cannot be started once stopped\" to this.\n\nFixes https://github.com/redpanda-data/redpanda/issues/8405", "code_diff": "@@ -7,8 +7,6 @@\n # the Business Source License, use of this software will be governed\n # by the Apache License, Version 2.0\n \n-import re\n-import threading\n import os\n import json\n import collections\n@@ -19,6 +17,10 @@ from ducktape.utils.util import wait_until\n from rptest.services.utils import BadLogLines, NodeCrash\n from rptest.services.openmessaging_benchmark_configs import OMBSampleConfigurations\n \n+LOG_ALLOW_LIST = [\n+    \"No such file or directory\", \"cannot be started once stopped\"\n+]\n+\n \n # Benchmark worker that is used by benchmark process to run consumers and producers\n class OpenMessagingBenchmarkWorkers(Service):\n@@ -92,7 +94,13 @@ class OpenMessagingBenchmarkWorkers(Service):\n         for line in node.account.ssh_capture(\n                 f\"grep -e TimeoutException {OpenMessagingBenchmarkWorkers.STDOUT_STDERR_CAPTURE} || true\"\n         ):\n-            if \"No such file or directory\" not in line:\n+            allowed = False\n+            for a in LOG_ALLOW_LIST:\n+                if a in line:\n+                    allowed = True\n+                    break\n+\n+            if not allowed:\n                 bad_lines[node].append(line)\n \n         if bad_lines:\n@@ -266,7 +274,13 @@ class OpenMessagingBenchmark(Service):\n         for line in node.account.ssh_capture(\n                 f\"grep -e Exception {OpenMessagingBenchmark.STDOUT_STDERR_CAPTURE} || true\"\n         ):\n-            if \"No such file or directory\" not in line:\n+            allowed = False\n+            for a in LOG_ALLOW_LIST:\n+                if a in line:\n+                    allowed = True\n+                    break\n+\n+            if not allowed:\n                 bad_lines[node].append(line)\n \n         if bad_lines:\n", "changed_method_name": "raise_on_bad_log_lines"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/3c732d3c063f82f87547c965445693473ae08c4d", "commit_message": "raft: Use async_adl for serde_async_write\n\nWe are seeing reactor stalls in\n`HighThroughputPartitionMovementTest/test_interrupting_partition_movement_under_load`\nDT test caused by serializing `record_batch`s.\n\nJohn recently added async adl support in\n785201e6e49b7b6bd03f8101232eb645861020f7.\n\nThis patch makes use of that so that we can potentially yield in between\nserializing records in a batch and avoid reactor stalls.\n\nIssue https://github.com/redpanda-data/redpanda/issues/10418", "code_diff": "@@ -11,12 +11,14 @@\n \n #include \"model/fundamental.h\"\n #include \"model/metadata.h\"\n+#include \"model/record.h\"\n #include \"model/record_batch_reader.h\"\n #include \"model/timeout_clock.h\"\n #include \"raft/consensus_utils.h\"\n #include \"raft/errc.h\"\n #include \"raft/group_configuration.h\"\n #include \"reflection/adl.h\"\n+#include \"reflection/async_adl.h\"\n #include \"utils/to_string.h\"\n #include \"vassert.h\"\n #include \"vlog.h\"\n@@ -598,7 +600,8 @@ ss::future<> append_entries_request::serde_async_write(iobuf& dst) {\n     class streaming_writer {\n     public:\n         ss::future<ss::stop_iteration> operator()(model::record_batch b) {\n-            reflection::serialize(_out, std::move(b));\n+            co_await reflection::async_adl<model::record_batch>{}.to(\n+              _out, std::move(b));\n             ++_count;\n             co_return ss::stop_iteration::no;\n         }\n", "changed_method_name": "raft::append_entries_request::serde_async_write"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/6d1223d4e4b0489c0fade40a727b9633d506398a", "commit_message": "kafka: Disable use of separate fetch scheduling group\n\nThis partially reverts 9a93a9c22238e145dfa9d7fe297eb494d7c5f0bf\n\nWhile the original motiviation isn't invalidated we have now found a\ncounter example where the extra fetch groups makes things worse overall.\n\n`ManyPartitionTest` fails on ARM with the extra group but passes\nwithout. With the group in use CPU util hits 100% and grinds everything\nto halt.\n\nFetch seems to be a lot slower on ARM. Hence, with the guaranteed share\nof the extra group the whole system gets affected and hits CPU limits.\n\nBecause this is incredibly hard to reason about and it wasn't the core\nfetch optimization we decided to revert back to keeping it disabled by\ndefault.\n\nWe still keep the option around as it might be useful potentially in\ncorner cases.\n\nFixes https://github.com/redpanda-data/redpanda/issues/10507", "code_diff": "@@ -516,7 +516,7 @@ configuration::configuration()\n       \"use_fetch_scheduler_group\",\n       \"Use a separate scheduler group for fetch processing\",\n       {.needs_restart = needs_restart::no, .visibility = visibility::tunable},\n-      true)\n+      false)\n   , metadata_status_wait_timeout_ms(\n       *this,\n       \"metadata_status_wait_timeout_ms\",\n", "changed_method_name": "config::configuration::configuration"}
{"commit_url": "https://github.com/redpanda-data/redpanda/commit/a52d0ada31dd0c842333fadc2efc37d4aaf2b471", "commit_message": "c/balancer_backend: first initialize planner and then call plan\n\nThis change is a part of an effort to identify and fix rare segmentation\nfault in Redpanda that happens after it was suspended with `SIGSTOP`\nsignal.\nAccording to the C++ standard the temporary should be kept alive until\nthe expression ends. The crash we are observing indicates the UAF issue.\nThe only way the variable, that access causes the segfault, can be\ndeleted is by getting out of scope which in this situation should be\nguaranteed.\n\nGiven our experience with coroutines and different types of lifecycle\nbugs that we found in past this is a poor man's effort to avoid the\nissue.\n\nSigned-off-by: Micha\u0142 Ma\u015blanka <michal@redpanda.com>", "code_diff": "@@ -357,24 +357,26 @@ ss::future<> partition_balancer_backend::do_tick() {\n     // claim node unresponsive it doesn't responded to at least 7\n     // status requests by default 700ms\n     auto const node_responsiveness_timeout = _node_status_interval() * 7;\n-    auto plan_data\n-      = co_await partition_balancer_planner(\n-          planner_config{\n-            .mode = _mode(),\n-            .soft_max_disk_usage_ratio = soft_max_disk_usage_ratio,\n-            .hard_max_disk_usage_ratio = hard_max_disk_usage_ratio,\n-            .max_concurrent_actions = _max_concurrent_actions(),\n-            .node_availability_timeout_sec = _availability_timeout(),\n-            .ondemand_rebalance_requested\n-            = _cur_term->_ondemand_rebalance_requested,\n-            .segment_fallocation_step = _segment_fallocation_step(),\n-            .min_partition_size_threshold = get_min_partition_size_threshold(),\n-            .node_responsiveness_timeout = node_responsiveness_timeout,\n-            .topic_aware = _topic_aware(),\n-          },\n-          _state,\n-          _partition_allocator)\n-          .plan_actions(health_report.value(), _tick_in_progress.value());\n+\n+    partition_balancer_planner planner(\n+      planner_config{\n+        .mode = _mode(),\n+        .soft_max_disk_usage_ratio = soft_max_disk_usage_ratio,\n+        .hard_max_disk_usage_ratio = hard_max_disk_usage_ratio,\n+        .max_concurrent_actions = _max_concurrent_actions(),\n+        .node_availability_timeout_sec = _availability_timeout(),\n+        .ondemand_rebalance_requested\n+        = _cur_term->_ondemand_rebalance_requested,\n+        .segment_fallocation_step = _segment_fallocation_step(),\n+        .min_partition_size_threshold = get_min_partition_size_threshold(),\n+        .node_responsiveness_timeout = node_responsiveness_timeout,\n+        .topic_aware = _topic_aware(),\n+      },\n+      _state,\n+      _partition_allocator);\n+\n+    auto plan_data = co_await planner.plan_actions(\n+      health_report.value(), _tick_in_progress.value());\n \n     _cur_term->last_tick_time = clock_t::now();\n     _cur_term->last_violations = std::move(plan_data.violations);\n", "changed_method_name": "cluster::partition_balancer_backend::do_tick"}
{"commit_url": "https://github.com/ElucidataInc/ElMaven/commit/b4583dbb174a13617ff592a4599a099dd3389519", "commit_message": "Remove unnecessary vector copy\n\nTemporary matrices being used in `find_path` method are attempted\nto be copied into ObiWarp's class matrices at the end. But these\nmatrices themselves are not being used anywhere else. The copy\noperation was causing a crash due to attempts to free unallocated\nmemory. The copy operations have been removed to prevent this\ncrash.\n\nIssue: #859", "code_diff": "@@ -1301,10 +1301,5 @@ void DynProg::find_path(MatF& smat, VecF &gap_penalty, int minimize, float diag_\n     _traceback(tmp_tb, smat, optimal_m, optimal_n, tmp_tbpath, _mCoords, _nCoords, _sCoords); \n     int _equivLastInd = _mCoords.dim()-1;\n     _bestScore = tmp_asmat(_mCoords[_equivLastInd],_nCoords[_equivLastInd]);\n-\n-    _asmat.take(tmp_asmat);\n-    _tb.take(tmp_tb);\n-    _tbpath.take(tmp_tbpath);\n-    _gapmat.take(tmp_gapmat);\n }\n \n", "changed_method_name": "DynProg::find_path"}
{"commit_url": "https://github.com/TheGameCreators/GameGuruRepo/commit/198cadd3d865f0127711b4dfac55ef0db6419012", "commit_message": "Bug-Fix: Frozen Weapon Animations when light rays in use\n\nSee bug https://github.com/TheGameCreators/GameGuruRepo/issues/4010", "code_diff": "@@ -386,6 +386,7 @@ void postprocess_preterrain ( void )\n \t//  Render pre-terrain post process cameras for 'glightraycameraid' and 'rift mode' (below)\n \tif (  g.gpostprocessing>0 && t.glightraycameraid>0 && t.visuals.lightraymode>0 ) \n \t{\n+\t\t\n \t\t//  switch to black textures technique (fast)\n \t\tif (  t.terrain.vegetationshaderindex>0 ) \n \t\t{\n@@ -411,6 +412,7 @@ void postprocess_preterrain ( void )\n \t\t\t\tSetEffectTechnique ( t.teffectid, \"blacktextured\" );\n \t\t\t}\n \t\t}\n+\n \t\t// remove sky from lightray\n \t\tif (  ObjectExist(t.terrain.objectstartindex+4) == 1 ) \n \t\t{\n@@ -468,8 +470,9 @@ void postprocess_preterrain ( void )\n \n \t\t\tif (ObjectExist(t.terrain.objectstartindex + 9) == 1)  PositionObject(t.terrain.objectstartindex + 9, CameraPositionX(0), (11000 + t.terrain.waterliney_f), CameraPositionZ(0));\n \t\t\n-\t\t\t//SetObjectMask (  t.terrain.objectstartindex+9,1);//(t.tskymaskforcamerasnoshadow),0,0,0 ); no relfection of scroll\n-\t\t\tSetObjectMask(t.terrain.objectstartindex + 9, (t.tskymaskforcamerasnoshadow), 0, 0, 0);\n+\t\t\t//Bug-Fix: Frozen Weapon Animations when light rays in use\n+\t\t\tSetObjectMask (  t.terrain.objectstartindex+9,1);//(t.tskymaskforcamerasnoshadow),0,0,0 ); no relfection of scroll\n+\t\t\t////SetObjectMask(t.terrain.objectstartindex + 9, (t.tskymaskforcamerasnoshadow), 0, 0, 0);\n \t\t}\n \n \t\t// reset skyscroll technique\n", "changed_method_name": "postprocess_preterrain"}
{"commit_url": "https://github.com/molpopgen/fwdpy11/commit/99c8e5abdbe62b605089277051bd6ac7ed29d061", "commit_message": "Return early from fwdpy11.infinite_sites when possible.\nRemove code sorting the mutation table, as it is done upstream in\nthe fwdpp function.  Closes #293.", "code_diff": "@@ -38,7 +38,11 @@ init_infinite_sites(py::module& m)\n     m.def(\n         \"infinite_sites\",\n         [](const fwdpy11::GSLrng_t& rng, fwdpy11::Population& pop,\n-           const double mu) {\n+           const double mu) -> unsigned {\n+            if (mu <= 0.0)\n+                {\n+                    return 0u;\n+                }\n             fwdpp::flagged_mutation_queue recycling_bin\n                 = fwdpp::ts::make_mut_queue(pop.mcounts,\n                                             pop.mcounts_from_preserved_nodes);\n@@ -66,13 +70,10 @@ init_infinite_sites(py::module& m)\n                   };\n             auto nmuts = fwdpp::ts::mutate_tables(rng, apply_mutations,\n                                                   pop.tables, samples, mu);\n-            std::sort(pop.tables.mutation_table.begin(),\n-                      pop.tables.mutation_table.end(),\n-                      [&pop](const fwdpp::ts::mutation_record& a,\n-                             const fwdpp::ts::mutation_record& b) {\n-                          return pop.mutations[a.key].pos\n-                                 < pop.mutations[b.key].pos;\n-                      });\n+            if (nmuts == 0)\n+                {\n+                    return nmuts;\n+                }\n             fwdpp::ts::count_mutations(pop.tables, pop.mutations, samples,\n                                        pop.mcounts,\n                                        pop.mcounts_from_preserved_nodes);\n", "changed_method_name": "init_infinite_sites"}
{"commit_url": "https://github.com/project-asgard/asgard/commit/076f8b296ce85de4ae54ccfea88f8074e42c6ffe", "commit_message": "improve perf of combine dims using views", "code_diff": "@@ -219,10 +219,11 @@ combine_dimensions(int const degree, element_table const &table,\n           degree > 1 ? (((id + 1) * degree) - 1) : index_start;\n       kron_list.push_back(vectors[j].extract(index_start, index_end));\n     }\n-    fk::vector<P> const partial_result =\n-        kron_d(kron_list, kron_list.size()) * time_scale;\n-    combined.set_subvector((i - start_element) * std::pow(degree, num_dims),\n-                           partial_result);\n+    int const start_index = (i - start_element) * std::pow(degree, num_dims);\n+    int const stop_index  = start_index + std::pow(degree, num_dims) - 1;\n+    fk::vector<P, mem_type::view> combined_view(combined, start_index,\n+                                                stop_index);\n+    combined_view = kron_d(kron_list, kron_list.size()) * time_scale;\n   }\n   return combined;\n }\n", "changed_method_name": "combine_dimensions"}
{"commit_url": "https://github.com/matplotlib/matplotlib/commit/1e32084fccdd8e027790a5afe0680a87454f5f89", "commit_message": "Avoid quadratic behavior when accumulating stickies.\n\nWhen plotting 10000 lines, this gives a ~10% improvement in performance.", "code_diff": "@@ -2405,12 +2405,12 @@ class _AxesBase(martist.Artist):\n \n         if self.use_sticky_edges and (self._xmargin or self._ymargin):\n             stickies = [artist.sticky_edges for artist in self.get_children()]\n-            x_stickies = sum([sticky.x for sticky in stickies], [])\n-            y_stickies = sum([sticky.y for sticky in stickies], [])\n+            x_stickies = np.array([x for sticky in stickies for x in sticky.x])\n+            y_stickies = np.array([y for sticky in stickies for y in sticky.y])\n             if self.get_xscale().lower() == 'log':\n-                x_stickies = [xs for xs in x_stickies if xs > 0]\n+                x_stickies = x_stickies[x_stickies > 0]\n             if self.get_yscale().lower() == 'log':\n-                y_stickies = [ys for ys in y_stickies if ys > 0]\n+                y_stickies = y_stickies[y_stickies > 0]\n         else:  # Small optimization.\n             x_stickies, y_stickies = [], []\n \n", "changed_method_name": "autoscale_view"}
{"commit_url": "https://github.com/matplotlib/matplotlib/commit/318a5894fd4958ce9cdaaed9ac81e1143cbc37a6", "commit_message": "Disable sticky edge accumulation if no autoscaling.\n\nIf there's no margin to be added, we don't need sticky edges, but if\nautoscaling is off, we _also_ don't need the sticky edges. This saves a\nlot of time when plotting many artists, like in #12542.", "code_diff": "@@ -2403,7 +2403,9 @@ class _AxesBase(martist.Artist):\n         if tight is not None:\n             self._tight = bool(tight)\n \n-        if self.use_sticky_edges and (self._xmargin or self._ymargin):\n+        if self.use_sticky_edges and (\n+                (self._xmargin and scalex and self._autoscaleXon) or\n+                (self._ymargin and scaley and self._autoscaleYon)):\n             stickies = [artist.sticky_edges for artist in self.get_children()]\n             x_stickies = sum([sticky.x for sticky in stickies], [])\n             y_stickies = sum([sticky.y for sticky in stickies], [])\n", "changed_method_name": "autoscale_view"}
{"commit_url": "https://github.com/matplotlib/matplotlib/commit/0f889dc3e39c2174f370614dc4942b380586ec21", "commit_message": "Manually linewrap PS hexlines. Fixes #17176", "code_diff": "@@ -7,12 +7,12 @@ from enum import Enum\n import glob\n from io import StringIO, TextIOWrapper\n import logging\n+import math\n import os\n import pathlib\n import re\n import shutil\n from tempfile import TemporaryDirectory\n-import textwrap\n import time\n \n import numpy as np\n@@ -286,9 +286,17 @@ class RendererPS(_backend_pdf_ps.RendererPDFPSBase):\n         h, w = im.shape[:2]\n         imagecmd = \"false 3 colorimage\"\n         data = im[::-1, :, :3]  # Vertically flipped rgb values.\n-        # data.tobytes().hex() has no spaces, so can be linewrapped by relying\n-        # on textwrap.fill breaking long words.\n-        hexlines = textwrap.fill(data.tobytes().hex(), 128)\n+        # data.tobytes().hex() has no spaces, so can be linewrapped by simply\n+        # splitting data every nchars. It's equivalent to textwrap.fill only\n+        # much faster.\n+        nchars = 128\n+        data = data.tobytes().hex()\n+        hexlines = \"\\n\".join(\n+            [\n+                data[n * nchars:(n + 1) * nchars]\n+                for n in range(math.ceil(len(data) / nchars))\n+            ]\n+        )\n \n         if transform is None:\n             matrix = \"1 0 0 1 0 0\"\n", "changed_method_name": "draw_image"}
{"commit_url": "https://github.com/matplotlib/matplotlib/commit/1e6352ab60409d936c8073343329abf3ebbe750b", "commit_message": "Special case degree-1 Bezier curves.\n\nThis greatly speeds up extent computation for the common case of\npolylines.  (We were previously only special-casing the degree-0 case.)", "code_diff": "@@ -287,10 +287,10 @@ class BezierSegment:\n             0`\n         \"\"\"\n         n = self.degree\n+        if n <= 1:\n+            return np.array([]), np.array([])\n         Cj = self.polynomial_coefficients\n         dCj = np.arange(1, n+1)[:, None] * Cj[1:]\n-        if len(dCj) == 0:\n-            return np.array([]), np.array([])\n         dims = []\n         roots = []\n         for i, pi in enumerate(dCj.T):\n", "changed_method_name": "axis_aligned_extrema"}
{"commit_url": "https://github.com/matplotlib/matplotlib/commit/3d35d26817b9c78b83658526815e8501b83e3870", "commit_message": "Avoid using Bbox machinery in Path.get_extents; special case polylines.\n\nupdate_from_data_xy is much slower than needed; we can use plain numpy\ninstead.\nFor polylines, we can completely skip the bezier handling for even more\nspeedup.", "code_diff": "@@ -588,13 +588,19 @@ class Path:\n         from .transforms import Bbox\n         if transform is not None:\n             self = transform.transform_path(self)\n-        bbox = Bbox.null()\n-        for curve, code in self.iter_bezier(**kwargs):\n-            # places where the derivative is zero can be extrema\n-            _, dzeros = curve.axis_aligned_extrema()\n-            # as can the ends of the curve\n-            bbox.update_from_data_xy(curve([0, *dzeros, 1]), ignore=False)\n-        return bbox\n+        if self.codes is None:\n+            xys = self.vertices\n+        elif len(np.intersect1d(self.codes, [Path.CURVE3, Path.CURVE4])) == 0:\n+            xys = self.vertices[self.codes != Path.CLOSEPOLY]\n+        else:\n+            xys = []\n+            for curve, code in self.iter_bezier(**kwargs):\n+                # places where the derivative is zero can be extrema\n+                _, dzeros = curve.axis_aligned_extrema()\n+                # as can the ends of the curve\n+                xys.append(curve([0, *dzeros, 1]))\n+            xys = np.concatenate(xys)\n+        return Bbox([xys.min(axis=0), xys.max(axis=0)])\n \n     def intersects_path(self, other, filled=True):\n         \"\"\"\n", "changed_method_name": "get_extents"}
{"commit_url": "https://github.com/iterative/dvc/commit/c4836b4578f59a0923be5655259a11538ca193fc", "commit_message": "dvc: optmize PathInfo.isin()\n\nShould help with #2203", "code_diff": "@@ -19,6 +19,10 @@ if is_py2:\n \n \n class PathInfo(pathlib.PurePath):\n+    # Use __slots__ in PathInfo objects following PurePath implementation.\n+    # This makes objects smaller and speeds up attribute access.\n+    # We don't add any fields so it's empty.\n+    __slots__ = ()\n     scheme = \"local\"\n \n     def __new__(cls, *args):\n@@ -57,7 +61,9 @@ class PathInfo(pathlib.PurePath):\n             other = self.__class__(other)\n         elif self.__class__ != other.__class__:\n             return False\n-        return any(p == other for p in self.parents)\n+        # Use cached casefolded parts to compare paths\n+        n = len(other._cparts)\n+        return len(self._cparts) > n and self._cparts[:n] == other._cparts\n \n     # pathlib2 uses bytes internally in Python 2, and we use unicode everywhere\n     # for paths in both pythons, thus we need this glue.\n", "changed_method_name": "isin"}
{"commit_url": "https://github.com/sympy/sympy/commit/64f8c719146044058836d1fcc007ecef27b8023e", "commit_message": "Use xreplace instead of subs in Derivative instantiation\n\nSpeedup differentiation of expressions by using xreplace instead of\nsubs.", "code_diff": "@@ -1111,14 +1111,19 @@ def __new__(cls, expr, *variables, **assumptions):\n                 if not is_symbol:\n                     new_v = Dummy('xi_%i' % i)\n                     new_v.dummy_index = hash(v)\n-                    expr = expr.subs(v, new_v)\n+                    expr = expr.xreplace({v: new_v})\n                     old_v = v\n                     v = new_v\n                 obj = expr._eval_derivative(v)\n                 nderivs += 1\n                 if not is_symbol:\n                     if obj is not None:\n-                        obj = obj.subs(v, old_v)\n+                        if not old_v.is_Symbol and obj.is_Derivative:\n+                            # Derivative evaluated at a point that is not a\n+                            # symbol\n+                            obj = Subs(obj, v, old_v)\n+                        else:\n+                            obj = obj.xreplace({v: old_v})\n                     v = old_v\n \n             if obj is None:\n", "changed_method_name": "__new__"}
{"commit_url": "https://github.com/sympy/sympy/commit/fb1437ff39ba07db4449a81b3540537bef9ce900", "commit_message": "fix perfomance", "code_diff": "@@ -343,13 +343,16 @@ def eval(cls, arg):\n                 elif a.is_extended_positive:\n                     pass\n                 else:\n-                    ai = im(a)\n-                    if a.is_imaginary and ai.is_comparable:  # i.e. a = I*real\n-                        s *= S.ImaginaryUnit\n-                        if ai.is_extended_negative:\n-                            # can't use sign(ai) here since ai might not be\n-                            # a Number\n-                            s = -s\n+                    if a.is_imaginary:\n+                        ai = im(a)\n+                        if ai.is_comparable:  # i.e. a = I*real\n+                            s *= S.ImaginaryUnit\n+                            if ai.is_extended_negative:\n+                                # can't use sign(ai) here since ai might not be\n+                                # a Number\n+                                s = -s\n+                        else:\n+                            unk.append(a)\n                     else:\n                         unk.append(a)\n             if c is S.One and len(unk) == len(args):\n", "changed_method_name": "eval"}
{"commit_url": "https://github.com/sympy/sympy/commit/f7cecfea016dedf7ad042be0c8da9df1ef19c756", "commit_message": "perf(core): make sympify faster for Basic subclasses.\n\nThe behaviour of sympify was changed in\n\n  https://github.com/sympy/sympy/pull/20128\n\nso that sympifying a Basic subclass would be an error when strict=True.\nThat change made the codepath for calling e.g. _sympify(Basic) slower as\nmore checks would be done before returning. This commit adds an early\nraise for the case of calling _sympify(a) where a is a Basic subclass.", "code_diff": "@@ -344,8 +344,13 @@ def sympify(a, locals=None, convert_xor=True, strict=False, rational=False,\n     #\n     # https://github.com/sympy/sympy/issues/20124\n     is_sympy = getattr(a, '__sympy__', None)\n-    if is_sympy is True or (is_sympy is not None and not strict):\n+    if is_sympy is True:\n         return a\n+    elif is_sympy is not None:\n+        if not strict:\n+            return a\n+        else:\n+            raise SympifyError(a)\n \n     if isinstance(a, CantSympify):\n         raise SympifyError(a)\n", "changed_method_name": "sympify"}
{"commit_url": "https://github.com/python-pillow/Pillow/commit/eef4d1ced12119af1a8137adb1d5c1ce4d95da0f", "commit_message": "Moved mode check outside of loops", "code_diff": "@@ -432,18 +432,18 @@ fill_mask_L(\n         }\n \n     } else {\n+        int alpha_channel = strcmp(imOut->mode, \"RGBa\") == 0 ||\n+                            strcmp(imOut->mode, \"RGBA\") == 0 ||\n+                            strcmp(imOut->mode, \"La\") == 0 ||\n+                            strcmp(imOut->mode, \"LA\") == 0 ||\n+                            strcmp(imOut->mode, \"PA\") == 0;\n         for (y = 0; y < ysize; y++) {\n             UINT8 *out = (UINT8 *)imOut->image[y + dy] + dx * pixelsize;\n             UINT8 *mask = (UINT8 *)imMask->image[y + sy] + sx;\n             for (x = 0; x < xsize; x++) {\n                 for (i = 0; i < pixelsize; i++) {\n                     UINT8 channel_mask = *mask;\n-                    if ((strcmp(imOut->mode, \"RGBa\") == 0 ||\n-                         strcmp(imOut->mode, \"RGBA\") == 0 ||\n-                         strcmp(imOut->mode, \"La\") == 0 ||\n-                         strcmp(imOut->mode, \"LA\") == 0 ||\n-                         strcmp(imOut->mode, \"PA\") == 0) &&\n-                        i != 3 && channel_mask != 0) {\n+                    if (alpha_channel && i != 3 && channel_mask != 0) {\n                         channel_mask =\n                             255 - (255 - channel_mask) * (1 - (255 - out[3]) / 255);\n                     }\n", "changed_method_name": "fill_mask_L"}
{"commit_url": "https://github.com/frappe/frappe/commit/aeec01c7f933615c126fd7b6a1a832c54f8d230a", "commit_message": "perf(Scheduling): add jitter to job scheduling\n\nAddresses #19007", "code_diff": "@@ -2,7 +2,8 @@\n # License: MIT. See LICENSE\n \n import json\n-from datetime import datetime\n+from datetime import datetime, timedelta\n+from random import randint\n \n import click\n from croniter import croniter\n@@ -110,7 +111,12 @@ class ScheduledJobType(Document):\n \t\t# immediately, even when it's meant to be daily.\n \t\t# A dynamic fallback like current time might miss the scheduler interval and job will never start.\n \t\tlast_execution = get_datetime(self.last_execution or self.creation)\n-\t\treturn croniter(self.cron_format, last_execution).get_next(datetime)\n+\t\tnext_execution = croniter(self.cron_format, last_execution).get_next(datetime)\n+\n+\t\tjitter = 0\n+\t\tif self.frequency in (\"Hourly Long\", \"Daily Long\"):\n+\t\t\tjitter = randint(1, 600)\n+\t\treturn next_execution + timedelta(seconds=jitter)\n \n \tdef execute(self):\n \t\tself.scheduler_log = None\n", "changed_method_name": "get_next_execution"}
{"commit_url": "https://github.com/astropy/astropy/commit/752887c906ebc093c2b67474feb8e0b496887beb", "commit_message": "Replaced pow with a multiply", "code_diff": "@@ -43,7 +43,7 @@ void compute_sigma_clipped_bounds(double data_buffer[], int count, int use_media\n \n       std = 0;\n       for (i = 0; i < count; i++) {\n-        std += pow(mean - data_buffer[i], 2);\n+        std += (data_buffer[i] - mean) * (data_buffer[i] - mean);\n       }\n       std = sqrt(std / count);\n \n", "changed_method_name": "compute_sigma_clipped_bounds"}
{"commit_url": "https://github.com/Bitmessage/PyBitmessage/commit/03316496b7c3380c5ac408f86d049855dbcedac6", "commit_message": "Stop UDPSocket on socket.error 101 (Network is unreachable)", "code_diff": "@@ -146,6 +146,9 @@ class UDPSocket(BMProto):  # pylint: disable=too-many-instance-attributes\n             retval = self.socket.sendto(\n                 self.write_buf, ('<broadcast>', self.port))\n         except socket.error as e:\n-            logger.error(\"socket error on sendato: %s\", e)\n+            logger.error(\"socket error on sendto: %s\", e)\n+            if e.errno == 101:\n+                self.announcing = False\n+                self.socket.close()\n             retval = 0\n         self.slice_write_buf(retval)\n", "changed_method_name": "handle_write"}
{"commit_url": "https://github.com/Bitmessage/PyBitmessage/commit/11bec55be56e73502fbd569bf0327a8876b54315", "commit_message": "Don't put addresses into queue\n\n- attempt to fix #1598\n- seems to work\n- addresses won't be uploaded/announced anymore other than after connecting,\n  Later I need to find out how to announce them without causing problems, but\n  for the time disabling this seems an acceptable drawback", "code_diff": "@@ -31,7 +31,7 @@ from network.dandelion import Dandelion\n from network.proxy import ProxyError\n from node import Node, Peer\n from objectracker import ObjectTracker, missingObjects\n-from queues import addrQueue, invQueue, objectProcessorQueue, portCheckerQueue\n+from queues import invQueue, objectProcessorQueue, portCheckerQueue\n from randomtrackingdict import RandomTrackingDict\n \n logger = logging.getLogger('default')\n@@ -466,8 +466,9 @@ class BMProto(AdvancedDispatcher, ObjectTracker):\n                             }\n                     # since we don't track peers outside of knownnodes,\n                     # only spread if in knownnodes to prevent flood\n-                    addrQueue.put((stream, peer, seenTime,\n-                                   self.destination))\n+                    # DISABLED TO WORKAROUND FLOOD/LEAK\n+                    # addrQueue.put((stream, peer, seenTime,\n+                    #               self.destination))\n         return True\n \n     def bm_command_portcheck(self):\n", "changed_method_name": "bm_command_addr"}
{"commit_url": "https://github.com/python-rope/rope/commit/ae8ea5d0a615936bc6f5b0964910566565d6b6b7", "commit_message": "Modifies default_config.py", "code_diff": "@@ -15,7 +15,8 @@ def set_prefs(prefs):\n     # 'build/*.o': matches 'build/lib.o' but not 'build/sub/lib.o'\n     # 'build//*.o': matches 'build/lib.o' and 'build/sub/lib.o'\n     prefs['ignored_resources'] = ['*.pyc', '*~', '.ropeproject',\n-                                  '.hg', '.svn', '_svn', '.git', '.tox']\n+                                  '.hg', '.svn', '_svn', '.git', '.tox',\n+                                  '.venv', 'venv']\n \n     # Specifies which files should be considered python files.  It is\n     # useful when you have scripts inside your project.  Only files\n", "changed_method_name": "set_prefs"}
{"commit_url": "https://github.com/scikit-bio/scikit-bio/commit/76b40140b6e9104288932f60c2a10b7b445f41fa", "commit_message": "ENH: performance enhancement, fixes #411", "code_diff": "@@ -1014,7 +1014,7 @@ class BiologicalSequence(Sequence):\n             step = k\n \n         for i in range(0, sequence_length - k + 1, step):\n-            yield constructor(self[i:i+k])\n+            yield self._sequence[i:i+k]\n \n     def k_word_counts(self, k, overlapping=True, constructor=str):\n         \"\"\"Get the counts of words of length k\n", "changed_method_name": "k_words"}
{"commit_url": "https://github.com/microsoft/ptvsd/commit/1a13aa2450fee2a0aac4ecbe84d448a1700309b7", "commit_message": "Fix #967: Debugging Django in VSC with subprocess debugging is very slow\n\nAvoid hogging CPU while waiting on the subprocess notification queue.", "code_diff": "@@ -1296,7 +1296,7 @@ class VSCodeMessageProcessor(VSCLifecycleMsgProcessor):\n     def _subprocess_notifier(self):\n         while not self.closed:\n             try:\n-                subprocess_request, subprocess_response = multiproc.subprocess_queue.get(block=False, timeout=0.1)\n+                subprocess_request, subprocess_response = multiproc.subprocess_queue.get(timeout=0.1)\n             except queue.Empty:\n                 continue\n \n", "changed_method_name": "_subprocess_notifier"}
{"commit_url": "https://github.com/GeoStat-Framework/GSTools/commit/c88f7bab0e309f7a32c1a61786ba340245d215a2", "commit_message": "emcee: vectorize sampling since all ln_pdfs accept numpy arrays", "code_diff": "@@ -86,7 +86,7 @@ class RNG:\n             self.random.rand(nwalkers).reshape((nwalkers, 1)) * sample_around\n         )\n         # initialize the sampler\n-        sampler = mc.EnsembleSampler(nwalkers, 1, ln_pdf)\n+        sampler = mc.EnsembleSampler(nwalkers, 1, ln_pdf, vectorize=True)\n         # burn in phase with saving of last position\n         initial_state = State(init_guess, copy=True)\n         initial_state.random_state = self.random.get_state()\n", "changed_method_name": "sample_ln_pdf"}
{"commit_url": "https://github.com/datalad/datalad/commit/de284831b35291be1abead557e7ef98fc266d400", "commit_message": "OPT: Faster approach to GitRepo.get_hexsha()\n\nThe previous `git-show` can be slow with more complex commits (e.g.\noctopus merge commit, see\nhttps://github.com/datalad/datalad/issues/4801)\n\nSwitch to `git-rev-parse` instead as suggested by @kyleam in\nhttps://github.com/datalad/datalad/issues/4801#issuecomment-669480561\n\nBehavior of `GitRepo.get_hexsha()` should stay constant for all\nobserved usage patterns. In particular raising a ValueError when\nquerying for a specific, but non existent commitish is critical\nfor subdataset handling.\n\nFixes gh-4801", "code_diff": "@@ -1741,14 +1741,28 @@ class GitRepo(RepoInterface, metaclass=PathBasedFlyweight):\n \n         Returns\n         -------\n-        str or, if there are not commits yet, None.\n+        str or, if no commitish was given and there are no commits yet, None.\n+\n+        Raises\n+        ------\n+        ValueError\n+          If a commitish was given, but no corresponding commit could be\n+          determined.\n         \"\"\"\n-        stdout = self.format_commit(\"%{}\".format('h' if short else 'H'),\n-                                    commitish)\n-        if stdout is not None:\n-            stdout = stdout.splitlines()\n-            assert(len(stdout) == 1)\n-            return stdout[0]\n+        # use --quiet because the 'Needed a single revision' error message\n+        # that is the result of running this in a repo with no commits\n+        # isn't useful to report\n+        cmd = ['rev-parse', '--quiet', '--verify', '{}^{{commit}}'.format(\n+            commitish if commitish else 'HEAD')\n+        ]\n+        if short:\n+            cmd.append('--short')\n+        try:\n+            return self.call_git_oneline(cmd)\n+        except CommandError as e:\n+            if commitish is None:\n+                return None\n+            raise ValueError(\"Unknown commit identifier: %s\" % commitish)\n \n     @normalize_paths(match_return_type=False)\n     def get_last_commit_hexsha(self, files):\n", "changed_method_name": "get_hexsha"}
{"commit_url": "https://github.com/datalad/datalad/commit/01c5a68a4b8aaaf45726091c8154e94ec07af4a3", "commit_message": "OPT: Faster approach to GitRepo.format_commit()\n\nThe previous git-show can be slow with more complex commits (e.g.\noctopus merge commit, see gh-4801)\n\nSwitch to `git-log`", "code_diff": "@@ -1708,14 +1708,17 @@ class GitRepo(RepoInterface, metaclass=PathBasedFlyweight):\n         -------\n         str or, if there are not commits yet, None.\n         \"\"\"\n-        cmd = ['git', 'show', '-z', '--no-patch', '--format=' + fmt]\n+        # use git-log and not git-show due to faster performance with\n+        # complex commits (e.g. octopus merges)\n+        # https://github.com/datalad/datalad/issues/4801\n+        cmd = ['log', '-1', '-z', '--format=' + fmt]\n         if commitish is not None:\n             cmd.append(commitish + \"^{commit}\")\n         # make sure Git takes our argument as a revision\n         cmd.append('--')\n         try:\n-            stdout, stderr = self._git_custom_command(\n-                '', cmd, expect_stderr=True, expect_fail=True)\n+            stdout = self.call_git(\n+                cmd, expect_stderr=True, expect_fail=True)\n         except CommandError as e:\n             if 'bad revision' in e.stderr:\n                 raise ValueError(\"Unknown commit identifier: %s\" % commitish)\n", "changed_method_name": "format_commit"}
{"commit_url": "https://github.com/yt-project/yt/commit/68068dd0212b3cb5bad2fde085ddc8be4cff4a33", "commit_message": "I believe this logic needs to be reversed.", "code_diff": "@@ -218,12 +218,12 @@ class ParticleIndex(Index):\n             else:\n                 # TODO: only return files\n                 if getattr(dobj.selector, 'is_all_data', False):\n+                    nfiles = self.regions.nfiles\n+                    dfi = np.arange(nfiles)\n+                else:\n                     dfi, file_masks, addfi = self.regions.identify_file_masks(\n                         dobj.selector)\n                     nfiles = len(file_masks)\n-                else:\n-                    nfiles = self.regions.nfiles\n-                    dfi = np.arange(nfiles)\n                 dobj._chunk_info = [None for _ in range(nfiles)]\n                 for i in range(nfiles):\n                     domain_id = i+1\n", "changed_method_name": "_identify_base_chunk"}
{"commit_url": "https://github.com/mindsdb/lightwood/commit/f4449086b19f018d3c5d5fb8741fa8ecf6b50d1b", "commit_message": "fix: activate optuna for lgbm_array iff horizon < 10", "code_diff": "@@ -49,7 +49,7 @@ class LightGBMArray(BaseMixer):\n                                 dtype_dict,\n                                 input_cols,\n                                 False,  # fit_on_dev,\n-                                True,  # use_optuna\n+                                True if tss.horizon < 10 else False,  # use_optuna\n                                 target_encoder)\n                        for _, target_col in zip(range(self.horizon), [target] + self.offset_pred_cols)]\n         self.ts_analysis = ts_analysis\n", "changed_method_name": "__init__"}
{"commit_url": "https://github.com/static-frame/static-frame/commit/2cda5f824ba56a082b3874a836c4a7c8c607d902", "commit_message": "'Fix' property test by ignoring date cases. Is referenced by issue #132", "code_diff": "@@ -158,7 +158,9 @@ class TestUnit(TestCase):\n     @given(sfst.get_frame()) # type: ignore\n     def test_frame_isin(self, f1: Frame) -> None:\n         value = f1.iloc[0, 0]\n-        if not isna_element(value):\n+        if (not isna_element(value) and\n+                not isinstance(value, np.datetime64) and\n+                not isinstance(value, np.timedelta64)):\n             self.assertTrue(f1.isin((value,)).iloc[0, 0])\n \n \n", "changed_method_name": "test_frame_isin"}
{"commit_url": "https://github.com/gem/oq-engine/commit/6c563f3ee2e20a7084a3cdacb1c43fa9538dbb81", "commit_message": "Optimized the management of the assets in ebrisk by using pandas [skip hazardlib][demos]", "code_diff": "@@ -44,13 +44,18 @@ gmf_info_dt = numpy.dtype([('rup_id', U32), ('task_no', U16),\n \n \n def calc_risk(gmfs, param, monitor):\n+    \"\"\"\n+    :param gmfs: an array of GMFs with fields sid, eid, gmv\n+    :param param: a dictionary of parameters coming from the job.ini\n+    :param monitor: a Monitor instance\n+    :returns: a dictionary of arrays with keys elt, alt, losses_by_A, ...\n+    \"\"\"\n     mon_risk = monitor('computing risk', measuremem=False)\n     mon_agg = monitor('aggregating losses', measuremem=False)\n     eids = numpy.unique(gmfs['eid'])\n     dstore = datastore.read(param['hdf5path'])\n     with monitor('getting assets'):\n-        assetcol = dstore['assetcol']\n-        assets_by_site = assetcol.assets_by_site()\n+        assets_df = dstore.read_df('assetcol/array', 'ordinal')\n         exposed_values = dstore['exposed_values/agg'][()]\n     with monitor('getting crmodel'):\n         crmodel = riskmodels.CompositeRiskModel.read(dstore)\n@@ -70,53 +75,58 @@ def calc_risk(gmfs, param, monitor):\n     aggby = param['aggregate_by']\n \n     minimum_loss = []\n-    fraction = param['minimum_loss_fraction'] / len(assetcol)\n+    fraction = param['minimum_loss_fraction'] / len(assets_df)\n     for lt, lti in crmodel.lti.items():\n         val = exposed_values[lti] * fraction\n         minimum_loss.append(val)\n         if lt in lba.policy_dict:  # same order as in lba.compute\n             minimum_loss.append(val)\n \n-    for sid, haz in general.group_array(gmfs, 'sid').items():\n-        assets_on_sid = assets_by_site[sid]\n-        if len(assets_on_sid) == 0:\n+    haz_by_sid = general.group_array(gmfs, 'sid')\n+    for sid, asset_df in assets_df.groupby('site_id'):\n+        try:\n+            haz = haz_by_sid[sid]\n+        except KeyError:  # no hazard here\n             continue\n+        assets = asset_df.to_records()\n         with mon_risk:\n             acc['events_per_sid'] += len(haz)\n             if param['avg_losses']:\n                 ws = weights[[eid2rlz[eid] for eid in haz['eid']]]\n-            assets_by_taxo = get_assets_by_taxo(assets_on_sid, tempname)\n+            assets_by_taxo = get_assets_by_taxo(assets, tempname)  # fast\n             eidx = numpy.array([eid2idx[eid] for eid in haz['eid']])\n             out = get_output(crmodel, assets_by_taxo, haz)\n-        for lti, lt in enumerate(crmodel.loss_types):\n-            lratios = out[lt]\n-            if lt == 'occupants':\n-                field = 'occupants_None'\n-            else:\n-                field = 'value-' + lt\n-            for a, asset in enumerate(assets_on_sid):\n+        with mon_agg:\n+            for lti, lt in enumerate(crmodel.loss_types):\n+                lratios = out[lt]\n+                if lt == 'occupants':\n+                    field = 'occupants_None'\n+                else:\n+                    field = 'value-' + lt\n                 if aggby:\n-                    idx = ','.join(map(str, asset[aggby]))\n-                aid = asset['ordinal']\n-                ls = asset[field] * lratios[a]\n-                for loss_idx, losses in lba.compute(asset, ls, lt):\n-                    kept = 0\n-                    with mon_agg:\n+                    tagidxs = assets[aggby]\n+                for a, asset in enumerate(assets):\n+                    if aggby:\n+                        idx = ','.join(map(str, tagidxs[a]))\n+                    aid = asset['ordinal']\n+                    ls = asset[field] * lratios[a]\n+                    for loss_idx, losses in lba.compute(asset, ls, lt):\n+                        kept = 0\n                         if aggby:\n                             for loss, eid in zip(losses, out.eids):\n                                 if loss >= minimum_loss[loss_idx]:\n                                     alt[idx][eid][loss_idx] += loss\n                                     kept += 1\n                         arr[eidx, loss_idx] += losses\n-                    if param['avg_losses']:  # this is really fast\n-                        lba.losses_by_A[aid, loss_idx] += losses @ ws\n-                    acc['numlosses'] += numpy.array([kept, len(losses)])\n+                        if param['avg_losses']:  # this is really fast\n+                            lba.losses_by_A[aid, loss_idx] += losses @ ws\n+                        acc['numlosses'] += numpy.array([kept, len(losses)])\n     if len(gmfs):\n         acc['events_per_sid'] /= len(gmfs)\n     acc['elt'] = numpy.fromiter(  # this is ultra-fast\n         ((event['id'], event['rlz_id'], losses)\n          for event, losses in zip(events, arr) if losses.sum()), elt_dt)\n-    acc['alt'] = {idx: numpy.fromiter(  # already sorted by aid\n+    acc['alt'] = {idx: numpy.fromiter(  # already sorted by aid, ultra-fast\n         ((eid, eid2rlz[eid], loss) for eid, loss in alt[idx].items()),\n         elt_dt) for idx in alt}\n     if param['avg_losses']:\n", "changed_method_name": "calc_risk"}
{"commit_url": "https://github.com/gem/oq-engine/commit/37c2e86bd0821960318310771d6d7f824b33c7e5", "commit_message": "Optimizing gmpe_table._get_mean", "code_diff": "@@ -50,11 +50,8 @@ def _get_mean_(kind, data, dists, table_dists):\n     :return:\n         The mean intensity measure level from the tables.\n     \"\"\"\n-    # For values outside of the interpolation range use -999. to ensure\n     # value is identifiable and outside of potential real values\n-    interpolator_mean = interp1d(\n-        table_dists, data, bounds_error=False, fill_value=-999.)\n-    mean = interpolator_mean(dists)\n+    mean = numpy.interp(dists, table_dists, data)\n     # For those distances less than or equal to the shortest distance\n     # extrapolate the shortest distance value\n     mean[dists < (table_dists[0] + 1.0E-3)] = data[0]\n", "changed_method_name": "_get_mean_"}
{"commit_url": "https://github.com/gem/oq-engine/commit/be813aa4a76c3689f45aca18c7b93aa64a446c0b", "commit_message": "Fixed misprint making counting the ruptures ultra-slow in event_based", "code_diff": "@@ -403,7 +403,7 @@ class EventBasedCalculator(base.HazardCalculator):\n         with self.monitor('counting ruptures', measuremem=True):\n             nrups = parallel.Starmap( # weighting the heavy sources\n                 count_ruptures, [(src,) for src in sources\n-                                 if src.code == b'AMSC'],\n+                                 if src.code in b'AMSC'],\n                 progress=logging.debug).reduce()\n             # NB: multifault sources must be considered light to avoid a large\n             # data transfer, even if .count_ruptures can be slow\n", "changed_method_name": "build_events_from_sources"}
{"commit_url": "https://github.com/gem/oq-engine/commit/c9a92dac9b354a5418c8771ceb5b7d529186c728", "commit_message": "Gzipping the _rates", "code_diff": "@@ -400,7 +400,7 @@ class ClassicalCalculator(base.HazardCalculator):\n         self.cmakers = read_cmakers(self.datastore, self.csm)\n         self.cfactor = numpy.zeros(3)\n         self.rel_ruptures = AccumDict(accum=0)  # grp_id -> rel_ruptures\n-        self.datastore.create_df('_rates', rates_dt.items())\n+        self.datastore.create_df('_rates', rates_dt.items(), 'gzip')\n         self.datastore.create_dset('_rates/slice_by_sid', slice_dt)\n         # NB: compressing the dataset causes a big slowdown in writing :-(\n \n", "changed_method_name": "init_poes"}
{"commit_url": "https://github.com/biocore/qiime/commit/3a5c9966facda29041a5aebfc81ea0c0ebf2238e", "commit_message": "TST/REVERT: reduction of the runtime of the test, plus reversion of misguided attept at doing that yesterday", "code_diff": "@@ -145,8 +145,9 @@ class CoreDiversityAnalysesTests(TestCase):\n             20,\n             output_dir=self.test_out,\n             params=parse_qiime_parameters({}),\n+            arare_num_steps=3,\n             qiime_config=self.qiime_config,\n-            categories=['SampleType'],\n+            categories=['SampleType', 'days_since_epoch'],\n             tree_fp=self.test_data['tree'][0],\n             parallel=True,\n             status_update_callback=no_status_updates)\n@@ -158,7 +159,7 @@ class CoreDiversityAnalysesTests(TestCase):\n             '%s/taxa_plots' % self.test_out,\n             '%s/bdiv_even20/unweighted_unifrac_dm.txt' % self.test_out,\n             '%s/bdiv_even20/weighted_unifrac_pc.txt' % self.test_out,\n-            '%s/arare_max20/compare_chao1/SampleType_stats.txt' % self.test_out,\n+            '%s/arare_max20/compare_chao1/days_since_epoch_stats.txt' % self.test_out,\n             '%s/arare_max20/compare_PD_whole_tree/SampleType_boxplots.pdf' % self.test_out,\n             '%s/index.html' % self.test_out,\n             '%s/table_mc%d.biom.gz' % (self.test_out, 20)\n", "changed_method_name": "test_run_core_diversity_analyses_parallel"}
{"commit_url": "https://github.com/gocd/gocd/commit/aeb493d2a43c22d6af90735bbe78b077bd4d716d", "commit_message": "#1249 Refactor validateAllScmRevisionsAreSameWithinAFingerprint to have linear complexity", "code_diff": "@@ -18,6 +18,7 @@ package com.thoughtworks.go.server.service.dd;\n \n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.HashMap;\n import java.util.HashSet;\n import java.util.LinkedHashMap;\n import java.util.List;\n@@ -36,7 +37,6 @@ import com.thoughtworks.go.server.domain.PipelineTimeline;\n import com.thoughtworks.go.server.service.NoCompatibleUpstreamRevisionsException;\n import com.thoughtworks.go.util.Pair;\n import org.apache.commons.collections.CollectionUtils;\n-import org.apache.commons.collections.Predicate;\n import org.apache.log4j.Logger;\n \n import static com.thoughtworks.go.server.service.dd.DependencyFanInNode.RevisionAlteration.ALL_OPTIONS_EXHAUSTED;\n@@ -180,19 +180,14 @@ public class DependencyFanInNode extends FanInNode {\n         if (pIdScmPair == null) {\n             return false;\n         }\n+        Map<FaninScmMaterial, PipelineTimelineEntry.Revision> versionsByMaterial = new HashMap<FaninScmMaterial, PipelineTimelineEntry.Revision>();\n         List<FaninScmMaterial> scmMaterialList = pIdScmPair.last();\n         for (final FaninScmMaterial scmMaterial : scmMaterialList) {\n-            Collection<FaninScmMaterial> scmMaterialOfSameFingerprint = CollectionUtils.select(scmMaterialList, new Predicate() {\n-                @Override\n-                public boolean evaluate(Object o) {\n-                    return scmMaterial.equals(o);\n-                }\n-            });\n-\n-            for (FaninScmMaterial faninScmMaterial : scmMaterialOfSameFingerprint) {\n-                if (!faninScmMaterial.revision.equals(scmMaterial.revision)) {\n-                    return false;\n-                }\n+            PipelineTimelineEntry.Revision revision = versionsByMaterial.get(scmMaterial);\n+            if (revision == null) {\n+                versionsByMaterial.put(scmMaterial, scmMaterial.revision);\n+            } else if (!revision.equals(scmMaterial.revision)) {\n+                return false;\n             }\n         }\n         return true;\n", "changed_method_name": "DependencyFanInNode::validateAllScmRevisionsAreSameWithinAFingerprint"}
{"commit_url": "https://github.com/gocd/gocd/commit/52d5ddd78f1edb926219c207a0ad4b73b5668470", "commit_message": "Ensure input streams are buffered when unzipping uploaded artifacts\n\nCurrently these streams are generally coming from Spring/Jetty from `MultipartFile`s. It seems\nthey are generally handed off to disk (probably due to size of upload) and thus `FileInputStream`s\nwhich are not buffered. We buffer explicitly in other cases so let's buffer here too. On slower disks\nthis might make quite a big difference. Local testing on a super-fast SSD even shows marked difference\nin speed when buffering the input stream.", "code_diff": "@@ -32,10 +32,7 @@ import org.slf4j.LoggerFactory;\n import org.springframework.beans.factory.annotation.Autowired;\n import org.springframework.stereotype.Service;\n \n-import java.io.File;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.io.InputStream;\n+import java.io.*;\n import java.util.zip.ZipInputStream;\n \n import static com.thoughtworks.go.util.SystemEnvironment.ARTIFACT_COPY_BUFFER_SIZE;\n@@ -43,7 +40,7 @@ import static java.lang.String.format;\n \n @Service\n public class ArtifactsService implements ArtifactUrlReader {\n-    public static final Logger LOGGER = LoggerFactory.getLogger(ArtifactsService.class);\n+    private static final Logger LOGGER = LoggerFactory.getLogger(ArtifactsService.class);\n     public static final String LOG_XML_NAME = \"log.xml\";\n     private final ArtifactsDirHolder artifactsDirHolder;\n     private final ZipUtil zipUtil;\n@@ -80,7 +77,7 @@ public class ArtifactsService implements ArtifactUrlReader {\n         try {\n             LOGGER.trace(\"Saving file [{}]\", destPath);\n             if (shouldUnzip) {\n-                zipUtil.unzip(new ZipInputStream(stream), dest);\n+                zipUtil.unzip(new ZipInputStream(IOUtils.buffer(stream, bufferSize)), dest);\n             } else {\n                 try (FileOutputStream out = FileUtils.openOutputStream(dest, true)) {\n                     IOUtils.copy(stream, out, bufferSize);\n", "changed_method_name": "ArtifactsService::saveFile"}
{"commit_url": "https://github.com/aol/cyclops/commit/2ccc16aa8d87c73e96420d7410d435655ab013e7", "commit_message": "Fix for #1056", "code_diff": "@@ -260,7 +260,11 @@ public interface Seq<T> extends ImmutableList<T>,\n         return Cons.cons(value,this);\n     }\n     default Seq<T> prependAll(Iterable<? extends T> it){\n-      return (Seq<T>)ImmutableList.super.prependAll(it);\n+        Seq<T> res = this;\n+        for(T next : it){\n+            res = res.prepend(next);\n+        }\n+      return res;\n     }\n \n     default Seq<T> take(final long num) {\n", "changed_method_name": "prependAll"}
{"commit_url": "https://github.com/RPTools/maptool/commit/565f5c92c1d671afb74ada7e5270dc1f89bf629a", "commit_message": "Fix JTS geometry errors in calculateVisibility()\n\nThere are lots of ways in which cleared area geometry can become invalid, even though it is composed of many valid\npolygons. Operations like unions are particularly susceptible to breaking as a result. To avoid issues with unions of\ncleared area while also avoiding excessive buffering, we now build a `GeometryCollection` containing all of the cleared\narea geometry. We then `buffer(0)` that, which has the effect of creating a single valid geometry without growing the\ngeometry at all.\n\nIn order to make this work robustly without having to buffer each individual cleared area, we no longer try to union\nindividual cleared areas together and instead only union the many cleared areas in a single `UnaryUnionOp`.  Although\nthis can theoretically limit the number of culled segments, performance actually improves in practice.", "code_diff": "@@ -59,6 +59,7 @@\n import org.apache.logging.log4j.Logger;\n import org.locationtech.jts.awt.ShapeWriter;\n import org.locationtech.jts.geom.Geometry;\n+import org.locationtech.jts.geom.GeometryFactory;\n import org.locationtech.jts.geom.prep.PreparedGeometry;\n import org.locationtech.jts.geom.prep.PreparedGeometryFactory;\n import org.locationtech.jts.operation.union.UnaryUnionOp;\n@@ -110,26 +111,19 @@ public static Area calculateVisibility(int x, int y, Area vision, AreaTree topol\n       }\n       var area = segment.getGeometry();\n \n-      timer.start(\"combine\");\n-      Geometry intersectedArea = area;\n-      for (var iter = clearedAreaList.listIterator(); iter.hasNext(); ) {\n-        var clearedArea = iter.next();\n-        if (clearedArea.intersects(boundingBox)) {\n-          intersectedArea = clearedArea.getGeometry().union(area);\n-          iter.remove(); // we'll put it on the back of the list to prevent crazy growth at the\n-          // front\n-          break;\n-        }\n-      }\n-      timer.stop(\"combine\");\n-\n-      clearedAreaList.add(PreparedGeometryFactory.prepare(intersectedArea));\n+      clearedAreaList.add(PreparedGeometryFactory.prepare(area));\n     }\n \n     if (!clearedAreaList.isEmpty()) {\n-      List<Geometry> plainGeometries =\n-          clearedAreaList.stream().map(PreparedGeometry::getGeometry).toList();\n-      var totalClearedArea = new UnaryUnionOp(plainGeometries).union();\n+\n+      var geometry =\n+          new GeometryFactory()\n+              .createGeometryCollection(\n+                  clearedAreaList.stream()\n+                      .map(PreparedGeometry::getGeometry)\n+                      .toArray(Geometry[]::new))\n+              .buffer(0);\n+      var totalClearedArea = new UnaryUnionOp(geometry).union();\n \n       // Convert back to AWT area to modify vision.\n       var shapeWriter = new ShapeWriter();\n", "changed_method_name": "FogUtil::calculateVisibility"}
{"commit_url": "https://github.com/RPTools/maptool/commit/0fd4d1e1451dc0244360cba44830787ec04b7696", "commit_message": "Remove extra vision transform that is no longer needed", "code_diff": "@@ -147,7 +147,6 @@ protected void paintComponent(Graphics g) {\n             pitVblTree);\n \n     final var obstructedVision = new Area(unobstructedVision);\n-    obstructedVision.transform(AffineTransform.getTranslateInstance(point.getX(), point.getY()));\n \n     g2d.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_OVER, .5f));\n     if (vision != null) {\n", "changed_method_name": "VisibilityInspector::paintComponent"}
{"commit_url": "https://github.com/RPTools/maptool/commit/e7086a8db30ad9bc350bfd782f047428df126be7", "commit_message": "Include temporary token light sources\n\nThis adds back in necessary functionality for Expose Last Path to function correctly.", "code_diff": "@@ -455,6 +455,13 @@ private Illumination getIllumination(IlluminationKey illuminationKey) {\n         illuminationKey, key -> getUpToDateIlluminator(key).getIllumination());\n   }\n \n+  /**\n+   * Add personal lights and daylight for a token, as well as any normal lights if the token is\n+   * temporary.\n+   *\n+   * @param token\n+   * @return All extra light contributions to be made for this token.\n+   */\n   private @Nonnull List<ContributedLight> getPersonalTokenContributions(Token token) {\n     if (!token.getHasSight()) {\n       return Collections.emptyList();\n@@ -477,6 +484,14 @@ private Illumination getIllumination(IlluminationKey illuminationKey) {\n         personalLights.add(contributedLight);\n       }\n \n+      if (token.hasLightSources()\n+          && !lightSourceMap\n+              .getOrDefault(LightSource.Type.NORMAL, Collections.emptySet())\n+              .contains(token.getId())) {\n+        // This accounts for temporary tokens (such as during an Expose Last Path)\n+        personalLights.addAll(calculateLitAreas(token, sight.getMultiplier()));\n+      }\n+\n       if (sight.hasPersonalLightSource()) {\n         // Calculate the personal light area here.\n         // Note that a personal light is affected by its own sight's magnification, but that's it.\n", "changed_method_name": "ZoneView::getPersonalTokenContributions"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/d0cf26216986d1e7689369d0f7b01490a6a2e9c6", "commit_message": "#4346: Listing samples for a lab officer can be slow", "code_diff": "@@ -305,6 +305,12 @@ public class SampleService extends AbstractCoreAdoService<Sample> {\n \n \t\tPredicate filter = createUserFilterWithoutCase(cb, joins);\n \n+\t\tUser currentUser = getCurrentUser();\n+\t\tfinal JurisdictionLevel jurisdictionLevel = currentUser.getJurisdictionLevel();\n+\t\tif (jurisdictionLevel == JurisdictionLevel.LABORATORY || jurisdictionLevel == JurisdictionLevel.EXTERNAL_LABORATORY) {\n+\t\t\treturn filter;\n+\t\t}\n+\n \t\tif (criteria != null) {\n \t\t\tfinal SampleAssociationType sampleAssociationType = criteria.getSampleAssociationType();\n \t\t\tif (sampleAssociationType == SampleAssociationType.CASE) {\n", "changed_method_name": "SampleService::createUserFilter"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/68c630f69c2882919152a077532652bdabf9cda3", "commit_message": "#4346: Workaround/Bugfix: Editing Samples not working", "code_diff": "@@ -1,10 +1,13 @@\n package de.symeda.sormas.backend.sample;\n \n+import java.util.EnumSet;\n+\n import javax.ejb.EJB;\n import javax.ejb.LocalBean;\n import javax.ejb.Stateless;\n \n import de.symeda.sormas.api.sample.SampleJurisdictionDto;\n+import de.symeda.sormas.api.user.JurisdictionLevel;\n import de.symeda.sormas.api.utils.jurisdiction.SampleJurisdictionHelper;\n import de.symeda.sormas.backend.event.EventParticipant;\n import de.symeda.sormas.backend.event.EventParticipantJurisdictionChecker;\n@@ -33,10 +36,13 @@ public class SampleJurisdictionChecker {\n \n \t\tUser user = userService.getCurrentUser();\n \n-\t\tif (sampleJurisdiction.getEventParticipantJurisdiction() != null) {\n-\t\t\tEventParticipant sampleEventParticipant =\n-\t\t\t\teventParticipantService.getByUuid(sampleJurisdiction.getEventParticipantJurisdiction().getEventParticipantUuid());\n-\t\t\treturn eventParticipantJurisdictionChecker.isInJurisdiction(sampleEventParticipant);\n+\t\tif (!EnumSet.of(JurisdictionLevel.NONE, JurisdictionLevel.NATION, JurisdictionLevel.LABORATORY, JurisdictionLevel.EXTERNAL_LABORATORY)\n+\t\t\t.contains(user.getJurisdictionLevel())) {\n+\t\t\tif (sampleJurisdiction.getEventParticipantJurisdiction() != null) {\n+\t\t\t\tEventParticipant sampleEventParticipant =\n+\t\t\t\t\teventParticipantService.getByUuid(sampleJurisdiction.getEventParticipantJurisdiction().getEventParticipantUuid());\n+\t\t\t\treturn eventParticipantJurisdictionChecker.isInJurisdiction(sampleEventParticipant);\n+\t\t\t}\n \t\t}\n \n \t\treturn SampleJurisdictionHelper\n", "changed_method_name": "SampleJurisdictionChecker::isInJurisdictionOrOwned"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/fcf3c0746d6e37ff18a376b7d2adeb19ec6195d8", "commit_message": "#5612: Increased chunk size for lazy loading to 100\n\n- Avoids a 2nd query when initializing a list view", "code_diff": "@@ -12,8 +12,8 @@ import com.vaadin.server.SerializableSupplier;\n import com.vaadin.ui.Grid;\n import com.vaadin.ui.renderers.HtmlRenderer;\n \n-import de.symeda.sormas.api.utils.criteria.BaseCriteria;\n import de.symeda.sormas.api.i18n.I18nProperties;\n+import de.symeda.sormas.api.utils.criteria.BaseCriteria;\n \n public class FilteredGrid<T, C extends BaseCriteria> extends Grid<T> {\n \n@@ -21,11 +21,17 @@ public class FilteredGrid<T, C extends BaseCriteria> extends Grid<T> {\n \n \tprivate static final long serialVersionUID = 8116377533153377424L;\n \n+\t/**\n+\t * For lazy loading: Defines how many entries are loaded into the grid when new data needs to be loaded for the visible range.\n+\t */\n+\tprivate static final int LAZY_BATCH_SIZE = 100;\n+\n \tprivate C criteria;\n \tprivate boolean inEagerMode;\n \n \tpublic FilteredGrid(Class<T> beanType) {\n \t\tsuper(beanType);\n+\t\tgetDataCommunicator().setMinPushSize(LAZY_BATCH_SIZE);\n \t}\n \n \tpublic C getCriteria() {\n", "changed_method_name": "FilteredGrid::FilteredGrid"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/b692df37993975be3ed7f20801bcfa81e4b18103", "commit_message": "#5644 - fix issue editing contact on mobile app", "code_diff": "@@ -48,7 +48,9 @@ public abstract class JurisdictionValidator<T> {\n \t\t\tfinal List<T> jurisdictionTypes = new ArrayList<>();\n \t\t\tjurisdictionTypes.add(isInJurisdiction());\n \t\t\tfor (JurisdictionValidator<T> jurisdictionValidator : associatedJurisdictionValidators) {\n-\t\t\t\tjurisdictionTypes.add(jurisdictionValidator.isInJurisdiction());\n+\t\t\t\tif (jurisdictionValidator != null) {\n+\t\t\t\t\tjurisdictionTypes.add(jurisdictionValidator.isInJurisdiction());\n+\t\t\t\t}\n \t\t\t}\n \t\t\treturn or(jurisdictionTypes);\n \t\t} else {\n", "changed_method_name": "JurisdictionValidator::inJurisdiction"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/5752ebd20a0c9e308984349c1382c696fb0e365d", "commit_message": "#6204 - reduce non needed complexity for all queries that use event user filter with health facility user", "code_diff": "@@ -396,6 +396,7 @@ public class EventService extends AbstractCoreAdoService<Event> {\n \t\t\t\tfilter = CriteriaBuilderHelper\n \t\t\t\t\t.or(cb, filter, cb.equal(eventJoins.getLocation().get(Location.DISTRICT), currentUser.getHealthFacility().getDistrict()));\n \t\t\t}\n+\t\t\tbreak;\n \t\tcase LABORATORY:\n \t\t\tfinal Subquery<Long> sampleSubQuery = cq.subquery(Long.class);\n \t\t\tfinal Root<Sample> sampleRoot = sampleSubQuery.from(Sample.class);\n", "changed_method_name": "EventService::createUserFilter"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/b6034b8b31cfac3c57fb97bc030566bb66b047fc", "commit_message": "Remove pointless method calls in onPersonChanged #6362", "code_diff": "@@ -1096,34 +1096,38 @@ public class PersonFacadeEjb implements PersonFacade {\n \n \tpublic void onPersonChanged(PersonDto existingPerson, Person newPerson, boolean syncShares) {\n \n-\t\tList<Case> personCases = caseService.findBy(new CaseCriteria().person(new PersonReferenceDto(newPerson.getUuid())), true);\n-\t\t// Call onCaseChanged once for every case to update case classification\n-\t\t// Attention: this may lead to infinite recursion when not properly implemented\n-\t\tfor (Case personCase : personCases) {\n-\t\t\tCaseDataDto existingCase = CaseFacadeEjbLocal.toDto(personCase);\n-\t\t\tcaseFacade.onCaseChanged(existingCase, personCase, syncShares);\n-\t\t}\n+\t\tList<Case> personCases = null;\n \n-\t\tList<Contact> personContacts = contactService.findBy(new ContactCriteria().setPerson(new PersonReferenceDto(newPerson.getUuid())), null);\n-\t\t// Call onContactChanged once for every contact\n-\t\t// Attention: this may lead to infinite recursion when not properly implemented\n-\t\tfor (Contact personContact : personContacts) {\n-\t\t\tcontactFacade.onContactChanged(ContactFacadeEjbLocal.toDto(personContact), syncShares);\n-\t\t}\n+\t\t// Update cases if present condition has changed\n+\t\t// Do not bother to update existing cases/contacts/eventparticipants on new Persons, as none should exist yet\n+\t\tif (existingPerson != null) {\n \n-\t\tList<EventParticipant> personEventParticipants =\n-\t\t\teventParticipantService.findBy(new EventParticipantCriteria().withPerson(new PersonReferenceDto(newPerson.getUuid())), null);\n-\t\t// Call onEventParticipantChange once for every event participant\n-\t\t// Attention: this may lead to infinite recursion when not properly implemented\n-\t\tfor (EventParticipant personEventParticipant : personEventParticipants) {\n-\t\t\teventParticipantFacade.onEventParticipantChanged(EventFacadeEjbLocal.toDto(personEventParticipant.getEvent()), syncShares);\n-\t\t}\n+\t\t\tpersonCases = caseService.findBy(new CaseCriteria().person(new PersonReferenceDto(newPerson.getUuid())), true);\n+\t\t\t// Call onCaseChanged once for every case to update case classification\n+\t\t\t// Attention: this may lead to infinite recursion when not properly implemented\n+\t\t\tfor (Case personCase : personCases) {\n+\t\t\t\tCaseDataDto existingCase = CaseFacadeEjbLocal.toDto(personCase);\n+\t\t\t\tcaseFacade.onCaseChanged(existingCase, personCase, syncShares);\n+\t\t\t}\n \n-\t\t// get the updated personCases\n-\t\tpersonCases = caseService.findBy(new CaseCriteria().person(new PersonReferenceDto(newPerson.getUuid())), true);\n+\t\t\tList<Contact> personContacts = contactService.findBy(new ContactCriteria().setPerson(new PersonReferenceDto(newPerson.getUuid())), null);\n+\t\t\t// Call onContactChanged once for every contact\n+\t\t\t// Attention: this may lead to infinite recursion when not properly implemented\n+\t\t\tfor (Contact personContact : personContacts) {\n+\t\t\t\tcontactFacade.onContactChanged(ContactFacadeEjbLocal.toDto(personContact), syncShares);\n+\t\t\t}\n+\n+\t\t\tList<EventParticipant> personEventParticipants =\n+\t\t\t\teventParticipantService.findBy(new EventParticipantCriteria().withPerson(new PersonReferenceDto(newPerson.getUuid())), null);\n+\t\t\t// Call onEventParticipantChange once for every event participant\n+\t\t\t// Attention: this may lead to infinite recursion when not properly implemented\n+\t\t\tfor (EventParticipant personEventParticipant : personEventParticipants) {\n+\t\t\t\teventParticipantFacade.onEventParticipantChanged(EventFacadeEjbLocal.toDto(personEventParticipant.getEvent()), syncShares);\n+\t\t\t}\n+\n+\t\t\t// get the updated personCases\n+\t\t\tpersonCases = caseService.findBy(new CaseCriteria().person(new PersonReferenceDto(newPerson.getUuid())), true);\n \n-\t\t// Update cases if present condition has changed\n-\t\tif (existingPerson != null) {\n \t\t\t// sort cases based on recency\n \t\t\tCollections.sort(\n \t\t\t\tpersonCases,\n@@ -1195,8 +1199,7 @@ public class PersonFacadeEjb implements PersonFacade {\n \t\t}\n \n \t\t// Update caseAge of all associated cases when approximateAge has changed\n-\t\tif ((existingPerson == null && newPerson.getApproximateAge() != null)\n-\t\t\t|| (existingPerson != null && existingPerson.getApproximateAge() != newPerson.getApproximateAge())) {\n+\t\tif (existingPerson != null && existingPerson.getApproximateAge() != newPerson.getApproximateAge()) {\n \t\t\t// Update case list after previous onCaseChanged\n \t\t\tpersonCases = caseService.findBy(new CaseCriteria().person(new PersonReferenceDto(newPerson.getUuid())), true);\n \t\t\tfor (Case personCase : personCases) {\n", "changed_method_name": "PersonFacadeEjb::onPersonChanged"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/0386b83f646e78828e86cbc066c0835e238f499a", "commit_message": "#9054 added missing null check", "code_diff": "@@ -175,17 +175,20 @@ public abstract class AbstractView extends VerticalLayout implements View {\n \t\t\tnewState = newState.substring(0, paramsIndex);\n \t\t}\n \n-\t\tString urlParams = Arrays.stream(criteriaList)\n-\t\t\t\t.filter(Objects::nonNull)\n-\t\t\t\t.map(BaseCriteria::toUrlParams)\n-\t\t\t\t.filter(params -> !DataHelper.isNullOrEmpty(params))\n-\t\t\t\t.collect(Collectors.joining(\"&\"));\n-\n-\t\tif (urlParams.length() > 0) {\n-\t\t\tif (newState.charAt(newState.length() - 1) != '/') {\n-\t\t\t\tnewState += \"/\";\n+\t\tif (criteriaList != null)\n+\t\t{\n+\t\t\tString urlParams = Arrays.stream(criteriaList)\n+\t\t\t\t\t.filter(Objects::nonNull)\n+\t\t\t\t\t.map(BaseCriteria::toUrlParams)\n+\t\t\t\t\t.filter(params -> !DataHelper.isNullOrEmpty(params))\n+\t\t\t\t\t.collect(Collectors.joining(\"&\"));\n+\n+\t\t\tif (urlParams.length() > 0) {\n+\t\t\t\tif (newState.charAt(newState.length() - 1) != '/') {\n+\t\t\t\t\tnewState += \"/\";\n+\t\t\t\t}\n+\t\t\t\tnewState += \"?\" + urlParams;\n \t\t\t}\n-\t\t\tnewState += \"?\" + urlParams;\n \t\t}\n \n \t\treturn newState;\n", "changed_method_name": "AbstractView::buildNavigationState"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/8fb89b3ee1a45a6ddfef90076505a49b8209a756", "commit_message": "#11465 - Limit case duplicate merging comparison based on creation date and archived status - alignement fix", "code_diff": "@@ -275,6 +275,8 @@ public class MergeCasesFilterComponent extends VerticalLayout {\n \n \t\tthirdRowLayout.addComponent(btnResetFilters);\n \n+\t\tHorizontalLayout relevanceStatusFilterLayout = new HorizontalLayout();\n+\n \t\tlblNumberOfDuplicates = new Label(\"\");\n \t\tlblNumberOfDuplicates.setId(\"numberOfDuplicates\");\n \t\tCssStyles.style(\n@@ -283,9 +285,8 @@ public class MergeCasesFilterComponent extends VerticalLayout {\n \t\t\tCssStyles.LABEL_ROUNDED_CORNERS,\n \t\t\tCssStyles.LABEL_BACKGROUND_FOCUS_LIGHT,\n \t\t\tCssStyles.LABEL_BOLD);\n-\t\tthirdRowLayout.addComponent(lblNumberOfDuplicates);\n-\t\tthirdRowLayout.setComponentAlignment(lblNumberOfDuplicates, Alignment.BOTTOM_RIGHT);\n-\t\tthirdRowLayout.setExpandRatio(lblNumberOfDuplicates, 1);\n+\t\trelevanceStatusFilterLayout.addComponent(lblNumberOfDuplicates);\n+\t\trelevanceStatusFilterLayout.setComponentAlignment(lblNumberOfDuplicates, Alignment.BOTTOM_LEFT);\n \n \t\trelevanceStatusFilter = new ComboBox<>();\n \t\trelevanceStatusFilter.setId(CaseCriteria.ENTITY_RELEVANCE_STATUS);\n@@ -307,8 +308,12 @@ public class MergeCasesFilterComponent extends VerticalLayout {\n \t\t});\n \n \t\tcriteriaBinder.bind(relevanceStatusFilter, CaseCriteria.ENTITY_RELEVANCE_STATUS);\n-\t\tthirdRowLayout.addComponent(relevanceStatusFilter);\n-\t\tthirdRowLayout.setComponentAlignment(relevanceStatusFilter, Alignment.BOTTOM_RIGHT);\n+\t\trelevanceStatusFilterLayout.addComponent(relevanceStatusFilter);\n+\t\trelevanceStatusFilterLayout.setComponentAlignment(relevanceStatusFilter, Alignment.BOTTOM_LEFT);\n+\n+\t\tthirdRowLayout.addComponent(relevanceStatusFilterLayout);\n+\t\tthirdRowLayout.setComponentAlignment(relevanceStatusFilterLayout, Alignment.BOTTOM_RIGHT);\n+\t\tthirdRowLayout.setExpandRatio(relevanceStatusFilterLayout, 1);\n \n \t\taddComponent(thirdRowLayout);\n \t}\n", "changed_method_name": "MergeCasesFilterComponent::addThirdRowLayout"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/31e1c111c766308e74e1f04d3e4e8ead3840bbdc", "commit_message": "#12256 - [Users] Loop of loading when updating the user role of a specific user", "code_diff": "@@ -923,7 +923,7 @@ public class UserFacadeEjb implements UserFacade {\n \t\tSet<User> possibleUsersForAvailableFacilities = new HashSet<>();\n \n \t\tpossibleFacilities.forEach(facility -> {\n-\t\t\tif (!facility.getUuid().equals(FacilityDto.NONE_FACILITY_UUID) && !facility.getUuid().equals(FacilityDto.OTHER_FACILITY_UUID)) {\n+\t\t\tif (!FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid()) && !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n \t\t\t\tpossibleUsersForAvailableFacilities.addAll(userService.getFacilityUsersOfHospital(facility));\n \t\t\t}\n \t\t});\n", "changed_method_name": "UserFacadeEjb::getPossibleUsersBasedOnCasesFacility"}
{"commit_url": "https://github.com/SORMAS-Foundation/SORMAS-Project/commit/d3e0466c9309d56fc39b52d529a3754a5c249dfd", "commit_message": "#12256 - [Users] Loop of loading when updating the user role of a specific user", "code_diff": "@@ -923,7 +923,9 @@ public class UserFacadeEjb implements UserFacade {\n \t\tSet<User> possibleUsersForAvailableFacilities = new HashSet<>();\n \n \t\tpossibleFacilities.forEach(facility -> {\n-\t\t\tif (!FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid()) && !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n+\t\t\tif (facility != null\n+\t\t\t\t&& !FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid())\n+\t\t\t\t&& !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n \t\t\t\tpossibleUsersForAvailableFacilities.addAll(userService.getFacilityUsersOfHospital(facility));\n \t\t\t}\n \t\t});\n", "changed_method_name": "UserFacadeEjb::getPossibleUsersBasedOnCasesFacility"}
{"commit_url": "https://github.com/finos/waltz/commit/4496e3d76fe26bd5b8c98e9bd5707bef35b9b2e4", "commit_message": "DataFlowStatsDao - remove timing wrappers\n\n#609", "code_diff": "@@ -66,26 +66,22 @@ public class DataFlowStatsDao {\n \n         checkNotNull(appIdSelector, \"appIdSelector cannot be null\");\n \n-        Select<Record1<Integer>> inAppCounter = FunctionUtilities.time(\"DFSD.inAppCounter\", ()\n-                -> countDistinctApps(\n+        Select<Record1<Integer>> inAppCounter = countDistinctApps(\n                     appIdSelector,\n                     df.TARGET_ENTITY_ID,\n                     df.SOURCE_ENTITY_ID\n-        ));\n+        );\n \n-        Select<Record1<Integer>> outAppCounter = FunctionUtilities.time(\"DFSD.outAppCounter\", ()\n-                -> countDistinctApps(\n+        Select<Record1<Integer>> outAppCounter = countDistinctApps(\n                     appIdSelector,\n                     df.SOURCE_ENTITY_ID,\n                     df.TARGET_ENTITY_ID\n-        ));\n+        );\n \n-\n-        Select<Record1<Integer>> intraAppCounter = FunctionUtilities.time(\"DFSD.intraAppCounter\", ()\n-                -> dsl\n+        Select<Record1<Integer>> intraAppCounter = dsl\n                     .select(DSL.count())\n                     .from(APPLICATION)\n-                    .where(dsl.renderInlined(APPLICATION.ID.in(appIdSelector))));\n+                    .where(dsl.renderInlined(APPLICATION.ID.in(appIdSelector)));\n \n         Select<Record1<Integer>> query = inAppCounter\n                 .unionAll(outAppCounter)\n", "changed_method_name": "DataFlowStatsDao::countDistinctAppInvolvementByAppIdSelector"}
{"commit_url": "https://github.com/finos/waltz/commit/c04fb6f95aeb5466ff666e0bbf00c0f330a99493", "commit_message": "Parallelize DB summary stat calculations\n\n#607", "code_diff": "@@ -9,6 +9,7 @@ import com.khartec.waltz.model.tally.Tally;\n import com.khartec.waltz.schema.tables.records.DatabaseInformationRecord;\n import org.jooq.*;\n import org.jooq.impl.DSL;\n+import org.jooq.lambda.Unchecked;\n import org.springframework.beans.factory.annotation.Autowired;\n import org.springframework.stereotype.Repository;\n \n@@ -16,10 +17,10 @@ import java.sql.Date;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n+import java.util.concurrent.Future;\n import java.util.stream.Collectors;\n \n-import static com.khartec.waltz.data.JooqUtilities.calculateStringTallies;\n-import static com.khartec.waltz.data.JooqUtilities.mkEndOfLifeStatusDerivedField;\n+import static com.khartec.waltz.data.JooqUtilities.*;\n import static com.khartec.waltz.schema.tables.Application.APPLICATION;\n import static com.khartec.waltz.schema.tables.DatabaseInformation.DATABASE_INFORMATION;\n import static com.khartec.waltz.schema.tables.EntityRelationship.ENTITY_RELATIONSHIP;\n@@ -110,29 +111,31 @@ public class DatabaseInformationDao {\n                 .where(APPLICATION.ID.in(appIdSelector))\n                 .asTable(\"distinct_db_info\");\n \n-        List<Tally<String>> vendorCounts = calculateStringTallies(\n+        Future<List<Tally<String>>> vendorPromise = DB_EXECUTOR_POOL.submit(() -> calculateStringTallies(\n                 dsl,\n                 distinctDbInfo,\n                 dbmsVendorInner,\n-                DSL.trueCondition());\n+                DSL.trueCondition()));\n \n-        List<Tally<String>> environmentCounts = calculateStringTallies(\n+        Future<List<Tally<String>>> environmentPromise = DB_EXECUTOR_POOL.submit(() -> calculateStringTallies(\n                 dsl,\n                 distinctDbInfo,\n                 environmentInner,\n-                DSL.trueCondition());\n+                DSL.trueCondition()));\n \n-        List<Tally<String>> endOfLifeStatusCounts = calculateStringTallies(\n+        Future<List<Tally<String>>> endOfLifeStatusPromise = DB_EXECUTOR_POOL.submit(() -> calculateStringTallies(\n                 dsl,\n                 distinctDbInfo,\n                 mkEndOfLifeStatusDerivedField(eolDateInner),\n-                DSL.trueCondition());\n-\n-        return ImmutableDatabaseSummaryStatistics.builder()\n-                .vendorCounts(vendorCounts)\n-                .environmentCounts(environmentCounts)\n-                .endOfLifeStatusCounts(endOfLifeStatusCounts)\n-                .build();\n+                DSL.trueCondition()));\n+\n+        return Unchecked.supplier(() ->\n+                    ImmutableDatabaseSummaryStatistics.builder()\n+                            .vendorCounts(vendorPromise.get())\n+                            .environmentCounts(environmentPromise.get())\n+                            .endOfLifeStatusCounts(endOfLifeStatusPromise.get())\n+                            .build())\n+                .get();\n     }\n \n }\n", "changed_method_name": "DatabaseInformationDao::findStatsForAppSelector"}
{"commit_url": "https://github.com/finos/waltz/commit/9533421c3825e6d3fbc3717a5d57b92b5af369cf", "commit_message": "CORS: cache OPT requests\n\n#3474", "code_diff": "@@ -203,10 +203,9 @@ public class Main {\n     private void enableCORS() {\n \n         options(\"/*\", (req, res) -> {\n-\n             handleCORSHeader(req, res, \"Access-Control-Request-Headers\", \"Access-Control-Allow-Headers\");\n             handleCORSHeader(req, res, \"Access-Control-Request-Method\", \"Access-Control-Allow-Methods\");\n-\n+            res.header(\"Access-Control-Max-Age\", \"600\");\n             return \"OK\";\n         });\n \n", "changed_method_name": "Main::enableCORS"}
{"commit_url": "https://github.com/eclipse/xtext-core/commit/aff077efc8332aa721a4500e323384890641ff79", "commit_message": "[#1734] improve performance of LiveShadowedAllContainerState.getContainedURIs\n\nSigned-off-by: Christian Dietrich <christian.dietrich@itemis.de>", "code_diff": "@@ -8,7 +8,11 @@\n  *******************************************************************************/\n package org.eclipse.xtext.resource.containers;\n \n+import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n import java.util.List;\n import java.util.Set;\n \n@@ -16,6 +20,7 @@ import org.eclipse.emf.common.util.URI;\n import org.eclipse.xtext.resource.IResourceDescription;\n import org.eclipse.xtext.resource.IResourceDescriptions;\n \n+import com.google.common.collect.Iterators;\n import com.google.common.collect.Sets;\n import com.google.inject.Inject;\n \n@@ -51,14 +56,84 @@ public class LiveShadowedAllContainerState implements IAllContainersState {\n \n \t@Override\n \tpublic Collection<URI> getContainedURIs(String containerHandle) {\n-\t\tSet<URI> result = Sets.newLinkedHashSet();\n+\t\tSet<URI> localContainedURIs = Sets.newLinkedHashSet();\n \t\tfor (IResourceDescription descriptions : localDescriptions.getAllResourceDescriptions()) {\n \t\t\tString computedHandle = getContainerHandle(descriptions.getURI());\n-\t\t\tif (computedHandle != null && computedHandle.equals(containerHandle))\n-\t\t\t\tresult.add(descriptions.getURI());\n+\t\t\tif (computedHandle != null && computedHandle.equals(containerHandle)) {\n+\t\t\t\tlocalContainedURIs.add(descriptions.getURI());\n+\t\t\t}\n \t\t}\n-\t\tresult.addAll(globalState.getContainedURIs(containerHandle));\n-\t\treturn result;\n+\t\tCollection<URI> globalContainedURIs = globalState.getContainedURIs(containerHandle);\n+\t\treturn new Collection<URI>() {\n+\n+\t\t\t@Override\n+\t\t\tpublic int size() {\n+\t\t\t\treturn localContainedURIs.size() + globalContainedURIs.size();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean isEmpty() {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean contains(Object o) {\n+\t\t\t\treturn localContainedURIs.contains(o) || globalContainedURIs.contains(o);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Iterator<URI> iterator() {\n+\t\t\t\treturn Iterators.concat(localContainedURIs.iterator(), globalContainedURIs.iterator());\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object[] toArray() {\n+\t\t\t\tSet<URI> result = new LinkedHashSet<>();\n+\t\t\t\tresult.addAll(localContainedURIs);\n+\t\t\t\tresult.addAll(globalContainedURIs);\n+\t\t\t\treturn result.toArray();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic <T> T[] toArray(T[] a) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean add(URI e) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean remove(Object o) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean containsAll(Collection<?> c) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean addAll(Collection<? extends URI> c) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean removeAll(Collection<?> c) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic boolean retainAll(Collection<?> c) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void clear() {\n+\t\t\t\tthrow new UnsupportedOperationException(\"not expected\");\n+\t\t\t}\n+\t\t};\n \t}\n \t\n \t@Override\n", "changed_method_name": "LiveShadowedAllContainerState::getContainedURIs"}
{"commit_url": "https://github.com/eclipse/xtext-eclipse/commit/b3d1739e61f06452899c0e8655fa7a2cf1d4018c", "commit_message": "[eclipse/xtext#1129] Moved recovery build trigger to workspace job to\navoid Eclipse start-up hang\n\nIn short, org.eclipse.xtext.ui.shared.internal.Activator.start\nlocks OSGI activation for XText, but waits on the workspace lock\nthat is held by the \"Worker-47: Build\" thread. There,\norg.eclipse.xtext.ui.ecore.Activator.start tries to activate xtext.ui.shared.\n\nthe org.eclipse.xtext.ui.shared Activator waits on the workspace lock due to\nListenerRegistrar.initialize() calling AbstractBuilderState.load().\nThis code attemps to lock the workspace.\n\nThis change moves the call to AbstractBuilderState.load()\n(during ListenerRegistrar.initialize()) to a workspace job,\navoiding the start-up hang.\n\nSigned-off-by: Simeon Andreev <simeon.danailov.andreev@gmail.com>", "code_diff": "@@ -10,6 +10,11 @@ package org.eclipse.xtext.ui.shared.internal;\n import java.util.Arrays;\n \n import org.eclipse.core.resources.IWorkspace;\n+import org.eclipse.core.resources.WorkspaceJob;\n+import org.eclipse.core.runtime.CoreException;\n+import org.eclipse.core.runtime.IProgressMonitor;\n+import org.eclipse.core.runtime.IStatus;\n+import org.eclipse.core.runtime.Status;\n import org.eclipse.xtext.builder.builderState.IBuilderState;\n import org.eclipse.xtext.builder.impl.BuildScheduler;\n import org.eclipse.xtext.builder.impl.IBuildFlag;\n@@ -41,9 +46,25 @@ public class ListenerRegistrar implements IEagerContribution {\n \t\t// after the fact, the project-open event is already done. Thus no build is triggered for the newly\n \t\t// opened project and the index state is corrupt.\n \t\t// thus we trigger a recovery build here\n-\t\tif (builderState.isEmpty()) {\n-\t\t\tbuildManager.scheduleBuildIfNecessary(Arrays.asList(workspace.getRoot().getProjects()), IBuildFlag.RECOVERY_BUILD);\n+\t\tclass RecoveryBuildTrigger extends WorkspaceJob {\n+\n+\t\t\tpublic RecoveryBuildTrigger() {\n+\t\t\t\tsuper(\"Schedule Xtext recovery build on start-up\");\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic IStatus runInWorkspace(IProgressMonitor monitor) throws CoreException {\n+\t\t\t\tif (monitor.isCanceled()) {\n+\t\t\t\t\treturn Status.CANCEL_STATUS;\n+\t\t\t\t}\n+\t\t\t\tif (builderState.isEmpty()) {\n+\t\t\t\t\tbuildManager.scheduleBuildIfNecessary(Arrays.asList(workspace.getRoot().getProjects()), IBuildFlag.RECOVERY_BUILD);\n+\t\t\t\t}\n+\t\t\t\treturn Status.OK_STATUS;\n+\t\t\t}\n \t\t}\n+\t\tRecoveryBuildTrigger recoveryBuildTrigger = new RecoveryBuildTrigger();\n+\t\trecoveryBuildTrigger.schedule();\n \t}\n \n \t@Override\n", "changed_method_name": "ListenerRegistrar::initialize"}
{"commit_url": "https://github.com/microsoft/kafka-connect-cosmosdb/commit/768cbc5d9363a09ce95995607d504d69a968d1ac", "commit_message": "MINOR: minor performance and readability improvement for logging", "code_diff": "@@ -69,7 +69,7 @@ public class CosmosDBSinkTask extends SinkTask {\n                 logger.debug(\"Writing record, value type: {}\", record.value().getClass().getName());\r\n                 logger.debug(\"Key Schema: {}\", record.keySchema());\r\n                 logger.debug(\"Value schema: {}\", record.valueSchema());\r\n-                logger.trace(\"Value.toString(): {}\", record.value() != null ? record.value().toString() : \"<null>\");\r\n+                logger.trace(\"Value.toString(): {}\", record.value());\r\n \r\n                 Object recordValue;\r\n                 if (record.value() instanceof Struct) {\r\n", "changed_method_name": "CosmosDBSinkTask::put"}
{"commit_url": "https://github.com/IEEE-VIT/weatherbes-android/commit/69acd47234aaa1342522db5c78cab4cab6d0b2f3", "commit_message": "fix: multiple toasts and back issue", "code_diff": "@@ -25,25 +25,27 @@ public class StartActivity extends AppCompatActivity {\n         go.setOnClickListener(new View.OnClickListener() {\n             @Override\n             public void onClick(View v) {\n-                for(int i=0; i<continents.length;i++) {\n+                boolean isContinent = false;\n+                for (int i = 0; i < continents.length; i++) {\n                     if (city.getText().toString().toUpperCase().equals(continents[i])) {\n                         Toast toast = Toast.makeText(getApplicationContext(), \"Weather not found!\", Toast.LENGTH_LONG);\n                         toast.show();\n+                        isContinent = true;\n                         break;\n                     }\n-                    else {\n-                        String inputCity = city.getText().toString().trim();\n-                        if (!inputCity.equals(\"\")) {\n-                            Intent intent = new Intent(StartActivity.this, MainActivity.class);\n-                            intent.putExtra(\"City\", inputCity);\n-                            startActivity(intent);\n-                            finish();\n-                        } else {\n-                            Toast.makeText(StartActivity.this, \"Please enter a city's name!\", Toast.LENGTH_SHORT).show();\n-                        }\n-                    }\n                 }\n \n+                if(!isContinent) {\n+                    String inputCity = city.getText().toString().trim();\n+                    if (!inputCity.equals(\"\")) {\n+                        Intent intent = new Intent(StartActivity.this, MainActivity.class);\n+                        intent.putExtra(\"City\", inputCity);\n+                        startActivity(intent);\n+                        finish();\n+                    } else {\n+                        Toast.makeText(StartActivity.this, \"Please enter a city's name!\", Toast.LENGTH_SHORT).show();\n+                    }\n+                }\n             }\n         });\n     }\n", "changed_method_name": "StartActivity::onCreate"}
