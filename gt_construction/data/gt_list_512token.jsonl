{"commit_url":"https://github.com/PrincetonUniversity/athena/commit/36a24ba98c79313fdc8eb99a2b6328e13432b187","commit_message":"fix a bug in disk.cpp related to #430","code_diff":"@@ -175,7 +175,7 @@ void GetCylCoord(Coordinates *pco,Real &rad,Real &phi,Real &z,int i,int j,int k)\n     z=pco->x3v(k);\n   } else if (std::strcmp(COORDINATE_SYSTEM, \"spherical_polar\") == 0) {\n     rad=std::abs(pco->x1v(i)*std::sin(pco->x2v(j)));\n-    phi=pco->x3v(i);\n+    phi=pco->x3v(k);\n     z=pco->x1v(i)*std::cos(pco->x2v(j));\n   }\n   return;\n","changed_method_name":"GetCylCoord","commit_message_token_length":11,"code_token_length":199,"combined_token_length":210}
{"commit_url":"https://github.com/mapbox/mapbox-gl-native/commit/bea457697a550392971c143071296e0d26a09fb4","commit_message":"Ignore tile URL sharding\n\nThe source returns sharded tile URLs, but sharding doesnâ€™t buy us anything on native platforms, so always grab the first.\n\nref mapbox/mapbox-gl-native#2007","code_diff":"@@ -98,7 +98,7 @@ void SourceInfo::parseTileJSONProperties(const rapidjson::Value& value) {\n }\n \n std::string SourceInfo::tileURL(const TileID& id, float pixelRatio) const {\n-    std::string result = tiles.at((id.x + id.y) % tiles.size());\n+    std::string result = tiles.at(0);\n     result = util::mapbox::normalizeTileURL(result, url, type);\n     result = util::replaceTokens(result, [&](const std::string &token) -> std::string {\n         if (token == \"z\") return util::toString(std::min(id.z, static_cast<int8_t>(max_zoom)));\n","changed_method_name":"mbgl::SourceInfo::tileURL","commit_message_token_length":50,"code_token_length":196,"combined_token_length":246}
{"commit_url":"https://github.com/mapbox/mapbox-gl-native/commit/2f30dfe97d851f2a2bb4141dc234013b01f8afd3","commit_message":"Disable heading updates for .FollowWithCourse\n\nFixes #2180.","code_diff":"@@ -2310,6 +2310,7 @@ CLLocationCoordinate2D MGLLocationCoordinate2DFromLatLng(mbgl::LatLng latLng)\n             break;\n         }\n         case MGLUserTrackingModeFollow:\n+        case MGLUserTrackingModeFollowWithCourse:\n         {\n             self.showsUserLocation = YES;\n \n@@ -2323,7 +2324,6 @@ CLLocationCoordinate2D MGLLocationCoordinate2DFromLatLng(mbgl::LatLng latLng)\n             break;\n         }\n         case MGLUserTrackingModeFollowWithHeading:\n-        case MGLUserTrackingModeFollowWithCourse:\n         {\n             self.showsUserLocation = YES;\n \n","changed_method_name":"switch","commit_message_token_length":16,"code_token_length":279,"combined_token_length":295}
{"commit_url":"https://github.com/mapbox/mapbox-gl-native/commit/253a007d99c2079b95d5c6d11715e16815067e16","commit_message":"[ios][bench] Add total and avg FPS to final summary","code_diff":"@@ -81,13 +81,17 @@ static const int benchmarkDuration = 200; // frames\n         // Do nothing. The benchmark is completed.\n         NSLog(@\"Benchmark completed.\");\n         NSLog(@\"Result:\");\n+        double totalFPS = 0;\n         size_t colWidth = 0;\n         for (const auto& row : result) {\n             colWidth = std::max(row.first.size(), colWidth);\n         }\n         for (const auto& row : result) {\n             NSLog(@\"| %-*s | %4.1f fps |\", int(colWidth), row.first.c_str(), row.second);\n+            totalFPS += row.second;\n         }\n+        NSLog(@\"Total FPS: %4.1f\", totalFPS);\n+        NSLog(@\"Average FPS: %4.1f\", totalFPS / result.size());\n         exit(0);\n     }\n }\n","changed_method_name":"for","commit_message_token_length":13,"code_token_length":349,"combined_token_length":362}
{"commit_url":"https://github.com/rwengine/openrw/commit/f7bd8701db2a98678e206ae78515b0a0e16b092b","commit_message":"setForcedUpdateAllAabbs to false; Reduces stepSimulation time by 35%.\n\nThis causes a drop from 6.5ms to 4.2ms on my machine.","code_diff":"@@ -92,6 +92,7 @@ GameWorld::GameWorld(Logger* log, GameData* dat)\n         _overlappingPairCallback.get());\n     gContactProcessedCallback = ContactProcessedCallback;\n     dynamicsWorld->setInternalTickCallback(PhysicsTickCallback, this);\n+    dynamicsWorld->setForceUpdateAllAabbs(false);\n }\n \n GameWorld::~GameWorld() {\n","changed_method_name":"GameWorld::GameWorld","commit_message_token_length":40,"code_token_length":115,"combined_token_length":155}
{"commit_url":"https://github.com/kokkos/kokkos/commit/910d43e45b04fb0e7d155233f7069cab0aeefc79","commit_message":"OpenMP: Adding an ifdef around chunksize for static schedule for GCC compiler.","code_diff":"@@ -101,8 +101,15 @@ class ParallelFor<FunctorType, Kokkos::RangePolicy<Traits...>, Kokkos::OpenMP> {\n   std::enable_if_t<!std::is_same<typename Policy::schedule_type::type,\n                                  Kokkos::Dynamic>::value>\n   execute_parallel() const {\n+// Specifying an chunksize with GCC compiler leads to performance regression\n+// with static schedule.\n+#ifdef KOKKOS_COMPILER_GNU\n+#pragma omp parallel for schedule(static) \\\n+    num_threads(m_instance->thread_pool_size())\n+#else\n #pragma omp parallel for schedule(static KOKKOS_OPENMP_OPTIONAL_CHUNK_SIZE) \\\n     num_threads(m_instance->thread_pool_size())\n+#endif\n     KOKKOS_PRAGMA_IVDEP_IF_ENABLED\n     for (auto iwork = m_policy.begin(); iwork < m_policy.end(); ++iwork) {\n       exec_work(m_functor, iwork);\n","changed_method_name":"Kokkos::Impl::ParallelFor::execute_parallel","commit_message_token_length":17,"code_token_length":324,"combined_token_length":341}
{"commit_url":"https://github.com/EasyRPG/Player/commit/a086820de8eadbd830f3aa5ebf822d548a465879","commit_message":"When the cache is exhausted do not delete unreferenced assets that were used during the last 50ms (approx. 3 frames).\n\nThe current behaviour (flush everything unreferenced when cache is full) was added to workaround out-of-memory issues on systems with limited RAM but this direct unload makes some use cases like face rendering in the Battle scene with Gauge style really slow (when the cache is full the face is unloaded and reloaded from disk once every frame).\n\nI hope that 50ms is a good compromise here.\n\nFix #2509","code_diff":"@@ -101,8 +101,15 @@ namespace {\n \t\t\t\tcontinue;\n \t\t\t}\n \n-\t\t\tif (cache_size <= cache_limit && cur_ticks - it->second.last_access < 3s) {\n-\t\t\t\t// Below memory limit and last access < 3s\n+\t\t\tauto last_access = cur_ticks - it->second.last_access;\n+\t\t\tbool cache_exhausted = cache_size > cache_limit;\n+\t\t\tif (cache_exhausted) {\n+\t\t\t\tif (last_access <= 50ms) {\n+\t\t\t\t\t// Used during the last 3 frames, must be important, keep it.\n+\t\t\t\t\t++it;\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t} else if (last_access <= 3s) {\n \t\t\t\t++it;\n \t\t\t\tcontinue;\n \t\t\t}\n","changed_method_name":"FreeBitmapMemory","commit_message_token_length":120,"code_token_length":238,"combined_token_length":358}
{"commit_url":"https://github.com/EasyRPG/Player/commit/72749b7a94ea09ec59870e099aac9bbf5de89a45","commit_message":"Effects cache: Fix memory leak\n\nThe BitmapRef reference counter never reached 0 because it was part of the key, preventing deletion.\nChanged it to a pointer. The value is never read so doesn't matter if it becomes stale.\n\nFix #3163","code_diff":"@@ -81,7 +81,7 @@ namespace {\n \tstd::unordered_map<tile_key_type, std::weak_ptr<Bitmap>> cache_tiles;\n \n \t// rect, flip_x, flip_y, tone, blend\n-\tusing effect_key_type = std::tuple<BitmapRef, Rect, bool, bool, Tone, Color>;\n+\tusing effect_key_type = std::tuple<Bitmap*, Rect, bool, bool, Tone, Color>;\n \tstd::map<effect_key_type, std::weak_ptr<Bitmap>> cache_effects;\n \n \tstd::string system_name;\n@@ -444,7 +444,7 @@ BitmapRef Cache::Tile(StringView filename, int tile_id) {\n \n BitmapRef Cache::SpriteEffect(const BitmapRef& src_bitmap, const Rect& rect, bool flip_x, bool flip_y, const Tone& tone, const Color& blend) {\n \tconst effect_key_type key {\n-\t\tsrc_bitmap,\n+\t\tsrc_bitmap.get(),\n \t\trect,\n \t\tflip_x,\n \t\tflip_y,\n","changed_method_name":"Cache::SpriteEffect","commit_message_token_length":55,"code_token_length":290,"combined_token_length":345}
{"commit_url":"https://github.com/mockingbirdnest/Principia/commit/2c7f44c717fc333bd98e8cd2d0748aaba4db7448","commit_message":"A more useful trace.","code_diff":"@@ -20,6 +20,8 @@ using base::HexadecimalEncoder;\n using base::UniqueArray;\n using interface::principia__ActivatePlayer;\n \n+using namespace std::chrono_literals;\n+\n namespace journal {\n \n Player::Player(std::filesystem::path const& path)\n@@ -67,8 +69,9 @@ bool Player::Play(int const index) {\n #include \"journal/player.generated.cc\"\n \n   auto const after = std::chrono::system_clock::now();\n-  if (after - before > std::chrono::milliseconds(100)) {\n-    LOG(ERROR) << \"Long method:\\n\" << method_in->DebugString();\n+  if (after - before > 100ms) {\n+    LOG(ERROR) << \"Long method (\" << (after - before) / 1ms << \" ms):\\n\"\n+               << method_in->DebugString();\n   }\n \n   last_method_in_.swap(method_in);\n","changed_method_name":"principia::journal::Player::Play","commit_message_token_length":5,"code_token_length":266,"combined_token_length":271}
{"commit_url":"https://github.com/stan-dev/math/commit/75bcadb96cc061cf3dea20c68c5847bbdc09a3ce","commit_message":"avoid putting un-needed vars on chain stack and place them on nochain stack to reduce chain calls","code_diff":"@@ -43,7 +43,9 @@ void gradient(const F& f, const Eigen::Matrix<double, Eigen::Dynamic, 1>& x,\n               double& fx, Eigen::Matrix<double, Eigen::Dynamic, 1>& grad_fx) {\n   start_nested();\n   try {\n-    Eigen::Matrix<var, Eigen::Dynamic, 1> x_var(x);\n+    Eigen::Matrix<var, Eigen::Dynamic, 1> x_var(x.size());\n+    for (int i = 0; i < x.size(); ++i)\n+      x_var(i) = var(new vari(x(i), false));\n     var fx_var = f(x_var);\n     fx = fx_var.val();\n     grad_fx.resize(x.size());\n","changed_method_name":"stan::math::gradient","commit_message_token_length":21,"code_token_length":236,"combined_token_length":257}
{"commit_url":"https://github.com/godotengine/godot/commit/27a6ab457b15e448147ab97ab6b23e835ac9e741","commit_message":"Turn off physics and 3d navigation servers in the Project Manager.\n\nCopying the editor behaviour of deactivating these servers we're not\nusing, to reduce CPU load.","code_diff":"@@ -52,6 +52,7 @@\n #include \"scene/gui/texture_rect.h\"\n #include \"scene/main/window.h\"\n #include \"servers/display_server.h\"\n+#include \"servers/navigation_server_3d.h\"\n \n static inline String get_project_key_from_path(const String &dir) {\n \treturn dir.replace(\"/\", \"::\");\n@@ -2387,6 +2388,11 @@ ProjectManager::ProjectManager() {\n \t\tEditorSettings::create();\n \t}\n \n+\t// Turn off some servers we aren't going to be using in the Project Manager.\n+\tNavigationServer3D::get_singleton()->set_active(false);\n+\tPhysicsServer3D::get_singleton()->set_active(false);\n+\tPhysicsServer2D::get_singleton()->set_active(false);\n+\n \tEditorSettings::get_singleton()->set_optimize_save(false); //just write settings as they came\n \n \t{\n","changed_method_name":"ProjectManager::ProjectManager","commit_message_token_length":37,"code_token_length":257,"combined_token_length":294}
{"commit_url":"https://github.com/godotengine/godot/commit/ba431a9306f6f018488b6d19854ea40fa528f205","commit_message":"Fix volumetric fog memory leak on resize","code_diff":"@@ -4011,6 +4011,9 @@ void RendererSceneRenderRD::_volumetric_fog_erase(RenderBuffers *rb) {\n \tRD::get_singleton()->free(rb->volumetric_fog->prev_light_density_map);\n \tRD::get_singleton()->free(rb->volumetric_fog->light_density_map);\n \tRD::get_singleton()->free(rb->volumetric_fog->fog_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->density_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->light_map);\n+\tRD::get_singleton()->free(rb->volumetric_fog->emissive_map);\n \n \tif (rb->volumetric_fog->fog_uniform_set.is_valid() && RD::get_singleton()->uniform_set_is_valid(rb->volumetric_fog->fog_uniform_set)) {\n \t\tRD::get_singleton()->free(rb->volumetric_fog->fog_uniform_set);\n","changed_method_name":"RendererSceneRenderRD::_volumetric_fog_erase","commit_message_token_length":9,"code_token_length":298,"combined_token_length":307}
{"commit_url":"https://github.com/godotengine/godot/commit/c37bd41c794819e0b6bc3ad4b162548057098e1c","commit_message":"Increase RemoteDebuggerPeerTCP poll to 6.9ms\n\nFix high CPU usage on MacOS by reverting the polling for Network\ndebugging to match 144hz refresh rate.","code_diff":"@@ -190,7 +190,8 @@ Error RemoteDebuggerPeerTCP::connect_to_host(const String &p_host, uint16_t p_po\n }\n \n void RemoteDebuggerPeerTCP::_thread_func(void *p_ud) {\n-\tconst uint64_t min_tick = 100;\n+\t// Update in time for 144hz monitors\n+\tconst uint64_t min_tick = 6900;\n \tRemoteDebuggerPeerTCP *peer = (RemoteDebuggerPeerTCP *)p_ud;\n \twhile (peer->running && peer->is_peer_connected()) {\n \t\tuint64_t ticks_usec = OS::get_singleton()->get_ticks_usec();\n","changed_method_name":"RemoteDebuggerPeerTCP::_thread_func","commit_message_token_length":40,"code_token_length":179,"combined_token_length":219}
{"commit_url":"https://github.com/godotengine/godot/commit/66d27df12f5b710ac5f569144944c1db0c86c96b","commit_message":"Fix 3D sky update performance regression","code_diff":"@@ -1172,7 +1172,7 @@ void RendererSceneSkyRD::setup(RendererSceneEnvironmentRD *p_env, RID p_render_b\n \t\t\t\t}\n \t\t\t}\n \t\t\t// Check whether the directional_light_buffer changes\n-\t\t\tbool light_data_dirty = true;\n+\t\t\tbool light_data_dirty = false;\n \n \t\t\t// Light buffer is dirty if we have fewer or more lights\n \t\t\t// If we have fewer lights, make sure that old lights are disabled\n","changed_method_name":"RendererSceneSkyRD::setup","commit_message_token_length":7,"code_token_length":134,"combined_token_length":141}
{"commit_url":"https://github.com/godotengine/godot/commit/ab5eaf0ad97cfac2d3bc924ae99c5fc83cd3e838","commit_message":"Fix new performance regressions (short delay_usec)\n\nMy Mac was using 20% cpu again, which was related to the Javascript\nExport plugin.\n\nI had however no export templates setup in the project so this is more\nof a stopgap fix.","code_diff":"@@ -645,7 +645,7 @@ Ref<Texture2D> EditorExportPlatformJavaScript::get_run_icon() const {\n void EditorExportPlatformJavaScript::_server_thread_poll(void *data) {\n \tEditorExportPlatformJavaScript *ej = (EditorExportPlatformJavaScript *)data;\n \twhile (!ej->server_quit) {\n-\t\tOS::get_singleton()->delay_usec(1000);\n+\t\tOS::get_singleton()->delay_usec(6900);\n \t\t{\n \t\t\tMutexLock lock(ej->server_lock);\n \t\t\tej->server->poll();\n","changed_method_name":"EditorExportPlatformJavaScript::_server_thread_poll","commit_message_token_length":56,"code_token_length":158,"combined_token_length":214}
{"commit_url":"https://github.com/godotengine/godot/commit/32b16c876b92c3dae35046d37740fc6e5cc65b24","commit_message":"[Net] Fix SceneReplicationConfig setter.\n\nUsed by resource loader, it would always add properties as both sync and\nspawn, disregarding the actual option value.","code_diff":"@@ -52,11 +52,19 @@ bool SceneReplicationConfig::_set(const StringName &p_name, const Variant &p_val\n \t\tReplicationProperty &prop = properties[idx];\n \t\tif (what == \"sync\") {\n \t\t\tprop.sync = p_value;\n-\t\t\tsync_props.push_back(prop.name);\n+\t\t\tif (prop.sync) {\n+\t\t\t\tsync_props.push_back(prop.name);\n+\t\t\t} else {\n+\t\t\t\tsync_props.erase(prop.name);\n+\t\t\t}\n \t\t\treturn true;\n \t\t} else if (what == \"spawn\") {\n \t\t\tprop.spawn = p_value;\n-\t\t\tspawn_props.push_back(prop.name);\n+\t\t\tif (prop.spawn) {\n+\t\t\t\tspawn_props.push_back(prop.name);\n+\t\t\t} else {\n+\t\t\t\tspawn_props.erase(prop.name);\n+\t\t\t}\n \t\t\treturn true;\n \t\t}\n \t}\n","changed_method_name":"SceneReplicationConfig::_set","commit_message_token_length":37,"code_token_length":288,"combined_token_length":325}
{"commit_url":"https://github.com/godotengine/godot/commit/0cbd1c85a96384ef93978661e408ce7c1b721847","commit_message":"Fix burning CPU with udev disabled on Flatpak\n\nFixes #67355.","code_diff":"@@ -218,8 +218,8 @@ void JoypadLinux::monitor_joypads() {\n \t\t\t}\n \t\t}\n \t\tclosedir(input_directory);\n+\t\tusleep(1000000); // 1s\n \t}\n-\tusleep(1000000); // 1s\n }\n \n void JoypadLinux::close_joypads() {\n","changed_method_name":"JoypadLinux::monitor_joypads","commit_message_token_length":18,"code_token_length":91,"combined_token_length":109}
{"commit_url":"https://github.com/godotengine/godot/commit/51777a2914dc46f2831752b74db6276369df0905","commit_message":"Fix constant editor redraw after shortcut\n\nIntroduced by me by mistake on #71328. Fixes #71652.","code_diff":"@@ -385,6 +385,7 @@ void BaseButton::shortcut_input(const Ref<InputEvent> &p_event) {\n \t\tif (shortcut_feedback) {\n \t\t\tif (shortcut_feedback_timer == nullptr) {\n \t\t\t\tshortcut_feedback_timer = memnew(Timer);\n+\t\t\t\tshortcut_feedback_timer->set_one_shot(true);\n \t\t\t\tadd_child(shortcut_feedback_timer);\n \t\t\t\tshortcut_feedback_timer->set_wait_time(GLOBAL_GET(\"gui/timers/button_shortcut_feedback_highlight_time\"));\n \t\t\t\tshortcut_feedback_timer->connect(\"timeout\", callable_mp(this, &BaseButton::_shortcut_feedback_timeout));\n","changed_method_name":"BaseButton::shortcut_input","commit_message_token_length":25,"code_token_length":206,"combined_token_length":231}
{"commit_url":"https://github.com/godotengine/godot/commit/340c3b84fd9f06f4d4a794c0a0b8e95165aeee9f","commit_message":"Set Default compression to VRAM uncompressed for LightmapGI\n\nThis increases the speed to be near instant and removes the perceived lightmap bake speed regression\n\nWe need to investigate the speed and quality issues with BPTC and re-enable compression when we can","code_diff":"@@ -163,7 +163,8 @@ Array LightmapGIData::_get_light_textures_data() const {\n \t\tconfig->set_value(\"remap\", \"importer\", \"2d_array_texture\");\n \t\tconfig->set_value(\"remap\", \"type\", \"CompressedTexture2DArray\");\n \t\tif (!config->has_section_key(\"params\", \"compress/mode\")) {\n-\t\t\tconfig->set_value(\"params\", \"compress/mode\", 2); //user may want another compression, so leave it be\n+\t\t\t// User may want another compression, so leave it be, but default to VRAM uncompressed.\n+\t\t\tconfig->set_value(\"params\", \"compress/mode\", 3);\n \t\t}\n \t\tconfig->set_value(\"params\", \"compress/channel_pack\", 1);\n \t\tconfig->set_value(\"params\", \"mipmaps/generate\", false);\n","changed_method_name":"LightmapGIData::_get_light_textures_data","commit_message_token_length":53,"code_token_length":229,"combined_token_length":282}
{"commit_url":"https://github.com/godotengine/godot/commit/f84c6df8d1aec35fe53521f241b26fc5312d26e3","commit_message":"Use DXT1 when compressing PNGs with RGB format\n\nThis results in much smaller file sizes with the same quality","code_diff":"@@ -66,7 +66,7 @@ EtcpakType _determine_dxt_type(Image::UsedChannels p_channels) {\n \t\tcase Image::USED_CHANNELS_RG:\n \t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5_RA_AS_RG;\n \t\tcase Image::USED_CHANNELS_RGB:\n-\t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5;\n+\t\t\treturn EtcpakType::ETCPAK_TYPE_DXT1;\n \t\tcase Image::USED_CHANNELS_RGBA:\n \t\t\treturn EtcpakType::ETCPAK_TYPE_DXT5;\n \t\tdefault:\n","changed_method_name":"_determine_dxt_type","commit_message_token_length":25,"code_token_length":185,"combined_token_length":210}
{"commit_url":"https://github.com/godotengine/godot/commit/e5bebbc9ffda8b4fa5a786842ea24f0a3ea0763a","commit_message":"Fix unnecessary break when calculating the height of visible lines\n\nThis break causes the minsize to be smaller than expected, and then\nthe size keeps increasing by one line to cover all visible lines.\nThis can cause performance issues when there are many visible lines.","code_diff":"@@ -290,9 +290,6 @@ void Label::_update_visible() {\n \tint last_line = MIN(lines_rid.size(), lines_visible + lines_skipped);\n \tfor (int64_t i = lines_skipped; i < last_line; i++) {\n \t\tminsize.height += TS->shaped_text_get_size(lines_rid[i]).y + line_spacing;\n-\t\tif (minsize.height > (get_size().height - style->get_minimum_size().height + line_spacing)) {\n-\t\t\tbreak;\n-\t\t}\n \t}\n }\n \n","changed_method_name":"Label::_update_visible","commit_message_token_length":53,"code_token_length":156,"combined_token_length":209}
{"commit_url":"https://github.com/godotengine/godot/commit/8cdab04d7fd57aaabd790349cd8a4e9ec21a7edd","commit_message":"Fix that the focus-out notification got sent deferred\n\nCurrently the window receives a focus-out notification, directly after\nit popup, because currently the signal is sent deferred.\nThe original intention was that the previously focused window must\nreceive a focus-out notification.\nThis change makes the notification more precise by only sending the\nfocus-out to the previously focused window.","code_diff":"@@ -1624,7 +1624,15 @@ void Window::popup(const Rect2i &p_screen_rect) {\n \t\t// Send a focus-out notification when opening a Window Manager Popup.\n \t\tSceneTree *scene_tree = get_tree();\n \t\tif (scene_tree) {\n-\t\t\tscene_tree->notify_group_flags(SceneTree::GROUP_CALL_DEFERRED, \"_viewports\", NOTIFICATION_WM_WINDOW_FOCUS_OUT);\n+\t\t\tList<Node *> list;\n+\t\t\tscene_tree->get_nodes_in_group(\"_viewports\", &list);\n+\t\t\tfor (Node *n : list) {\n+\t\t\t\tWindow *w = Object::cast_to<Window>(n);\n+\t\t\t\tif (w && !w->get_embedder() && w->has_focus()) {\n+\t\t\t\t\tw->_event_callback(DisplayServer::WINDOW_EVENT_FOCUS_OUT);\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \t}\n \n","changed_method_name":"Window::popup","commit_message_token_length":78,"code_token_length":282,"combined_token_length":360}
{"commit_url":"https://github.com/godotengine/godot/commit/bc1aef88eef7bb09d1c537bf150414c50abfa374","commit_message":"SCons: Disable misbehaving MSVC incremental linking\n\nFixes #77968.","code_diff":"@@ -355,6 +355,9 @@ def configure_msvc(env, vcvars_msvc_config):\n         else:\n             env.AppendUnique(CCFLAGS=[\"/MD\"])\n \n+    # MSVC incremental linking is broken and _increases_ link time (GH-77968).\n+    env.Append(LINKFLAGS=[\"/INCREMENTAL:NO\"])\n+\n     if env[\"arch\"] == \"x86_32\":\n         env[\"x86_libtheora_opt_vc\"] = True\n \n","changed_method_name":"configure_msvc","commit_message_token_length":19,"code_token_length":164,"combined_token_length":183}
{"commit_url":"https://github.com/godotengine/godot/commit/085255bd0ed2c36b217769c6f75c8ea856725b1a","commit_message":"Improved X11 screen_get_refresh_rate performance","code_diff":"@@ -1571,7 +1571,7 @@ float DisplayServerX11::screen_get_refresh_rate(int p_screen) const {\n \n \t//Use xrandr to get screen refresh rate.\n \tif (xrandr_ext_ok) {\n-\t\tXRRScreenResources *screen_info = XRRGetScreenResources(x11_display, windows[MAIN_WINDOW_ID].x11_window);\n+\t\tXRRScreenResources *screen_info = XRRGetScreenResourcesCurrent(x11_display, windows[MAIN_WINDOW_ID].x11_window);\n \t\tif (screen_info) {\n \t\t\tRRMode current_mode = 0;\n \t\t\txrr_monitor_info *monitors = nullptr;\n","changed_method_name":"DisplayServerX11::screen_get_refresh_rate","commit_message_token_length":12,"code_token_length":186,"combined_token_length":198}
{"commit_url":"https://github.com/godotengine/godot/commit/3a67eb26754d779b308d478e92b5d12252c70a5a","commit_message":"Disable a prohibitively slow code branch when reparenting nodes","code_diff":"@@ -1762,6 +1762,8 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t} break;\n \n+// FIXME: This approach causes a significant performance regression, see GH-84910.\n+#if 0\n \t\tcase Variant::OBJECT: {\n \t\t\tResource *resource = Object::cast_to<Resource>(r_variant);\n \t\t\tif (!resource) {\n@@ -1792,6 +1794,7 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t\tbreak;\n \t\t};\n+#endif\n \n \t\tdefault: {\n \t\t}\n","changed_method_name":"SceneTreeDock::_check_node_path_recursive","commit_message_token_length":13,"code_token_length":204,"combined_token_length":217}
{"commit_url":"https://github.com/godotengine/godot/commit/5dd11e8eee739f2ac3bbecd93cd4862c151aebc8","commit_message":"Limit window size updates on title change.","code_diff":"@@ -284,7 +284,13 @@ void Window::set_title(const String &p_title) {\n \t\tembedder->_sub_window_update(this);\n \t} else if (window_id != DisplayServer::INVALID_WINDOW_ID) {\n \t\tDisplayServer::get_singleton()->window_set_title(tr_title, window_id);\n-\t\t_update_window_size();\n+\t\tif (keep_title_visible) {\n+\t\t\tSize2i title_size = DisplayServer::get_singleton()->window_get_title_size(tr_title, window_id);\n+\t\t\tSize2i size_limit = get_clamped_minimum_size();\n+\t\t\tif (title_size.x > size_limit.x || title_size.y > size_limit.y) {\n+\t\t\t\t_update_window_size();\n+\t\t\t}\n+\t\t}\n \t}\n }\n \n","changed_method_name":"Window::set_title","commit_message_token_length":8,"code_token_length":239,"combined_token_length":247}
{"commit_url":"https://github.com/godotengine/godot/commit/829349d2ca6e49eaf0703154467ee8020484b387","commit_message":"Do not reload resources and send notification if locale is not changed.","code_diff":"@@ -518,8 +518,12 @@ String TranslationServer::get_country_name(const String &p_country) const {\n }\n \n void TranslationServer::set_locale(const String &p_locale) {\n-\tlocale = standardize_locale(p_locale);\n+\tString new_locale = standardize_locale(p_locale);\n+\tif (locale == new_locale) {\n+\t\treturn;\n+\t}\n \n+\tlocale = new_locale;\n \tResourceLoader::reload_translation_remaps();\n \n \tif (OS::get_singleton()->get_main_loop()) {\n","changed_method_name":"TranslationServer::set_locale","commit_message_token_length":13,"code_token_length":163,"combined_token_length":176}
{"commit_url":"https://github.com/godotengine/godot/commit/f9c42d9fffa8244eb84f6f0ab1f3d5f546b59c11","commit_message":"Limit window size updates on title translation change.","code_diff":"@@ -1301,7 +1301,13 @@ void Window::_notification(int p_what) {\n \n \t\t\tif (!embedder && window_id != DisplayServer::INVALID_WINDOW_ID) {\n \t\t\t\tDisplayServer::get_singleton()->window_set_title(tr_title, window_id);\n-\t\t\t\t_update_window_size();\n+\t\t\t\tif (keep_title_visible) {\n+\t\t\t\t\tSize2i title_size = DisplayServer::get_singleton()->window_get_title_size(tr_title, window_id);\n+\t\t\t\t\tSize2i size_limit = get_clamped_minimum_size();\n+\t\t\t\t\tif (title_size.x > size_limit.x || title_size.y > size_limit.y) {\n+\t\t\t\t\t\t_update_window_size();\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \t\t} break;\n \n","changed_method_name":"Window::_notification","commit_message_token_length":9,"code_token_length":253,"combined_token_length":262}
{"commit_url":"https://github.com/godotengine/godot/commit/77879d4288443fcacb0668bd867438b53bd800ae","commit_message":"Update NodePaths only in built-in resources","code_diff":"@@ -1812,8 +1812,6 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t}\n \t\t} break;\n \n-// FIXME: This approach causes a significant performance regression, see GH-84910.\n-#if 0\n \t\tcase Variant::OBJECT: {\n \t\t\tResource *resource = Object::cast_to<Resource>(r_variant);\n \t\t\tif (!resource) {\n@@ -1825,6 +1823,11 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t\tbreak;\n \t\t\t}\n \n+\t\t\tif (!resource->is_built_in()) {\n+\t\t\t\t// For performance reasons, assume that scene paths are no concern for external resources.\n+\t\t\t\tbreak;\n+\t\t\t}\n+\n \t\t\tList<PropertyInfo> properties;\n \t\t\tresource->get_property_list(&properties);\n \n@@ -1841,9 +1844,7 @@ bool SceneTreeDock::_check_node_path_recursive(Node *p_root_node, Variant &r_var\n \t\t\t\t\tundo_redo->add_undo_property(resource, propertyname, old_variant);\n \t\t\t\t}\n \t\t\t}\n-\t\t\tbreak;\n-\t\t};\n-#endif\n+\t\t} break;\n \n \t\tdefault: {\n \t\t}\n","changed_method_name":"SceneTreeDock::_check_node_path_recursive","commit_message_token_length":10,"code_token_length":386,"combined_token_length":396}
{"commit_url":"https://github.com/godotengine/godot/commit/b9225f67c85c3eab47df96b9fa1bd5a5034eae53","commit_message":"Warn that navigation mesh baking from Meshes is bad for runtime performance\n\nWarns that navigation mesh baking from Meshes is bad for runtime performance.","code_diff":"@@ -158,6 +158,15 @@ void NavigationMeshSourceGeometryData3D::_add_faces(const PackedVector3Array &p_\n \n void NavigationMeshSourceGeometryData3D::add_mesh(const Ref<Mesh> &p_mesh, const Transform3D &p_xform) {\n \tERR_FAIL_COND(!p_mesh.is_valid());\n+\n+#ifdef DEBUG_ENABLED\n+\tif (!Engine::get_singleton()->is_editor_hint()) {\n+\t\tWARN_PRINT_ONCE(\"Source geometry parsing for navigation mesh baking had to parse RenderingServer meshes at runtime.\\n\\\n+\t\tThis poses a significant performance issues as visual meshes store geometry data on the GPU and transferring this data back to the CPU blocks the rendering.\\n\\\n+\t\tFor runtime (re)baking navigation meshes use and parse collision shapes as source geometry or create geometry data procedurally in scripts.\");\n+\t}\n+#endif\n+\n \t_add_mesh(p_mesh, root_node_transform * p_xform);\n }\n \n","changed_method_name":"NavigationMeshSourceGeometryData3D::add_mesh","commit_message_token_length":32,"code_token_length":266,"combined_token_length":298}
{"commit_url":"https://github.com/godotengine/godot/commit/e201e5bf30bdd7ea4f515ebf8d47a70172683f7e","commit_message":"Fix Windows Activate Process","code_diff":"@@ -4727,6 +4727,7 @@ LRESULT DisplayServerWindows::WndProc(HWND hWnd, UINT uMsg, WPARAM wParam, LPARA\n \t\t} break;\n \t\tcase WM_EXITSIZEMOVE: {\n \t\t\tKillTimer(windows[window_id].hWnd, windows[window_id].move_timer_id);\n+\t\t\twindows[window_id].move_timer_id = 0;\n \t\t} break;\n \t\tcase WM_TIMER: {\n \t\t\tif (wParam == windows[window_id].move_timer_id) {\n","changed_method_name":"DisplayServerWindows::WndProc","commit_message_token_length":4,"code_token_length":159,"combined_token_length":163}
{"commit_url":"https://github.com/godotengine/godot/commit/4b266871d7ff6be7c1e73ecad07aac0954c85236","commit_message":"CVTT: Include float formats for hdr compression","code_diff":"@@ -149,7 +149,7 @@ void image_compress_cvtt(Image *p_image, Image::UsedChannels p_channels) {\n \tint h = p_image->get_height();\n \n \tbool is_ldr = (p_image->get_format() <= Image::FORMAT_RGBA8);\n-\tbool is_hdr = (p_image->get_format() >= Image::FORMAT_RH) && (p_image->get_format() <= Image::FORMAT_RGBE9995);\n+\tbool is_hdr = (p_image->get_format() >= Image::FORMAT_RF) && (p_image->get_format() <= Image::FORMAT_RGBE9995);\n \n \tif (!is_ldr && !is_hdr) {\n \t\treturn; // Not a usable source format\n","changed_method_name":"image_compress_cvtt","commit_message_token_length":10,"code_token_length":206,"combined_token_length":216}
{"commit_url":"https://github.com/godotengine/godot/commit/97085358ce154f34d6ab571c35529a8b0e063513","commit_message":"[Editor] Prevent unnecessary editor theme regeneration on unrelated system setting update.","code_diff":"@@ -670,7 +670,10 @@ void EditorNode::_notification(int p_what) {\n \n \t\t\tcallable_mp(this, &EditorNode::_begin_first_scan).call_deferred();\n \n-\t\t\tDisplayServer::get_singleton()->set_system_theme_change_callback(callable_mp(this, &EditorNode::_update_theme).bind(false));\n+\t\t\tlast_dark_mode_state = DisplayServer::get_singleton()->is_dark_mode();\n+\t\t\tlast_system_accent_color = DisplayServer::get_singleton()->get_accent_color();\n+\t\t\tlast_system_base_color = DisplayServer::get_singleton()->get_base_color();\n+\t\t\tDisplayServer::get_singleton()->set_system_theme_change_callback(callable_mp(this, &EditorNode::_check_system_theme_changed));\n \n \t\t\t/* DO NOT LOAD SCENES HERE, WAIT FOR FILE SCANNING AND REIMPORT TO COMPLETE */\n \t\t} break;\n","changed_method_name":"EditorNode::_notification","commit_message_token_length":14,"code_token_length":270,"combined_token_length":284}
{"commit_url":"https://github.com/opencv/opencv/commit/1ce5a724c73b26c4ec40f11c53eec22ac27b5031","commit_message":"Fixed StereoBM uniqueness check","code_diff":"@@ -558,7 +558,10 @@ static void findStereoCorrespondenceBM_SIMD( const Mat& left, const Mat& right,\n                 {\n                     v_int32 sad4_l = vx_load_expand((short*)sad + d);\n                     if (v_check_any((thresh4 > sad4_l) & ((d1 > d4) | (d4 > d2))))\n+                    {\n+                        dptr[y*dstep] = FILTERED;\n                         continue;\n+                    }\n                     d += v_int16::nlanes;\n                 }\n                 for( ; d < ndisp; d++ )\n","changed_method_name":"cv::findStereoCorrespondenceBM_SIMD","commit_message_token_length":6,"code_token_length":338,"combined_token_length":344}
{"commit_url":"https://github.com/opencv/opencv/commit/3bc1b5396258a85c6b3078b176cab3f7c8210c3a","commit_message":"Added YUV conversion fix\n\nFixed OpenCV issue #18878","code_diff":"@@ -104,6 +104,7 @@ inline int dstChannels(int code)\n             return 4;\n \n         case COLOR_BGRA2BGR: case COLOR_RGBA2BGR: case COLOR_RGB2BGR:\n+        case COLOR_YUV2RGB: case COLOR_YUV2BGR: case COLOR_RGB2YUV: case COLOR_BGR2YUV:\n         case COLOR_BGR5652BGR: case COLOR_BGR5552BGR: case COLOR_BGR5652RGB: case COLOR_BGR5552RGB:\n         case COLOR_GRAY2BGR:\n         case COLOR_YUV2BGR_NV21: case COLOR_YUV2RGB_NV21: case COLOR_YUV2BGR_NV12: case COLOR_YUV2RGB_NV12:\n","changed_method_name":"cv::impl::dstChannels","commit_message_token_length":14,"code_token_length":252,"combined_token_length":266}
{"commit_url":"https://github.com/opencv/opencv/commit/c526705f4f72fbd5720fb739ae7f01eb2d45d7b4","commit_message":"[cv::transform] Enable CV_SIMD for the 16U case on AArch64.","code_diff":"@@ -1537,7 +1537,7 @@ transform_8u( const uchar* src, uchar* dst, const float* m, int len, int scn, in\n static void\n transform_16u( const ushort* src, ushort* dst, const float* m, int len, int scn, int dcn )\n {\n-#if CV_SIMD && !defined(__aarch64__) && !defined(_M_ARM64)\n+#if CV_SIMD\n     if( scn == 3 && dcn == 3 )\n     {\n         int x = 0;\n","changed_method_name":"cv::transform_16u","commit_message_token_length":20,"code_token_length":154,"combined_token_length":174}
{"commit_url":"https://github.com/opencv/opencv/commit/028d4d83d3087db6ce789ec3fa900649cde761d9","commit_message":"imgproc: sigma2=sigma1 in top-level function of GaussianBlur","code_diff":"@@ -635,6 +635,9 @@ void GaussianBlur(InputArray _src, OutputArray _dst, Size ksize,\n         return;\n     }\n \n+    if (sigma2 <= 0)\n+        sigma2 = sigma1;\n+\n     bool useOpenCL = ocl::isOpenCLActivated() && _dst.isUMat() && _src.dims() <= 2 &&\n                _src.rows() >= ksize.height && _src.cols() >= ksize.width &&\n                ksize.width > 1 && ksize.height > 1;\n","changed_method_name":"cv::GaussianBlur","commit_message_token_length":20,"code_token_length":190,"combined_token_length":210}
{"commit_url":"https://github.com/opencv/opencv/commit/5dc5b2785884736f2889402502f35020b0481f45","commit_message":"Enable build with OpenVINO in Debug","code_diff":"@@ -252,7 +252,7 @@ void NetImplOpenVINO::addNgraphOutputs(LayerData& ld)\n             CV_Assert(!ieInpNode->net.empty());\n             if (layerNet != ieInpNode->net)\n             {\n-                CV_LOG_DEBUG(NULL, \"DNN/IE: pin output between subnets: \" << ieInpNode->node->get_friendly_name());\n+                CV_LOG_DEBUG(NULL, \"DNN/IE: pin output between subnets: \" << ieInpNode->node.get_node()->get_friendly_name());\n                 ieInpNode->net->addOutput(ieInpNode);\n             }\n         }\n","changed_method_name":"cv::dnn::NetImplOpenVINO::addNgraphOutputs","commit_message_token_length":8,"code_token_length":262,"combined_token_length":270}
{"commit_url":"https://github.com/opencv/opencv/commit/a289eba357dcdc21d088315cbf981afe9d7bb439","commit_message":"Fixes #24677","code_diff":"@@ -338,7 +338,7 @@ int runWinograd63(InputArray _input, InputArray _fusedAddMat, OutputArray _outpu\n                         }\n #if CV_TRY_AVX2\n                         if (conv->useAVX2)\n-                            opt_AVX::winofunc_AtXA_8x8_F32((float *)out_wbuf + ((k - k0)*CONV_WINO_IBLOCK + (block_id - block_id0))*CONV_WINO_AREA, CONV_WINO_SIZE,\n+                            opt_AVX2::winofunc_AtXA_8x8_F32((float *)out_wbuf + ((k - k0)*CONV_WINO_IBLOCK + (block_id - block_id0))*CONV_WINO_AREA, CONV_WINO_SIZE,\n                                                                 bpptr, outstep, outptr, outstep, biasv, minval, maxval, ifMinMaxAct);\n                         else\n #endif\n","changed_method_name":"cv::dnn::runWinograd63","commit_message_token_length":5,"code_token_length":428,"combined_token_length":433}
{"commit_url":"https://github.com/opencv/opencv/commit/db3654ef51b156feab4f59c13f2ee41ca2ab9a85","commit_message":"python: prefer cv::Mat over cv::UMat in python binding","code_diff":"@@ -854,7 +854,22 @@ class FuncInfo(object):\n \n         all_code_variants = []\n \n+        # See https://github.com/opencv/opencv/issues/25928\n+        # Conversion to UMat is expensive more than conversion to Mat.\n+        # To reduce this cost, conversion to Mat is prefer than to UMat.\n+        variants = []\n+        variants_umat = []\n         for v in self.variants:\n+            hasUMat = False\n+            for a in v.args:\n+                hasUMat = hasUMat or \"UMat\" in a.tp\n+            if hasUMat :\n+                variants_umat.append(v)\n+            else:\n+                variants.append(v)\n+        variants.extend(variants_umat)\n+\n+        for v in variants:\n             code_decl = \"\"\n             code_ret = \"\"\n             code_cvt_list = []\n","changed_method_name":"gen_code","commit_message_token_length":16,"code_token_length":402,"combined_token_length":418}
{"commit_url":"https://github.com/ClickHouse/ClickHouse/commit/eeb78bf29172112ee832e2e46ed75271961717d9","commit_message":"slightly optimize very short queries with LowCardinality","code_diff":"@@ -43,11 +43,13 @@ MergeTreeReaderStream::MergeTreeReaderStream(\n         /// If the end of range is inside the block, we will need to read it too.\n         if (right_mark < marks_count && marks_loader.getMark(right_mark).offset_in_decompressed_block > 0)\n         {\n-            while (right_mark < marks_count\n-                && marks_loader.getMark(right_mark).offset_in_compressed_file == marks_loader.getMark(mark_range.end).offset_in_compressed_file)\n+            auto indices = ext::range(right_mark, marks_count);\n+            auto it = std::upper_bound(indices.begin(), indices.end(), right_mark, [this](size_t i, size_t j)\n             {\n-                ++right_mark;\n-            }\n+                return marks_loader.getMark(i).offset_in_compressed_file < marks_loader.getMark(j).offset_in_compressed_file;\n+            });\n+\n+            right_mark = (it == indices.end() ? marks_count : *it);\n         }\n \n         size_t mark_range_bytes;\n","changed_method_name":"DB::MergeTreeReaderStream::MergeTreeReaderStream","commit_message_token_length":11,"code_token_length":442,"combined_token_length":453}
{"commit_url":"https://github.com/ClickHouse/ClickHouse/commit/3e700e854d6a1620bf80da5340206e1ec691b22a","commit_message":"cancel merges before acquiring lock for truncate","code_diff":"@@ -35,6 +35,10 @@ namespace ErrorCodes\n     extern const int TABLE_IS_READ_ONLY;\n }\n \n+namespace ActionLocks\n+{\n+    extern const StorageActionBlockType PartsMerge;\n+}\n \n static DatabasePtr tryGetDatabase(const String & database_name, bool if_exists)\n {\n@@ -202,7 +206,15 @@ BlockIO InterpreterDropQuery::executeToTableImpl(ContextPtr context_, ASTDropQue\n \n             table->checkTableCanBeDropped();\n \n-            auto table_lock = table->lockExclusively(context_->getCurrentQueryId(), context_->getSettingsRef().lock_acquire_timeout);\n+            TableExclusiveLockHolder table_lock;\n+            /// We don't need this lock for ReplicatedMergeTree\n+            if (!table->supportsReplication())\n+            {\n+                /// And for simple MergeTree we can stop merges before acquiring the lock\n+                auto merges_blocker = table->getActionLock(ActionLocks::PartsMerge);\n+                auto table_lock = table->lockExclusively(context_->getCurrentQueryId(), context_->getSettingsRef().lock_acquire_timeout);\n+            }\n+\n             auto metadata_snapshot = table->getInMemoryMetadataPtr();\n             /// Drop table data, don't touch metadata\n             table->truncate(query_ptr, metadata_snapshot, context_, table_lock);\n","changed_method_name":"DB::InterpreterDropQuery::executeToTableImpl","commit_message_token_length":10,"code_token_length":501,"combined_token_length":511}
{"commit_url":"https://github.com/ClickHouse/ClickHouse/commit/a45e3d47adacc59699de26b59e5966307b97b8fb","commit_message":"Remove useless codec from system.asynchronous_metric_log","code_diff":"@@ -40,7 +40,7 @@ struct AsynchronousMetricLogElement\n         return \"event_date Date CODEC(Delta(2), ZSTD(1)), \"\n                \"event_time DateTime CODEC(Delta(4), ZSTD(1)), \"\n                \"metric LowCardinality(String) CODEC(ZSTD(1)), \"\n-               \"value Float64 CODEC(Gorilla, ZSTD(3))\";\n+               \"value Float64 CODEC(ZSTD(3))\";\n     }\n };\n \n","changed_method_name":"DB::AsynchronousMetricLogElement::getCustomColumnList","commit_message_token_length":13,"code_token_length":193,"combined_token_length":206}
{"commit_url":"https://github.com/ClickHouse/ClickHouse/commit/6c8fc4cd11966cb96571fe3a2dfaac6b82cbd03c","commit_message":"fix hashjoin debug code condition","code_diff":"@@ -495,7 +495,7 @@ size_t HashJoin::getTotalByteCount() const\n     if (!data)\n         return 0;\n \n-#ifdef NDEBUG\n+#ifndef NDEBUG\n     size_t debug_blocks_allocated_size = 0;\n     for (const auto & block : data->blocks)\n         debug_blocks_allocated_size += block.allocatedBytes();\n","changed_method_name":"DB::HashJoin::getTotalByteCount","commit_message_token_length":6,"code_token_length":120,"combined_token_length":126}
{"commit_url":"https://github.com/qbittorrent/qBittorrent/commit/534ed91d043abfe8ad7ccd307e4c8b060bdaf214","commit_message":"Change MixedModeAlgorithm default to TCP. Closes #7779.\n\nMixedModeAlgorithm::Proportional will throttle TCP connections when utp\nis in use and users is expecting maximum speed no matter what, so now\ndisable the throttling.","code_diff":"@@ -304,7 +304,7 @@ Session::Session(QObject *parent)\n     , m_btProtocol(BITTORRENT_SESSION_KEY(\"BTProtocol\"), BTProtocol::Both\n         , clampValue(BTProtocol::Both, BTProtocol::UTP))\n     , m_isUTPRateLimited(BITTORRENT_SESSION_KEY(\"uTPRateLimited\"), true)\n-    , m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::Proportional\n+    , m_utpMixedMode(BITTORRENT_SESSION_KEY(\"uTPMixedMode\"), MixedModeAlgorithm::TCP\n         , clampValue(MixedModeAlgorithm::TCP, MixedModeAlgorithm::Proportional))\n     , m_multiConnectionsPerIpEnabled(BITTORRENT_SESSION_KEY(\"MultiConnectionsPerIp\"), false)\n     , m_isAddTrackersEnabled(BITTORRENT_SESSION_KEY(\"AddTrackersEnabled\"), false)\n","changed_method_name":"Session::Session","commit_message_token_length":55,"code_token_length":287,"combined_token_length":342}
{"commit_url":"https://github.com/qbittorrent/qBittorrent/commit/2f1a0ffe5c3e01a1903809106df7156e86a22201","commit_message":"Use a more detailed alert mask where possible\n\nCloses #9547","code_diff":"@@ -392,7 +392,11 @@ Session::Session(QObject *parent)\n                     | libt::alert::tracker_notification\n                     | libt::alert::status_notification\n                     | libt::alert::ip_block_notification\n+#if LIBTORRENT_VERSION_NUM < 10110\n                     | libt::alert::progress_notification\n+#else\n+                    | libt::alert::file_progress_notification\n+#endif\n                     | libt::alert::stats_notification;\n \n #if LIBTORRENT_VERSION_NUM < 10100\n","changed_method_name":"Session::Session","commit_message_token_length":15,"code_token_length":251,"combined_token_length":266}
{"commit_url":"https://github.com/hrydgard/ppsspp/commit/354d263ccf4cf8f3913433b1b6f7f534e497de82","commit_message":"sceKernelFindModuleByName:Add delay for Fake module\n\nFix #13601","code_diff":"@@ -2444,12 +2444,18 @@ u32 sceKernelFindModuleByName(const char *name)\n \t\tPSPModule *module = kernelObjects.Get<PSPModule>(moduleId, error);\n \t\tif (!module)\n \t\t\tcontinue;\n-\t\tif (!module->isFake && strcmp(name, module->nm.name) == 0) {\n-\t\t\tINFO_LOG(SCEMODULE, \"%d = sceKernelFindModuleByName(%s)\", module->modulePtr, name);\n-\t\t\treturn module->modulePtr;\n+\t\tif (strcmp(name, module->nm.name) == 0) {\n+\t\t\tif (!module->isFake) {\n+\t\t\t\tINFO_LOG(SCEMODULE, \"%d = sceKernelFindModuleByName(%s)\", module->modulePtr, name);\n+\t\t\t\treturn module->modulePtr;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Fake\", name);\n+\t\t\t\treturn hleDelayResult(0, \"Module Fake\", 1000 * 1000);\n+\t\t\t}\n \t\t}\n \t}\n-\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Not Found or Fake\", name);\n+\tWARN_LOG(SCEMODULE, \"0 = sceKernelFindModuleByName(%s): Module Not Found\", name);\n \treturn 0;\n }\n \n","changed_method_name":"sceKernelFindModuleByName","commit_message_token_length":20,"code_token_length":389,"combined_token_length":409}
{"commit_url":"https://github.com/hrydgard/ppsspp/commit/adda49d05d1091decb8832259f0ca4e9e085f143","commit_message":"Add a heuristic avoiding joining framebuffers horizontally\n\n...when texturing from the other one.\n\nGreatly improves GPU performance in Rainbow Six: Vegas.\n\nFixes #9324.","code_diff":"@@ -426,6 +426,14 @@ VirtualFramebuffer *FramebufferManagerCommon::DoSetRenderFrameBuffer(Framebuffer\n \t\t\tu32 v_fb_end_ptr = v->fb_address + v->fb_stride * v->height * bpp;\n \n \t\t\tif (params.fb_address > v->fb_address && params.fb_address < v_fb_first_line_end_ptr) {\n+\t\t\t\t// If the framebuffer we can join to is currently bound as a texture, we likely have\n+\t\t\t\t// a situation like in #9324 and don't want to do this.\n+\t\t\t\tu32 curTextureAddress = gstate.getTextureAddress(0);\n+\t\t\t\tif (v->fb_address == curTextureAddress) {\n+\t\t\t\t\t// Don't try these joining shenanigans.\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\n \t\t\t\tconst int x_offset = (params.fb_address - v->fb_address) / bpp;\n \t\t\t\tif (x_offset < params.fb_stride && v->height >= drawing_height) {\n \t\t\t\t\t// Pretty certainly a pure render-to-X-offset.\n","changed_method_name":"FramebufferManagerCommon::DoSetRenderFrameBuffer","commit_message_token_length":42,"code_token_length":300,"combined_token_length":342}
{"commit_url":"https://github.com/NixOS/nix/commit/0f977bf91e29192d7f0c0f9cad16351bad7cd137","commit_message":"Remove a useless debug message in filetransfer.cc\n\nRemove the `verify TLS: Nix CA file = 'blah'` message that Nix used to print when fetching anything as it's both useless (`libcurl` prints the same info in its logs) and misleading (gives the impression that a new TLS connection is being established which might not be the case because of multiplexing. See #7011 )","code_diff":"@@ -322,7 +322,6 @@ struct curlFileTransfer : public FileTransfer\n             }\n \n             if (request.verifyTLS) {\n-                debug(\"verify TLS: Nix CA file = '%s'\", settings.caFile);\n                 if (settings.caFile != \"\")\n                     curl_easy_setopt(req, CURLOPT_CAINFO, settings.caFile.c_str());\n             } else {\n","changed_method_name":"nix::curlFileTransfer::TransferItem::init","commit_message_token_length":91,"code_token_length":185,"combined_token_length":276}
{"commit_url":"https://github.com/redpanda-data/redpanda/commit/666085351618ea56365661c47af7c97fd959bfd0","commit_message":"k/topic_utils: do not return an error when waiting for leaders failed\n\nRecently introduced change (waiting for leaders before returning from\ncreate topics) change the behavior of `CreateTopicsRequest` handler.\nPreviously handler was waiting for topic partitions to be created but\neven if that timed out it returned success to the client. Waiting for\nleaders threw an exception when timeout was trigger which caused client\nconnection to be dropped.\n\nFixed an error by going back to previous behavior i.e. ignoring waiting\ntimeouts. The timeout does not indicate that topic creation failed\nactually when waiting for creation topic already exists.\n\nFixes: #7942\n\nSigned-off-by: Michal Maslanka <michal@redpanda.com>","code_diff":"@@ -79,7 +79,11 @@ ss::future<> wait_for_topics(\n                    })\n             .then([&md_cache, &results, timeout]() {\n                 return wait_for_leaders(md_cache, results, timeout)\n-                  .discard_result();\n+                  .discard_result()\n+                  .handle_exception_type([](const ss::timed_out_error&) {\n+                      // discard timed out exception, even tho waiting failed\n+                      // the topic is created\n+                  });\n             });\n       });\n }\n","changed_method_name":"kafka::wait_for_topics","commit_message_token_length":162,"code_token_length":299,"combined_token_length":461}
{"commit_url":"https://github.com/redpanda-data/redpanda/commit/074e634a856dbcb45132d41a470d33f9922d6a2d","commit_message":"rptest/scale_tests: limit producer rate in test","code_diff":"@@ -40,6 +40,12 @@ class ManyClientsTest(RedpandaTest):\n             # Enable segment size jitter as this is a stress test and does not\n             # rely on exact segment counts.\n             'log_segment_size_jitter_percent': 5,\n+            # This limit caps the produce throughput to a sustainable rate for a RP\n+            # cluster that has 384MB of memory per shard. It is set here to\n+            # since our current backpressure mechanisms will allow producers to\n+            # produce at a much higher rate and cause RP to run out of memory.\n+            'target_quota_byte_rate':\n+            31460000,  # 30MiB/s of throughput per shard\n         }\n         super().__init__(*args, **kwargs)\n \n","changed_method_name":"__init__","commit_message_token_length":13,"code_token_length":293,"combined_token_length":306}
{"commit_url":"https://github.com/redpanda-data/redpanda/commit/6d1223d4e4b0489c0fade40a727b9633d506398a","commit_message":"kafka: Disable use of separate fetch scheduling group\n\nThis partially reverts 9a93a9c22238e145dfa9d7fe297eb494d7c5f0bf\n\nWhile the original motiviation isn't invalidated we have now found a\ncounter example where the extra fetch groups makes things worse overall.\n\n`ManyPartitionTest` fails on ARM with the extra group but passes\nwithout. With the group in use CPU util hits 100% and grinds everything\nto halt.\n\nFetch seems to be a lot slower on ARM. Hence, with the guaranteed share\nof the extra group the whole system gets affected and hits CPU limits.\n\nBecause this is incredibly hard to reason about and it wasn't the core\nfetch optimization we decided to revert back to keeping it disabled by\ndefault.\n\nWe still keep the option around as it might be useful potentially in\ncorner cases.\n\nFixes https://github.com/redpanda-data/redpanda/issues/10507","code_diff":"@@ -516,7 +516,7 @@ configuration::configuration()\n       \"use_fetch_scheduler_group\",\n       \"Use a separate scheduler group for fetch processing\",\n       {.needs_restart = needs_restart::no, .visibility = visibility::tunable},\n-      true)\n+      false)\n   , metadata_status_wait_timeout_ms(\n       *this,\n       \"metadata_status_wait_timeout_ms\",\n","changed_method_name":"config::configuration::configuration","commit_message_token_length":221,"code_token_length":143,"combined_token_length":364}
{"commit_url":"https://github.com/ElucidataInc/ElMaven/commit/b4583dbb174a13617ff592a4599a099dd3389519","commit_message":"Remove unnecessary vector copy\n\nTemporary matrices being used in `find_path` method are attempted\nto be copied into ObiWarp's class matrices at the end. But these\nmatrices themselves are not being used anywhere else. The copy\noperation was causing a crash due to attempts to free unallocated\nmemory. The copy operations have been removed to prevent this\ncrash.\n\nIssue: #859","code_diff":"@@ -1301,10 +1301,5 @@ void DynProg::find_path(MatF& smat, VecF &gap_penalty, int minimize, float diag_\n     _traceback(tmp_tb, smat, optimal_m, optimal_n, tmp_tbpath, _mCoords, _nCoords, _sCoords); \n     int _equivLastInd = _mCoords.dim()-1;\n     _bestScore = tmp_asmat(_mCoords[_equivLastInd],_nCoords[_equivLastInd]);\n-\n-    _asmat.take(tmp_asmat);\n-    _tb.take(tmp_tb);\n-    _tbpath.take(tmp_tbpath);\n-    _gapmat.take(tmp_gapmat);\n }\n \n","changed_method_name":"DynProg::find_path","commit_message_token_length":89,"code_token_length":217,"combined_token_length":306}
{"commit_url":"https://github.com/project-asgard/asgard/commit/076f8b296ce85de4ae54ccfea88f8074e42c6ffe","commit_message":"improve perf of combine dims using views","code_diff":"@@ -219,10 +219,11 @@ combine_dimensions(int const degree, element_table const &table,\n           degree > 1 ? (((id + 1) * degree) - 1) : index_start;\n       kron_list.push_back(vectors[j].extract(index_start, index_end));\n     }\n-    fk::vector<P> const partial_result =\n-        kron_d(kron_list, kron_list.size()) * time_scale;\n-    combined.set_subvector((i - start_element) * std::pow(degree, num_dims),\n-                           partial_result);\n+    int const start_index = (i - start_element) * std::pow(degree, num_dims);\n+    int const stop_index  = start_index + std::pow(degree, num_dims) - 1;\n+    fk::vector<P, mem_type::view> combined_view(combined, start_index,\n+                                                stop_index);\n+    combined_view = kron_d(kron_list, kron_list.size()) * time_scale;\n   }\n   return combined;\n }\n","changed_method_name":"combine_dimensions","commit_message_token_length":8,"code_token_length":395,"combined_token_length":403}
{"commit_url":"https://github.com/matplotlib/matplotlib/commit/1e32084fccdd8e027790a5afe0680a87454f5f89","commit_message":"Avoid quadratic behavior when accumulating stickies.\n\nWhen plotting 10000 lines, this gives a ~10% improvement in performance.","code_diff":"@@ -2405,12 +2405,12 @@ class _AxesBase(martist.Artist):\n \n         if self.use_sticky_edges and (self._xmargin or self._ymargin):\n             stickies = [artist.sticky_edges for artist in self.get_children()]\n-            x_stickies = sum([sticky.x for sticky in stickies], [])\n-            y_stickies = sum([sticky.y for sticky in stickies], [])\n+            x_stickies = np.array([x for sticky in stickies for x in sticky.x])\n+            y_stickies = np.array([y for sticky in stickies for y in sticky.y])\n             if self.get_xscale().lower() == 'log':\n-                x_stickies = [xs for xs in x_stickies if xs > 0]\n+                x_stickies = x_stickies[x_stickies > 0]\n             if self.get_yscale().lower() == 'log':\n-                y_stickies = [ys for ys in y_stickies if ys > 0]\n+                y_stickies = y_stickies[y_stickies > 0]\n         else:  # Small optimization.\n             x_stickies, y_stickies = [], []\n \n","changed_method_name":"autoscale_view","commit_message_token_length":27,"code_token_length":468,"combined_token_length":495}
{"commit_url":"https://github.com/matplotlib/matplotlib/commit/318a5894fd4958ce9cdaaed9ac81e1143cbc37a6","commit_message":"Disable sticky edge accumulation if no autoscaling.\n\nIf there's no margin to be added, we don't need sticky edges, but if\nautoscaling is off, we _also_ don't need the sticky edges. This saves a\nlot of time when plotting many artists, like in #12542.","code_diff":"@@ -2403,7 +2403,9 @@ class _AxesBase(martist.Artist):\n         if tight is not None:\n             self._tight = bool(tight)\n \n-        if self.use_sticky_edges and (self._xmargin or self._ymargin):\n+        if self.use_sticky_edges and (\n+                (self._xmargin and scalex and self._autoscaleXon) or\n+                (self._ymargin and scaley and self._autoscaleYon)):\n             stickies = [artist.sticky_edges for artist in self.get_children()]\n             x_stickies = sum([sticky.x for sticky in stickies], [])\n             y_stickies = sum([sticky.y for sticky in stickies], [])\n","changed_method_name":"autoscale_view","commit_message_token_length":66,"code_token_length":284,"combined_token_length":350}
{"commit_url":"https://github.com/matplotlib/matplotlib/commit/1e6352ab60409d936c8073343329abf3ebbe750b","commit_message":"Special case degree-1 Bezier curves.\n\nThis greatly speeds up extent computation for the common case of\npolylines.  (We were previously only special-casing the degree-0 case.)","code_diff":"@@ -287,10 +287,10 @@ class BezierSegment:\n             0`\n         \"\"\"\n         n = self.degree\n+        if n <= 1:\n+            return np.array([]), np.array([])\n         Cj = self.polynomial_coefficients\n         dCj = np.arange(1, n+1)[:, None] * Cj[1:]\n-        if len(dCj) == 0:\n-            return np.array([]), np.array([])\n         dims = []\n         roots = []\n         for i, pi in enumerate(dCj.T):\n","changed_method_name":"axis_aligned_extrema","commit_message_token_length":43,"code_token_length":241,"combined_token_length":284}
{"commit_url":"https://github.com/iterative/dvc/commit/c4836b4578f59a0923be5655259a11538ca193fc","commit_message":"dvc: optmize PathInfo.isin()\n\nShould help with #2203","code_diff":"@@ -19,6 +19,10 @@ if is_py2:\n \n \n class PathInfo(pathlib.PurePath):\n+    # Use __slots__ in PathInfo objects following PurePath implementation.\n+    # This makes objects smaller and speeds up attribute access.\n+    # We don't add any fields so it's empty.\n+    __slots__ = ()\n     scheme = \"local\"\n \n     def __new__(cls, *args):\n@@ -57,7 +61,9 @@ class PathInfo(pathlib.PurePath):\n             other = self.__class__(other)\n         elif self.__class__ != other.__class__:\n             return False\n-        return any(p == other for p in self.parents)\n+        # Use cached casefolded parts to compare paths\n+        n = len(other._cparts)\n+        return len(self._cparts) > n and self._cparts[:n] == other._cparts\n \n     # pathlib2 uses bytes internally in Python 2, and we use unicode everywhere\n     # for paths in both pythons, thus we need this glue.\n","changed_method_name":"isin","commit_message_token_length":19,"code_token_length":342,"combined_token_length":361}
{"commit_url":"https://github.com/sympy/sympy/commit/f7cecfea016dedf7ad042be0c8da9df1ef19c756","commit_message":"perf(core): make sympify faster for Basic subclasses.\n\nThe behaviour of sympify was changed in\n\n  https://github.com/sympy/sympy/pull/20128\n\nso that sympifying a Basic subclass would be an error when strict=True.\nThat change made the codepath for calling e.g. _sympify(Basic) slower as\nmore checks would be done before returning. This commit adds an early\nraise for the case of calling _sympify(a) where a is a Basic subclass.","code_diff":"@@ -344,8 +344,13 @@ def sympify(a, locals=None, convert_xor=True, strict=False, rational=False,\n     #\n     # https://github.com/sympy/sympy/issues/20124\n     is_sympy = getattr(a, '__sympy__', None)\n-    if is_sympy is True or (is_sympy is not None and not strict):\n+    if is_sympy is True:\n         return a\n+    elif is_sympy is not None:\n+        if not strict:\n+            return a\n+        else:\n+            raise SympifyError(a)\n \n     if isinstance(a, CantSympify):\n         raise SympifyError(a)\n","changed_method_name":"sympify","commit_message_token_length":120,"code_token_length":254,"combined_token_length":374}
{"commit_url":"https://github.com/frappe/frappe/commit/aeec01c7f933615c126fd7b6a1a832c54f8d230a","commit_message":"perf(Scheduling): add jitter to job scheduling\n\nAddresses #19007","code_diff":"@@ -2,7 +2,8 @@\n # License: MIT. See LICENSE\n \n import json\n-from datetime import datetime\n+from datetime import datetime, timedelta\n+from random import randint\n \n import click\n from croniter import croniter\n@@ -110,7 +111,12 @@ class ScheduledJobType(Document):\n \t\t# immediately, even when it's meant to be daily.\n \t\t# A dynamic fallback like current time might miss the scheduler interval and job will never start.\n \t\tlast_execution = get_datetime(self.last_execution or self.creation)\n-\t\treturn croniter(self.cron_format, last_execution).get_next(datetime)\n+\t\tnext_execution = croniter(self.cron_format, last_execution).get_next(datetime)\n+\n+\t\tjitter = 0\n+\t\tif self.frequency in (\"Hourly Long\", \"Daily Long\"):\n+\t\t\tjitter = randint(1, 600)\n+\t\treturn next_execution + timedelta(seconds=jitter)\n \n \tdef execute(self):\n \t\tself.scheduler_log = None\n","changed_method_name":"get_next_execution","commit_message_token_length":20,"code_token_length":294,"combined_token_length":314}
{"commit_url":"https://github.com/astropy/astropy/commit/752887c906ebc093c2b67474feb8e0b496887beb","commit_message":"Replaced pow with a multiply","code_diff":"@@ -43,7 +43,7 @@ void compute_sigma_clipped_bounds(double data_buffer[], int count, int use_media\n \n       std = 0;\n       for (i = 0; i < count; i++) {\n-        std += pow(mean - data_buffer[i], 2);\n+        std += (data_buffer[i] - mean) * (data_buffer[i] - mean);\n       }\n       std = sqrt(std / count);\n \n","changed_method_name":"compute_sigma_clipped_bounds","commit_message_token_length":6,"code_token_length":150,"combined_token_length":156}
{"commit_url":"https://github.com/Bitmessage/PyBitmessage/commit/03316496b7c3380c5ac408f86d049855dbcedac6","commit_message":"Stop UDPSocket on socket.error 101 (Network is unreachable)","code_diff":"@@ -146,6 +146,9 @@ class UDPSocket(BMProto):  # pylint: disable=too-many-instance-attributes\n             retval = self.socket.sendto(\n                 self.write_buf, ('<broadcast>', self.port))\n         except socket.error as e:\n-            logger.error(\"socket error on sendato: %s\", e)\n+            logger.error(\"socket error on sendto: %s\", e)\n+            if e.errno == 101:\n+                self.announcing = False\n+                self.socket.close()\n             retval = 0\n         self.slice_write_buf(retval)\n","changed_method_name":"handle_write","commit_message_token_length":15,"code_token_length":268,"combined_token_length":283}
{"commit_url":"https://github.com/Bitmessage/PyBitmessage/commit/11bec55be56e73502fbd569bf0327a8876b54315","commit_message":"Don't put addresses into queue\n\n- attempt to fix #1598\n- seems to work\n- addresses won't be uploaded/announced anymore other than after connecting,\n  Later I need to find out how to announce them without causing problems, but\n  for the time disabling this seems an acceptable drawback","code_diff":"@@ -31,7 +31,7 @@ from network.dandelion import Dandelion\n from network.proxy import ProxyError\n from node import Node, Peer\n from objectracker import ObjectTracker, missingObjects\n-from queues import addrQueue, invQueue, objectProcessorQueue, portCheckerQueue\n+from queues import invQueue, objectProcessorQueue, portCheckerQueue\n from randomtrackingdict import RandomTrackingDict\n \n logger = logging.getLogger('default')\n@@ -466,8 +466,9 @@ class BMProto(AdvancedDispatcher, ObjectTracker):\n                             }\n                     # since we don't track peers outside of knownnodes,\n                     # only spread if in knownnodes to prevent flood\n-                    addrQueue.put((stream, peer, seenTime,\n-                                   self.destination))\n+                    # DISABLED TO WORKAROUND FLOOD/LEAK\n+                    # addrQueue.put((stream, peer, seenTime,\n+                    #               self.destination))\n         return True\n \n     def bm_command_portcheck(self):\n","changed_method_name":"bm_command_addr","commit_message_token_length":63,"code_token_length":441,"combined_token_length":504}
{"commit_url":"https://github.com/python-rope/rope/commit/ae8ea5d0a615936bc6f5b0964910566565d6b6b7","commit_message":"Modifies default_config.py","code_diff":"@@ -15,7 +15,8 @@ def set_prefs(prefs):\n     # 'build/*.o': matches 'build/lib.o' but not 'build/sub/lib.o'\n     # 'build//*.o': matches 'build/lib.o' and 'build/sub/lib.o'\n     prefs['ignored_resources'] = ['*.pyc', '*~', '.ropeproject',\n-                                  '.hg', '.svn', '_svn', '.git', '.tox']\n+                                  '.hg', '.svn', '_svn', '.git', '.tox',\n+                                  '.venv', 'venv']\n \n     # Specifies which files should be considered python files.  It is\n     # useful when you have scripts inside your project.  Only files\n","changed_method_name":"set_prefs","commit_message_token_length":7,"code_token_length":301,"combined_token_length":308}
{"commit_url":"https://github.com/scikit-bio/scikit-bio/commit/76b40140b6e9104288932f60c2a10b7b445f41fa","commit_message":"ENH: performance enhancement, fixes #411","code_diff":"@@ -1014,7 +1014,7 @@ class BiologicalSequence(Sequence):\n             step = k\n \n         for i in range(0, sequence_length - k + 1, step):\n-            yield constructor(self[i:i+k])\n+            yield self._sequence[i:i+k]\n \n     def k_word_counts(self, k, overlapping=True, constructor=str):\n         \"\"\"Get the counts of words of length k\n","changed_method_name":"k_words","commit_message_token_length":9,"code_token_length":158,"combined_token_length":167}
{"commit_url":"https://github.com/microsoft/ptvsd/commit/1a13aa2450fee2a0aac4ecbe84d448a1700309b7","commit_message":"Fix #967: Debugging Django in VSC with subprocess debugging is very slow\n\nAvoid hogging CPU while waiting on the subprocess notification queue.","code_diff":"@@ -1296,7 +1296,7 @@ class VSCodeMessageProcessor(VSCLifecycleMsgProcessor):\n     def _subprocess_notifier(self):\n         while not self.closed:\n             try:\n-                subprocess_request, subprocess_response = multiproc.subprocess_queue.get(block=False, timeout=0.1)\n+                subprocess_request, subprocess_response = multiproc.subprocess_queue.get(timeout=0.1)\n             except queue.Empty:\n                 continue\n \n","changed_method_name":"_subprocess_notifier","commit_message_token_length":33,"code_token_length":202,"combined_token_length":235}
{"commit_url":"https://github.com/GeoStat-Framework/GSTools/commit/c88f7bab0e309f7a32c1a61786ba340245d215a2","commit_message":"emcee: vectorize sampling since all ln_pdfs accept numpy arrays","code_diff":"@@ -86,7 +86,7 @@ class RNG:\n             self.random.rand(nwalkers).reshape((nwalkers, 1)) * sample_around\n         )\n         # initialize the sampler\n-        sampler = mc.EnsembleSampler(nwalkers, 1, ln_pdf)\n+        sampler = mc.EnsembleSampler(nwalkers, 1, ln_pdf, vectorize=True)\n         # burn in phase with saving of last position\n         initial_state = State(init_guess, copy=True)\n         initial_state.random_state = self.random.get_state()\n","changed_method_name":"sample_ln_pdf","commit_message_token_length":18,"code_token_length":207,"combined_token_length":225}
{"commit_url":"https://github.com/yt-project/yt/commit/68068dd0212b3cb5bad2fde085ddc8be4cff4a33","commit_message":"I believe this logic needs to be reversed.","code_diff":"@@ -218,12 +218,12 @@ class ParticleIndex(Index):\n             else:\n                 # TODO: only return files\n                 if getattr(dobj.selector, 'is_all_data', False):\n+                    nfiles = self.regions.nfiles\n+                    dfi = np.arange(nfiles)\n+                else:\n                     dfi, file_masks, addfi = self.regions.identify_file_masks(\n                         dobj.selector)\n                     nfiles = len(file_masks)\n-                else:\n-                    nfiles = self.regions.nfiles\n-                    dfi = np.arange(nfiles)\n                 dobj._chunk_info = [None for _ in range(nfiles)]\n                 for i in range(nfiles):\n                     domain_id = i+1\n","changed_method_name":"_identify_base_chunk","commit_message_token_length":9,"code_token_length":452,"combined_token_length":461}
{"commit_url":"https://github.com/mindsdb/lightwood/commit/f4449086b19f018d3c5d5fb8741fa8ecf6b50d1b","commit_message":"fix: activate optuna for lgbm_array iff horizon < 10","code_diff":"@@ -49,7 +49,7 @@ class LightGBMArray(BaseMixer):\n                                 dtype_dict,\n                                 input_cols,\n                                 False,  # fit_on_dev,\n-                                True,  # use_optuna\n+                                True if tss.horizon < 10 else False,  # use_optuna\n                                 target_encoder)\n                        for _, target_col in zip(range(self.horizon), [target] + self.offset_pred_cols)]\n         self.ts_analysis = ts_analysis\n","changed_method_name":"__init__","commit_message_token_length":16,"code_token_length":341,"combined_token_length":357}
{"commit_url":"https://github.com/static-frame/static-frame/commit/2cda5f824ba56a082b3874a836c4a7c8c607d902","commit_message":"'Fix' property test by ignoring date cases. Is referenced by issue #132","code_diff":"@@ -158,7 +158,9 @@ class TestUnit(TestCase):\n     @given(sfst.get_frame()) # type: ignore\n     def test_frame_isin(self, f1: Frame) -> None:\n         value = f1.iloc[0, 0]\n-        if not isna_element(value):\n+        if (not isna_element(value) and\n+                not isinstance(value, np.datetime64) and\n+                not isinstance(value, np.timedelta64)):\n             self.assertTrue(f1.isin((value,)).iloc[0, 0])\n \n \n","changed_method_name":"test_frame_isin","commit_message_token_length":16,"code_token_length":216,"combined_token_length":232}
{"commit_url":"https://github.com/gem/oq-engine/commit/37c2e86bd0821960318310771d6d7f824b33c7e5","commit_message":"Optimizing gmpe_table._get_mean","code_diff":"@@ -50,11 +50,8 @@ def _get_mean_(kind, data, dists, table_dists):\n     :return:\n         The mean intensity measure level from the tables.\n     \"\"\"\n-    # For values outside of the interpolation range use -999. to ensure\n     # value is identifiable and outside of potential real values\n-    interpolator_mean = interp1d(\n-        table_dists, data, bounds_error=False, fill_value=-999.)\n-    mean = interpolator_mean(dists)\n+    mean = numpy.interp(dists, table_dists, data)\n     # For those distances less than or equal to the shortest distance\n     # extrapolate the shortest distance value\n     mean[dists < (table_dists[0] + 1.0E-3)] = data[0]\n","changed_method_name":"_get_mean_","commit_message_token_length":12,"code_token_length":237,"combined_token_length":249}
{"commit_url":"https://github.com/gem/oq-engine/commit/be813aa4a76c3689f45aca18c7b93aa64a446c0b","commit_message":"Fixed misprint making counting the ruptures ultra-slow in event_based","code_diff":"@@ -403,7 +403,7 @@ class EventBasedCalculator(base.HazardCalculator):\n         with self.monitor('counting ruptures', measuremem=True):\n             nrups = parallel.Starmap( # weighting the heavy sources\n                 count_ruptures, [(src,) for src in sources\n-                                 if src.code == b'AMSC'],\n+                                 if src.code in b'AMSC'],\n                 progress=logging.debug).reduce()\n             # NB: multifault sources must be considered light to avoid a large\n             # data transfer, even if .count_ruptures can be slow\n","changed_method_name":"build_events_from_sources","commit_message_token_length":15,"code_token_length":279,"combined_token_length":294}
{"commit_url":"https://github.com/gem/oq-engine/commit/c9a92dac9b354a5418c8771ceb5b7d529186c728","commit_message":"Gzipping the _rates","code_diff":"@@ -400,7 +400,7 @@ class ClassicalCalculator(base.HazardCalculator):\n         self.cmakers = read_cmakers(self.datastore, self.csm)\n         self.cfactor = numpy.zeros(3)\n         self.rel_ruptures = AccumDict(accum=0)  # grp_id -> rel_ruptures\n-        self.datastore.create_df('_rates', rates_dt.items())\n+        self.datastore.create_df('_rates', rates_dt.items(), 'gzip')\n         self.datastore.create_dset('_rates/slice_by_sid', slice_dt)\n         # NB: compressing the dataset causes a big slowdown in writing :-(\n \n","changed_method_name":"init_poes","commit_message_token_length":6,"code_token_length":233,"combined_token_length":239}
{"commit_url":"https://github.com/aol/cyclops/commit/2ccc16aa8d87c73e96420d7410d435655ab013e7","commit_message":"Fix for #1056","code_diff":"@@ -260,7 +260,11 @@ public interface Seq<T> extends ImmutableList<T>,\n         return Cons.cons(value,this);\n     }\n     default Seq<T> prependAll(Iterable<? extends T> it){\n-      return (Seq<T>)ImmutableList.super.prependAll(it);\n+        Seq<T> res = this;\n+        for(T next : it){\n+            res = res.prepend(next);\n+        }\n+      return res;\n     }\n \n     default Seq<T> take(final long num) {\n","changed_method_name":"prependAll","commit_message_token_length":5,"code_token_length":200,"combined_token_length":205}
{"commit_url":"https://github.com/RPTools/maptool/commit/0fd4d1e1451dc0244360cba44830787ec04b7696","commit_message":"Remove extra vision transform that is no longer needed","code_diff":"@@ -147,7 +147,6 @@ protected void paintComponent(Graphics g) {\n             pitVblTree);\n \n     final var obstructedVision = new Area(unobstructedVision);\n-    obstructedVision.transform(AffineTransform.getTranslateInstance(point.getX(), point.getY()));\n \n     g2d.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_OVER, .5f));\n     if (vision != null) {\n","changed_method_name":"VisibilityInspector::paintComponent","commit_message_token_length":9,"code_token_length":140,"combined_token_length":149}
{"commit_url":"https://github.com/RPTools/maptool/commit/e7086a8db30ad9bc350bfd782f047428df126be7","commit_message":"Include temporary token light sources\n\nThis adds back in necessary functionality for Expose Last Path to function correctly.","code_diff":"@@ -455,6 +455,13 @@ private Illumination getIllumination(IlluminationKey illuminationKey) {\n         illuminationKey, key -> getUpToDateIlluminator(key).getIllumination());\n   }\n \n+  /**\n+   * Add personal lights and daylight for a token, as well as any normal lights if the token is\n+   * temporary.\n+   *\n+   * @param token\n+   * @return All extra light contributions to be made for this token.\n+   */\n   private @Nonnull List<ContributedLight> getPersonalTokenContributions(Token token) {\n     if (!token.getHasSight()) {\n       return Collections.emptyList();\n@@ -477,6 +484,14 @@ private Illumination getIllumination(IlluminationKey illuminationKey) {\n         personalLights.add(contributedLight);\n       }\n \n+      if (token.hasLightSources()\n+          && !lightSourceMap\n+              .getOrDefault(LightSource.Type.NORMAL, Collections.emptySet())\n+              .contains(token.getId())) {\n+        // This accounts for temporary tokens (such as during an Expose Last Path)\n+        personalLights.addAll(calculateLitAreas(token, sight.getMultiplier()));\n+      }\n+\n       if (sight.hasPersonalLightSource()) {\n         // Calculate the personal light area here.\n         // Note that a personal light is affected by its own sight's magnification, but that's it.\n","changed_method_name":"ZoneView::getPersonalTokenContributions","commit_message_token_length":23,"code_token_length":463,"combined_token_length":486}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/d0cf26216986d1e7689369d0f7b01490a6a2e9c6","commit_message":"#4346: Listing samples for a lab officer can be slow","code_diff":"@@ -305,6 +305,12 @@ public class SampleService extends AbstractCoreAdoService<Sample> {\n \n \t\tPredicate filter = createUserFilterWithoutCase(cb, joins);\n \n+\t\tUser currentUser = getCurrentUser();\n+\t\tfinal JurisdictionLevel jurisdictionLevel = currentUser.getJurisdictionLevel();\n+\t\tif (jurisdictionLevel == JurisdictionLevel.LABORATORY || jurisdictionLevel == JurisdictionLevel.EXTERNAL_LABORATORY) {\n+\t\t\treturn filter;\n+\t\t}\n+\n \t\tif (criteria != null) {\n \t\t\tfinal SampleAssociationType sampleAssociationType = criteria.getSampleAssociationType();\n \t\t\tif (sampleAssociationType == SampleAssociationType.CASE) {\n","changed_method_name":"SampleService::createUserFilter","commit_message_token_length":14,"code_token_length":192,"combined_token_length":206}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/fcf3c0746d6e37ff18a376b7d2adeb19ec6195d8","commit_message":"#5612: Increased chunk size for lazy loading to 100\n\n- Avoids a 2nd query when initializing a list view","code_diff":"@@ -12,8 +12,8 @@ import com.vaadin.server.SerializableSupplier;\n import com.vaadin.ui.Grid;\n import com.vaadin.ui.renderers.HtmlRenderer;\n \n-import de.symeda.sormas.api.utils.criteria.BaseCriteria;\n import de.symeda.sormas.api.i18n.I18nProperties;\n+import de.symeda.sormas.api.utils.criteria.BaseCriteria;\n \n public class FilteredGrid<T, C extends BaseCriteria> extends Grid<T> {\n \n@@ -21,11 +21,17 @@ public class FilteredGrid<T, C extends BaseCriteria> extends Grid<T> {\n \n \tprivate static final long serialVersionUID = 8116377533153377424L;\n \n+\t/**\n+\t * For lazy loading: Defines how many entries are loaded into the grid when new data needs to be loaded for the visible range.\n+\t */\n+\tprivate static final int LAZY_BATCH_SIZE = 100;\n+\n \tprivate C criteria;\n \tprivate boolean inEagerMode;\n \n \tpublic FilteredGrid(Class<T> beanType) {\n \t\tsuper(beanType);\n+\t\tgetDataCommunicator().setMinPushSize(LAZY_BATCH_SIZE);\n \t}\n \n \tpublic C getCriteria() {\n","changed_method_name":"FilteredGrid::FilteredGrid","commit_message_token_length":27,"code_token_length":350,"combined_token_length":377}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/b692df37993975be3ed7f20801bcfa81e4b18103","commit_message":"#5644 - fix issue editing contact on mobile app","code_diff":"@@ -48,7 +48,9 @@ public abstract class JurisdictionValidator<T> {\n \t\t\tfinal List<T> jurisdictionTypes = new ArrayList<>();\n \t\t\tjurisdictionTypes.add(isInJurisdiction());\n \t\t\tfor (JurisdictionValidator<T> jurisdictionValidator : associatedJurisdictionValidators) {\n-\t\t\t\tjurisdictionTypes.add(jurisdictionValidator.isInJurisdiction());\n+\t\t\t\tif (jurisdictionValidator != null) {\n+\t\t\t\t\tjurisdictionTypes.add(jurisdictionValidator.isInJurisdiction());\n+\t\t\t\t}\n \t\t\t}\n \t\t\treturn or(jurisdictionTypes);\n \t\t} else {\n","changed_method_name":"JurisdictionValidator::inJurisdiction","commit_message_token_length":11,"code_token_length":200,"combined_token_length":211}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/5752ebd20a0c9e308984349c1382c696fb0e365d","commit_message":"#6204 - reduce non needed complexity for all queries that use event user filter with health facility user","code_diff":"@@ -396,6 +396,7 @@ public class EventService extends AbstractCoreAdoService<Event> {\n \t\t\t\tfilter = CriteriaBuilderHelper\n \t\t\t\t\t.or(cb, filter, cb.equal(eventJoins.getLocation().get(Location.DISTRICT), currentUser.getHealthFacility().getDistrict()));\n \t\t\t}\n+\t\t\tbreak;\n \t\tcase LABORATORY:\n \t\t\tfinal Subquery<Long> sampleSubQuery = cq.subquery(Long.class);\n \t\t\tfinal Root<Sample> sampleRoot = sampleSubQuery.from(Sample.class);\n","changed_method_name":"EventService::createUserFilter","commit_message_token_length":20,"code_token_length":155,"combined_token_length":175}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/0386b83f646e78828e86cbc066c0835e238f499a","commit_message":"#9054 added missing null check","code_diff":"@@ -175,17 +175,20 @@ public abstract class AbstractView extends VerticalLayout implements View {\n \t\t\tnewState = newState.substring(0, paramsIndex);\n \t\t}\n \n-\t\tString urlParams = Arrays.stream(criteriaList)\n-\t\t\t\t.filter(Objects::nonNull)\n-\t\t\t\t.map(BaseCriteria::toUrlParams)\n-\t\t\t\t.filter(params -> !DataHelper.isNullOrEmpty(params))\n-\t\t\t\t.collect(Collectors.joining(\"&\"));\n-\n-\t\tif (urlParams.length() > 0) {\n-\t\t\tif (newState.charAt(newState.length() - 1) != '/') {\n-\t\t\t\tnewState += \"/\";\n+\t\tif (criteriaList != null)\n+\t\t{\n+\t\t\tString urlParams = Arrays.stream(criteriaList)\n+\t\t\t\t\t.filter(Objects::nonNull)\n+\t\t\t\t\t.map(BaseCriteria::toUrlParams)\n+\t\t\t\t\t.filter(params -> !DataHelper.isNullOrEmpty(params))\n+\t\t\t\t\t.collect(Collectors.joining(\"&\"));\n+\n+\t\t\tif (urlParams.length() > 0) {\n+\t\t\t\tif (newState.charAt(newState.length() - 1) != '/') {\n+\t\t\t\t\tnewState += \"/\";\n+\t\t\t\t}\n+\t\t\t\tnewState += \"?\" + urlParams;\n \t\t\t}\n-\t\t\tnewState += \"?\" + urlParams;\n \t\t}\n \n \t\treturn newState;\n","changed_method_name":"AbstractView::buildNavigationState","commit_message_token_length":7,"code_token_length":420,"combined_token_length":427}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/31e1c111c766308e74e1f04d3e4e8ead3840bbdc","commit_message":"#12256 - [Users] Loop of loading when updating the user role of a specific user","code_diff":"@@ -923,7 +923,7 @@ public class UserFacadeEjb implements UserFacade {\n \t\tSet<User> possibleUsersForAvailableFacilities = new HashSet<>();\n \n \t\tpossibleFacilities.forEach(facility -> {\n-\t\t\tif (!facility.getUuid().equals(FacilityDto.NONE_FACILITY_UUID) && !facility.getUuid().equals(FacilityDto.OTHER_FACILITY_UUID)) {\n+\t\t\tif (!FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid()) && !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n \t\t\t\tpossibleUsersForAvailableFacilities.addAll(userService.getFacilityUsersOfHospital(facility));\n \t\t\t}\n \t\t});\n","changed_method_name":"UserFacadeEjb::getPossibleUsersBasedOnCasesFacility","commit_message_token_length":19,"code_token_length":230,"combined_token_length":249}
{"commit_url":"https://github.com/SORMAS-Foundation/SORMAS-Project/commit/d3e0466c9309d56fc39b52d529a3754a5c249dfd","commit_message":"#12256 - [Users] Loop of loading when updating the user role of a specific user","code_diff":"@@ -923,7 +923,9 @@ public class UserFacadeEjb implements UserFacade {\n \t\tSet<User> possibleUsersForAvailableFacilities = new HashSet<>();\n \n \t\tpossibleFacilities.forEach(facility -> {\n-\t\t\tif (!FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid()) && !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n+\t\t\tif (facility != null\n+\t\t\t\t&& !FacilityDto.NONE_FACILITY_UUID.equals(facility.getUuid())\n+\t\t\t\t&& !FacilityDto.OTHER_FACILITY_UUID.equals(facility.getUuid())) {\n \t\t\t\tpossibleUsersForAvailableFacilities.addAll(userService.getFacilityUsersOfHospital(facility));\n \t\t\t}\n \t\t});\n","changed_method_name":"UserFacadeEjb::getPossibleUsersBasedOnCasesFacility","commit_message_token_length":19,"code_token_length":249,"combined_token_length":268}
{"commit_url":"https://github.com/finos/waltz/commit/9533421c3825e6d3fbc3717a5d57b92b5af369cf","commit_message":"CORS: cache OPT requests\n\n#3474","code_diff":"@@ -203,10 +203,9 @@ public class Main {\n     private void enableCORS() {\n \n         options(\"/*\", (req, res) -> {\n-\n             handleCORSHeader(req, res, \"Access-Control-Request-Headers\", \"Access-Control-Allow-Headers\");\n             handleCORSHeader(req, res, \"Access-Control-Request-Method\", \"Access-Control-Allow-Methods\");\n-\n+            res.header(\"Access-Control-Max-Age\", \"600\");\n             return \"OK\";\n         });\n \n","changed_method_name":"Main::enableCORS","commit_message_token_length":11,"code_token_length":194,"combined_token_length":205}
{"commit_url":"https://github.com/microsoft/kafka-connect-cosmosdb/commit/768cbc5d9363a09ce95995607d504d69a968d1ac","commit_message":"MINOR: minor performance and readability improvement for logging","code_diff":"@@ -69,7 +69,7 @@ public class CosmosDBSinkTask extends SinkTask {\n                 logger.debug(\"Writing record, value type: {}\", record.value().getClass().getName());\r\n                 logger.debug(\"Key Schema: {}\", record.keySchema());\r\n                 logger.debug(\"Value schema: {}\", record.valueSchema());\r\n-                logger.trace(\"Value.toString(): {}\", record.value() != null ? record.value().toString() : \"<null>\");\r\n+                logger.trace(\"Value.toString(): {}\", record.value());\r\n \r\n                 Object recordValue;\r\n                 if (record.value() instanceof Struct) {\r\n","changed_method_name":"CosmosDBSinkTask::put","commit_message_token_length":11,"code_token_length":265,"combined_token_length":276}