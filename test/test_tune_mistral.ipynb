{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_name='/home/ubuntu/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config = bnb_config\n",
    "    )\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\", max_new_tokens=5\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import re\n",
    "device = \"cuda\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Enables loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\", # Specifies the quantization type\n",
    "    bnb_4bit_use_double_quant=True, # Enables double quantization for better precision\n",
    ")\n",
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/Mistral-7B-Instruct-v0.2\")\n",
    "# Loading the model with BitsAndBytes configuration, and additional settings from Method-1\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/ubuntu/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float16, # Sets the tensor type to float16 for faster computation\n",
    "    device_map=\"auto\", # Automatically maps the model layers to the available devices\n",
    "    trust_remote_code=True, # Allows the execution of remote code for custom model configurations\n",
    "    #attn_implementation=\"flash_attention_2\", # Uses a specific attention implementation optimized for performance\n",
    "    quantization_config=bnb_config, # Applies the BitsAndBytes configuration\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard code one example\n",
    "sample_commit_message = \"realpath() with the second parameter as 0 / null is glibc only  summary: realpath() takes a second parameter of a buffer to use, take advantage of that so we're more portable on osx and freebsd  test plan: make maek fast_tests  diffcamp revision: 144309 reviewed by: andrewparoski commenters: mwilliams cc: hphp-diffs@lists, macvicar, andrewparoski, mwilliams revert plan: ok\"\n",
    "sample_Original_Code = 'bool RequestEvalState::includeFile(Variant &res, CStrRef path, bool once,\\n                                   LVariableTable* variables,\\n                                   const char *currentDir) {\\n  RequestEvalState *self = s_res.get();\\n  string spath(path.data());\\n  struct stat s;\\n  if (!FileRepository::findFile(spath, s, currentDir)) return false;\\n  map<string, PhpFile*>::const_iterator it = self->m_evaledFiles.find(spath);\\n  Eval::PhpFile *efile = NULL;\\n  if (it != self->m_evaledFiles.end()) {\\n    if (once) {\\n      res = true;\\n      return true;\\n    }\\n    efile = it->second;\\n  } else {\\n    char *rpath = realpath(spath.c_str(), 0);\\n    if (rpath && rpath != spath) {\\n      it = self->m_evaledFiles.find(rpath);\\n      if (it != self->m_evaledFiles.end()) {\\n        self->m_evaledFiles[spath] = efile = it->second;\\n        efile->incRef();\\n        if (once) {\\n          free(rpath);\\n          res = true;\\n          return true;\\n        }\\n      }\\n    } else {\\n      free(rpath);\\n      rpath = 0;\\n    }\\n    if (!efile) {\\n      efile = FileRepository::checkoutFile(rpath ? rpath : spath, s);\\n      if (efile) {\\n        self->m_evaledFiles[spath] = efile;\\n        if (rpath) {\\n          self->m_evaledFiles[rpath] = efile;\\n          efile->incRef();\\n        }\\n      }\\n    }\\n    free(rpath);\\n  }\\n  if (efile) {\\n    res = efile->eval(variables);\\n    return true;\\n  }\\n  return false;\\n}'\n",
    "sample_Modified_Code = 'bool RequestEvalState::includeFile(Variant &res, CStrRef path, bool once,\\n                                   LVariableTable* variables,\\n                                   const char *currentDir) {\\n  RequestEvalState *self = s_res.get();\\n  string spath(path.data());\\n  struct stat s;\\n  if (!FileRepository::findFile(spath, s, currentDir)) return false;\\n  map<string, PhpFile*>::const_iterator it = self->m_evaledFiles.find(spath);\\n  Eval::PhpFile *efile = NULL;\\n  if (it != self->m_evaledFiles.end()) {\\n    if (once) {\\n      res = true;\\n      return true;\\n    }\\n    efile = it->second;\\n  } else {\\n    char *rpath = (char *)malloc(PATH_MAX);\\n    if (rpath == NULL) {\\n      return false;\\n    }\\n    if (realpath(spath.c_str(), rpath) && rpath != spath) {\\n      it = self->m_evaledFiles.find(rpath);\\n      if (it != self->m_evaledFiles.end()) {\\n        self->m_evaledFiles[spath] = efile = it->second;\\n        efile->incRef();\\n        if (once) {\\n          free(rpath);\\n          res = true;\\n          return true;\\n        }\\n      }\\n    } else {\\n      free(rpath);\\n      rpath = 0;\\n    }\\n    if (!efile) {\\n      efile = FileRepository::checkoutFile(rpath ? rpath : spath, s);\\n      if (efile) {\\n        self->m_evaledFiles[spath] = efile;\\n        if (rpath) {\\n          self->m_evaledFiles[rpath] = efile;\\n          efile->incRef();\\n        }\\n      }\\n    }\\n    free(rpath);\\n  }\\n  if (efile) {\\n    res = efile->eval(variables);\\n    return true;\\n  }\\n  return false;\\n}'\n",
    "sample_Code_Diff = \"From 96eff6edb0ab75c3dd386c07fdf78dbb4ae0bbbd Mon Sep 17 00:00:00 2001\\nFrom: macvicar <macvicar@facebook.com>\\nDate: Wed, 11 Aug 2010 01:01:09 -0700\\nSubject: [PATCH] realpath() with the second parameter as 0 / NULL is glibc\\n only\\n\\nSummary:\\nrealpath() takes a second parameter of a buffer to use, take\\nadvantage of that so we're more portable on OSX and FreeBSD\\n\\nTest Plan:\\nmake\\nmaek fast_tests\\n\\nDiffCamp Revision: 144309\\nReviewed By: andrewparoski\\nCommenters: mwilliams\\nCC: hphp-diffs@lists, macvicar, andrewparoski, mwilliams\\nRevert Plan:\\nOk\\n---\\n src/runtime/eval/runtime/eval_state.cpp | 7 +++++--\\n 1 file changed, 5 insertions(+), 2 deletions(-)\\n\\ndiff --git a/src/runtime/eval/runtime/eval_state.cpp b/src/runtime/eval/runtime/eval_state.cpp\\nindex 2f5b442e2f20a..507f1fa742392 100644\\n--- a/src/runtime/eval/runtime/eval_state.cpp\\n+++ b/src/runtime/eval/runtime/eval_state.cpp\\n@@ -271,8 +271,11 @@ bool RequestEvalState::includeFile(Variant &res, CStrRef path, bool once,\\n     }\\n     efile = it->second;\\n   } else {\\n-    char *rpath = realpath(spath.c_str(), 0);\\n-    if (rpath && rpath != spath) {\\n+    char *rpath = (char *)malloc(PATH_MAX);\\n+    if (rpath == NULL) {\\n+      return false;\\n+    }\\n+    if (realpath(spath.c_str(), rpath) && rpath != spath) {\\n       it = self->m_evaledFiles.find(rpath);\\n       if (it != self->m_evaledFiles.end()) {\\n         self->m_evaledFiles[spath] = efile = it->second;\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = '''\n",
    "You are provided with a GitHub commit in this format:\n",
    "Commit Message: \"\"\"the commit message written by the author\"\"\"\n",
    "Original Code: \"\"\"the code before the code modificaton\"\"\"\n",
    "Modified Code: \"\"\"the code after the code modification\"\"\"\n",
    "Code Diff : \"\"\"the diff between the original and the modified code\"\"\"\n",
    "\n",
    "The commit is implementing a code optimization to improve software performance or resource utilization.\n",
    "\"\n",
    "Your task is to meticulously examine the commit message, the original code, the modified code, and the differences between the original and modified code (code diff) to understand the improvements in terms of  performance or resource usage. Focus primarily on the changes made to the code as well as the commit message to grasp how they improve performance or resource usage. Your goal is to understand:\"\n",
    "\n",
    "1) The root cause of inefficiency in the original code that necessitated these optimizations.\n",
    "2) Then categorize the inefficiency into one of the followwing categories:\n",
    "\n",
    "Category A: Performance inefficiency\n",
    "Category B: Resource inefficiency \n",
    "\n",
    "Model response guidelines:\n",
    "You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanation or notes in the response. \n",
    "\n",
    "- Commit Message: \n",
    "{commit_message}\n",
    "\n",
    "- Original Code:\n",
    "```{Original_Code}```\n",
    "\n",
    "- Modified Code:\n",
    "```{Modified_Code}```\n",
    "\n",
    "Code Diff:\n",
    "```{Code_Diff}```\n",
    "\n",
    "Model Response: '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category A: Performance inefficiency\n",
      "\n",
      "Ex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompt_template = ''' <s> [INST] You are an analytical tool specialized in processing and classifying GitHub Commit message. Your task is to assess developer's intent in a given commit message and categorize it into one of the following predefined categories based on its content:\n",
    "                      \n",
    "#                       'Yes':  A commit messages that explicitly mentions performance improvement or optimization, specifically in terms of execution time or resource utilization or trade-off between the two. The message should clearly indicate actions that made the code runs faster or  more efficiently, use less memory, or more efficiently utilize system resources. Also, if a commit message describes a change made to address a performance bottleneck, prevent performance degradation, reduce overheads or solve a problem that negatively affects performance. This includes optimizations like replacing inefficient code patterns that are known to kill performance even if the message does not use the words 'improvement' or 'performance' explicitly.\n",
    "#                       'No': A commit message that do not pertain to performance enhancements. This includes messages related to code changes for testing, documentation, performance profiling/monitoring/debugging/analysis and bug/error/crash fixes that don't explicitly mention performance improvement of the application itself, code refactoring or feature addition without explicit performance optimization,  and mentions of necessary or speculative or potential performance enhancements without concrete evidence or results. Also, a messages that is irrelevant, unclear, or ambiguous, and those that do not provide enough context to determine their intent.     \n",
    "\n",
    "#                     If the commit message doesn't fit clearly into any of the above categories, classify it as: 'No'. Additionally, pay close attention to the context in which terms like 'performance', 'improve' or 'improvements' are used. Not all improvements are related to performance—only, classify a message as 'Yes' if it specifically mentions enhancements related to execution time, memory usage, or resource efficiency. Avoid making assumptions based on ambiguous terms. You should have high confidence in classifying a message as 'Yes' based on careful examination of the information provided in the commit message.\n",
    "#                     If you encounter a commit message with multiple intentions, where at least one of those intentions includes a performance improvement in terms of execution time or resource utilization., classify the entire message as 'Yes'.\n",
    "#                     You will only respond with the predefined category. Do not include the word 'Category'. Do not provide explanations or notes.\n",
    "                    \n",
    "#                     Commit message : ```{commit_message}``` [/INST] Model answer:  </s> '''\n",
    "\n",
    "\n",
    "#sample_commit_message = \"the patch improves the run time at the expense of using more ram in some situations. please note: it doesn't improve the actual algorithm (iteration over all permutations). thanks to alexei sheplyakov.\"\n",
    "\n",
    "\n",
    "generated_prompt = prompt_template.format(commit_message=sample_commit_message, Original_Code= sample_Original_Code,Modified_Code = sample_Modified_Code, Code_Diff = sample_Code_Diff )\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            [{'role': 'user', 'content': generated_prompt}],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=10,\n",
    "        do_sample=False)\n",
    "\n",
    "def parse_output(out):\n",
    "    res = re.search(r'\\b(Yes|No)\\b', out)\n",
    "    if res:\n",
    "        return res.group(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "value = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "print(value)\n",
    "# if value == 'Yes':\n",
    "#     print(\"Perf\")\n",
    "# else:\n",
    "#     print(\"NonPerf\")\n",
    "#     print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "# prompt_template = ''' <s> [INST] You are an analytical tool specialized in processing and classifying GitHub Commit message. Your task is to assess developer's intent in a given commit message and categorize it into one of the following predefined categories based on its content:\n",
    "                      \n",
    "#                       'Yes':  A commit messages that explicitly mentions performance improvement or optimization, specifically in terms of execution time or resource utilization. The message should clearly indicate actions that made the code runs faster or  more efficiently, use less memory, or more efficiently utilize system resources. Also, if a commit message describes a change made to address a performance bottleneck, prevent performance degradation, reduce overheads or solve a problem that negatively affects performance. This includes optimizations like replacing inefficient code patterns that are known to kill performance even if the message does not use the words 'improvement' or 'performance' explicitly.\n",
    "#                       'No': A commit message that do not pertain to performance enhancements. This includes messages related to code changes for testing, documentation, performance profiling/monitoring/debugging/analysis and bug/error/crash fixes that don't explicitly mention performance improvement of the application itself, code refactoring or feature addition without explicit performance optimization,  and mentions of necessary or speculative or potential performance enhancements without concrete evidence or results. Also, a messages that is irrelevant, unclear, or ambiguous, and those that do not provide enough context to determine their intent.     \n",
    "\n",
    "#                     If the commit message doesn't fit clearly into any of the above categories, classify it as: 'No'. Additionally, pay close attention to the context in which terms like 'optimize', 'performance', 'improve' or 'improvements' are used. Not all improvements are related to performance—only, classify a message as 'Yes' if it specifically mentions enhancements related to execution time, memory usage, or resource efficiency. Avoid making assumptions based on ambiguous terms. You should have high confidence in classifying a message as 'Yes' based on careful examination of the information provided in the commit message.\n",
    "#                     If you encounter a commit message with multiple intentions, where at least one of those intentions includes a performance improvement, classify the entire message as 'Yes'.\n",
    "#                     You will only respond with the predefined category. Do not include the word 'Category'. Do not provide explanations or notes.\n",
    "                    \n",
    "#                     Commit message : ```{commit_message}``` [/INST] Model_answer:  </s> '''\n",
    "\n",
    "prompt_template = '''\n",
    "You are provided with a GitHub commit in this format:\n",
    "Commit Message: \"\"\"the commit message written by the author\"\"\"\n",
    "Original Code: \"\"\"the code before the code modificaton\"\"\"\n",
    "Modified Code: \"\"\"the code after the code modification\"\"\"\n",
    "Code Diff : \"\"\"the diff between the original and the modified code\"\"\"\n",
    "\n",
    "The commit is implementing a code optimization to improve software performance or resource utilization.\n",
    "\"\n",
    "Your task is to meticulously examine the commit message, the original code, the modified code, and the differences between the original and modified code (code diff) to understand the improvements in terms of  performance or resource usage. Focus primarily on the changes made to the code as well as the commit message to grasp how they improve performance or resource usage. Your goal is to understand:\"\n",
    "\n",
    "1) The root cause of inefficiency in the original code that necessitated these optimizations.\n",
    "2) Then categorize the inefficiency into one of the followwing categories:\n",
    "\n",
    "Category A: Performance inefficiency\n",
    "Category B: Resource inefficiency \n",
    "\n",
    "\n",
    "- Commit Message: \n",
    "{commit_message}\n",
    "\n",
    "- Original Code:\n",
    "```{Original_Code}```\n",
    "\n",
    "- Modified Code:\n",
    "```{Modified_Code}```\n",
    "\n",
    "Code Diff:\n",
    "```{Code_Diff}```\n",
    "\n",
    "Model Response: '''\n",
    "\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"commit_message\"], template=prompt_template\n",
    ")\n",
    "llm = LLMChain(llm=hf, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "prompt_template = ''' <s> [INST] You are an analytical tool specialized in processing and classifying GitHub Commit message. Your task is to assess developer's intent in a given commit message and categorize it into one of the following predefined categories based on its content:\n",
    "                      \n",
    "                      'Yes':  A commit messages that explicitly mentions performance improvement or optimization, specifically in terms of execution time or resource utilization. The message should clearly indicate actions that made the code runs faster or  more efficiently, use less memory, or more efficiently utilize system resources. Also, if a commit message describes a change made to address a performance bottleneck, prevent performance degradation, reduce overheads or solve a problem that negatively affects performance. This includes optimizations like replacing inefficient code patterns that are known to kill performance even if the message does not use the words 'improvement' or 'performance' explicitly.\n",
    "                      'No': A commit message that do not pertain to performance enhancements. This includes messages related to code changes for testing, documentation, performance profiling/monitoring/debugging/analysis and bug/error/crash fixes that don't explicitly mention performance improvement of the application itself, code refactoring or feature addition without explicit performance optimization,  and mentions of necessary or speculative or potential performance enhancements without concrete evidence or results. Also, a messages that is irrelevant, unclear, or ambiguous, and those that do not provide enough context to determine their intent.     \n",
    "\n",
    "                    If the commit message doesn't fit clearly into any of the above categories, classify it as: 'No'. Additionally, pay close attention to the context in which terms like 'optimize', 'performance', 'improve' or 'improvements' are used. Not all improvements are related to performance—only, classify a message as 'Yes' if it specifically mentions enhancements related to execution time, memory usage, or resource efficiency. Avoid making assumptions based on ambiguous terms. You should have high confidence in classifying a message as 'Yes' based on careful examination of the information provided in the commit message.\n",
    "                    If you encounter a commit message with multiple intentions, where at least one of those intentions includes a performance improvement, classify the entire message as 'Yes'.\n",
    "                    You will only respond with the predefined category. Do not include the word 'Category'. Do not provide explanations or notes.\n",
    "                    \n",
    "                    Commit message : ```{commit_message}``` [/INST] Model_answer:  </s> '''\n",
    "\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"commit_message\"], template=prompt_template\n",
    ")\n",
    "llm = LLMChain(llm=hf, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = '/home/ubuntu/data/train.jsonl'\n",
    "file_test = '/home/ubuntu/data/test.jsonl'\n",
    "# Read the JSON file into a Pandas DataFrame\n",
    "df_train = pd.read_json(json_file_path, lines = True)\n",
    "\n",
    "df_test = pd.read_json(file_test,lines=True)\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming df_train is already loaded and contains a target column named 'target'\n",
    "\n",
    "# Define the range you're interested in\n",
    "start_index = 1\n",
    "end_index = 2180\n",
    "\n",
    "# Filter df_train for the specified range before separating it based on the 'target' column\n",
    "df_train_filtered = df_train.loc[start_index:end_index]\n",
    "\n",
    "# Separate the filtered DataFrame into two based on the target column values\n",
    "df_0 = df_train_filtered[df_train_filtered['target'] == 0]\n",
    "df_1 = df_train_filtered[df_train_filtered['target'] == 1]\n",
    "\n",
    "# Sample rows from each subset\n",
    "# Note: Ensure there are at least 1000 rows in each filtered subset to avoid errors\n",
    "min_count = min(len(df_0), len(df_1))\n",
    "sample_0 = df_0.sample(n=min_count, random_state=42)\n",
    "sample_1 = df_1.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Concatenate the two samples to get a balanced dataset\n",
    "df_balanced_sample = pd.concat([sample_0, sample_1])\n",
    "\n",
    "print(df_balanced_sample)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming df_test is already loaded and contains a target column named 'target'\n",
    "\n",
    "# # Separate the DataFrame into two based on the target column values\n",
    "# df_0 = df_train[df_train['target'] == 0]\n",
    "# df_1 = df_train[df_train['target'] == 1]\n",
    "\n",
    "# # Calculate the minimum count to ensure equal number of 1s and 0s\n",
    "# #min_count = min(len(df_0), len(df_1))\n",
    "\n",
    "# # Sample min_count rows from each subset\n",
    "# sample_0 = df_0.sample(n=1000, random_state=42)\n",
    "# sample_1 = df_1.sample(n=1000, random_state=42)\n",
    "\n",
    "# # Concatenate the two samples to get a balanced dataset\n",
    "# df_balanced_sample = pd.concat([sample_0, sample_1])\n",
    "\n",
    "# print(df_balanced_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do predict with the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions=[]\n",
    "for i in df_balanced_sample['commit_message']:\n",
    "    Predictions.append(llm.run(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_balanced_sample['mistral_result']=Predictions\n",
    "df_balanced_sample['mistral_result_cleaned']=df_balanced_sample['mistral_result'].apply(lambda x: re.search(r'\\b(Yes|No)\\b', x).group(0) if re.search(r'\\b(Yes|No)\\b', x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_balanced_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_sample['mistral_result_cleaned'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_sample['mistral_target']=df_balanced_sample['mistral_result_cleaned'].replace('Yes',1).replace('No',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_sample['mistral_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_sample['mistral_target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_sample['mistral_target']=df_balanced_sample['mistral_target'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_true= df_balanced_sample['target']\n",
    "#y_pred=df_balanced_sample['mistral_target']\n",
    "y_true= df_balanced_sample[df_balanced_sample['mistral_target'].notnull()]['target']\n",
    "y_pred=df_balanced_sample[df_balanced_sample['mistral_target'].notnull()]['mistral_target']\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred,output_dict=True)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df1 = pd.DataFrame(report).transpose()\n",
    "df1 = df1.sort_values(by=['f1-score'], ascending=False)\n",
    "\n",
    "# Plot the classification report\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(df1[['precision', 'recall', 'f1-score']], annot=True, cmap='plasma')\n",
    "plt.title('Classification Report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Extract true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate false positives and false negatives\n",
    "false_positive = fp\n",
    "false_negative = fn\n",
    "\n",
    "print(\"False Positives:\", false_positive)\n",
    "print(\"False Negatives:\", false_negative)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Positive', 'Predicted Negative'], yticklabels=['Actual Positive', 'Actual Negative'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true and y_pred are already defined as shown previously\n",
    "import numpy as np\n",
    "unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with unique class labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=unique_labels, \n",
    "            yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the false positives and false negetives for tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_balanced_sample[ df_balanced_sample['mistral_target'] == 1]\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = pos[pos['target'] == 0]\n",
    "fp.to_csv('false_positive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df_balanced_sample[ df_balanced_sample['mistral_target'] == 0]  #for false negetives\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = pos[pos['target'] == 1]\n",
    "fn.to_csv('false_negetive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# import torch\n",
    "# # Specify the device for CUDA acceleration\n",
    "# device = \"cuda\" # Or \"cpu\" if you're not using NVIDIA GPUs\n",
    "# # Configuration for BitsAndBytes to optimize model loading\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, # Enables loading the model in 4-bit precision\n",
    "#     bnb_4bit_quant_type=\"nf4\", # Specifies the quantization type\n",
    "#     bnb_4bit_use_double_quant=True, # Enables double quantization for better precision\n",
    "# )\n",
    "# # Loading the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# # Loading the model with BitsAndBytes configuration, and additional settings from Method-1\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     torch_dtype=torch.float16, # Sets the tensor type to float16 for faster computation\n",
    "#     device_map=\"auto\", # Automatically maps the model layers to the available devices\n",
    "#     trust_remote_code=True, # Allows the execution of remote code for custom model configurations\n",
    "#     attn_implementation=\"flash_attention_2\", # Uses a specific attention implementation optimized for performance\n",
    "#     _config=bnb_config, # Applies the BitsAndBytes configuration\n",
    "# )\n",
    "# # Move the model to the specified device\n",
    "# model.to(device)\n",
    "# # Prepare the messages for encoding\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "# # Encode the messages using the tokenizer\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "# # Generate responses using the model\n",
    "# generated_ids = model.generate(encodeds, max_new_tokens=5, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# # Print the generated response\n",
    "# print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytoolenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
